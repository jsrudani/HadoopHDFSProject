// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: yarn_service_protos.proto

package org.apache.hadoop.yarn.proto;

public final class YarnServiceProtos {
  private YarnServiceProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface RegisterApplicationMasterRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string host = 1;
    /**
     * <code>optional string host = 1;</code>
     */
    boolean hasHost();
    /**
     * <code>optional string host = 1;</code>
     */
    java.lang.String getHost();
    /**
     * <code>optional string host = 1;</code>
     */
    com.google.protobuf.ByteString
        getHostBytes();

    // optional int32 rpc_port = 2;
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    boolean hasRpcPort();
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    int getRpcPort();

    // optional string tracking_url = 3;
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    boolean hasTrackingUrl();
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    java.lang.String getTrackingUrl();
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    com.google.protobuf.ByteString
        getTrackingUrlBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterRequestProto}
   *
   * <pre>
   *&#47;///////////////////////////////////////////////////
   * ///// AM_RM_Protocol ///////////////////////////////
   * ////////////////////////////////////////////////////
   * </pre>
   */
  public static final class RegisterApplicationMasterRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements RegisterApplicationMasterRequestProtoOrBuilder {
    // Use RegisterApplicationMasterRequestProto.newBuilder() to construct.
    private RegisterApplicationMasterRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegisterApplicationMasterRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegisterApplicationMasterRequestProto defaultInstance;
    public static RegisterApplicationMasterRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public RegisterApplicationMasterRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterApplicationMasterRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              host_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              rpcPort_ = input.readInt32();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              trackingUrl_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RegisterApplicationMasterRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<RegisterApplicationMasterRequestProto>() {
      public RegisterApplicationMasterRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterApplicationMasterRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterApplicationMasterRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string host = 1;
    public static final int HOST_FIELD_NUMBER = 1;
    private java.lang.Object host_;
    /**
     * <code>optional string host = 1;</code>
     */
    public boolean hasHost() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string host = 1;</code>
     */
    public java.lang.String getHost() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          host_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string host = 1;</code>
     */
    public com.google.protobuf.ByteString
        getHostBytes() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        host_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional int32 rpc_port = 2;
    public static final int RPC_PORT_FIELD_NUMBER = 2;
    private int rpcPort_;
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    public boolean hasRpcPort() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    public int getRpcPort() {
      return rpcPort_;
    }

    // optional string tracking_url = 3;
    public static final int TRACKING_URL_FIELD_NUMBER = 3;
    private java.lang.Object trackingUrl_;
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    public boolean hasTrackingUrl() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    public java.lang.String getTrackingUrl() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          trackingUrl_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    public com.google.protobuf.ByteString
        getTrackingUrlBytes() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        trackingUrl_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      host_ = "";
      rpcPort_ = 0;
      trackingUrl_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getHostBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, rpcPort_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getTrackingUrlBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getHostBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, rpcPort_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getTrackingUrlBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto) obj;

      boolean result = true;
      result = result && (hasHost() == other.hasHost());
      if (hasHost()) {
        result = result && getHost()
            .equals(other.getHost());
      }
      result = result && (hasRpcPort() == other.hasRpcPort());
      if (hasRpcPort()) {
        result = result && (getRpcPort()
            == other.getRpcPort());
      }
      result = result && (hasTrackingUrl() == other.hasTrackingUrl());
      if (hasTrackingUrl()) {
        result = result && getTrackingUrl()
            .equals(other.getTrackingUrl());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasHost()) {
        hash = (37 * hash) + HOST_FIELD_NUMBER;
        hash = (53 * hash) + getHost().hashCode();
      }
      if (hasRpcPort()) {
        hash = (37 * hash) + RPC_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getRpcPort();
      }
      if (hasTrackingUrl()) {
        hash = (37 * hash) + TRACKING_URL_FIELD_NUMBER;
        hash = (53 * hash) + getTrackingUrl().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterRequestProto}
     *
     * <pre>
     *&#47;///////////////////////////////////////////////////
     * ///// AM_RM_Protocol ///////////////////////////////
     * ////////////////////////////////////////////////////
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        host_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        rpcPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        trackingUrl_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.host_ = host_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.rpcPort_ = rpcPort_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.trackingUrl_ = trackingUrl_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.getDefaultInstance()) return this;
        if (other.hasHost()) {
          bitField0_ |= 0x00000001;
          host_ = other.host_;
          onChanged();
        }
        if (other.hasRpcPort()) {
          setRpcPort(other.getRpcPort());
        }
        if (other.hasTrackingUrl()) {
          bitField0_ |= 0x00000004;
          trackingUrl_ = other.trackingUrl_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string host = 1;
      private java.lang.Object host_ = "";
      /**
       * <code>optional string host = 1;</code>
       */
      public boolean hasHost() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public java.lang.String getHost() {
        java.lang.Object ref = host_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          host_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public com.google.protobuf.ByteString
          getHostBytes() {
        java.lang.Object ref = host_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          host_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public Builder setHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        host_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public Builder clearHost() {
        bitField0_ = (bitField0_ & ~0x00000001);
        host_ = getDefaultInstance().getHost();
        onChanged();
        return this;
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public Builder setHostBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        host_ = value;
        onChanged();
        return this;
      }

      // optional int32 rpc_port = 2;
      private int rpcPort_ ;
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public boolean hasRpcPort() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public int getRpcPort() {
        return rpcPort_;
      }
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public Builder setRpcPort(int value) {
        bitField0_ |= 0x00000002;
        rpcPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public Builder clearRpcPort() {
        bitField0_ = (bitField0_ & ~0x00000002);
        rpcPort_ = 0;
        onChanged();
        return this;
      }

      // optional string tracking_url = 3;
      private java.lang.Object trackingUrl_ = "";
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public boolean hasTrackingUrl() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public java.lang.String getTrackingUrl() {
        java.lang.Object ref = trackingUrl_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          trackingUrl_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public com.google.protobuf.ByteString
          getTrackingUrlBytes() {
        java.lang.Object ref = trackingUrl_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          trackingUrl_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public Builder setTrackingUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        trackingUrl_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public Builder clearTrackingUrl() {
        bitField0_ = (bitField0_ & ~0x00000004);
        trackingUrl_ = getDefaultInstance().getTrackingUrl();
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public Builder setTrackingUrlBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        trackingUrl_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterApplicationMasterRequestProto)
    }

    static {
      defaultInstance = new RegisterApplicationMasterRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterApplicationMasterRequestProto)
  }

  public interface RegisterApplicationMasterResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ResourceProto maximumCapability = 1;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    boolean hasMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder();

    // optional bytes client_to_am_token_master_key = 2;
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    boolean hasClientToAmTokenMasterKey();
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    com.google.protobuf.ByteString getClientToAmTokenMasterKey();

    // repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> 
        getApplicationACLsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto getApplicationACLs(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    int getApplicationACLsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
        getApplicationACLsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder getApplicationACLsOrBuilder(
        int index);

    // optional string queue = 5;
    /**
     * <code>optional string queue = 5;</code>
     */
    boolean hasQueue();
    /**
     * <code>optional string queue = 5;</code>
     */
    java.lang.String getQueue();
    /**
     * <code>optional string queue = 5;</code>
     */
    com.google.protobuf.ByteString
        getQueueBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterResponseProto}
   */
  public static final class RegisterApplicationMasterResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements RegisterApplicationMasterResponseProtoOrBuilder {
    // Use RegisterApplicationMasterResponseProto.newBuilder() to construct.
    private RegisterApplicationMasterResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegisterApplicationMasterResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegisterApplicationMasterResponseProto defaultInstance;
    public static RegisterApplicationMasterResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public RegisterApplicationMasterResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterApplicationMasterResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = maximumCapability_.toBuilder();
              }
              maximumCapability_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(maximumCapability_);
                maximumCapability_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              clientToAmTokenMasterKey_ = input.readBytes();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                applicationACLs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              applicationACLs_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              bitField0_ |= 0x00000004;
              queue_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          applicationACLs_ = java.util.Collections.unmodifiableList(applicationACLs_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RegisterApplicationMasterResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<RegisterApplicationMasterResponseProto>() {
      public RegisterApplicationMasterResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterApplicationMasterResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterApplicationMasterResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ResourceProto maximumCapability = 1;
    public static final int MAXIMUMCAPABILITY_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    public boolean hasMaximumCapability() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
      return maximumCapability_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
      return maximumCapability_;
    }

    // optional bytes client_to_am_token_master_key = 2;
    public static final int CLIENT_TO_AM_TOKEN_MASTER_KEY_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString clientToAmTokenMasterKey_;
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    public boolean hasClientToAmTokenMasterKey() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    public com.google.protobuf.ByteString getClientToAmTokenMasterKey() {
      return clientToAmTokenMasterKey_;
    }

    // repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;
    public static final int APPLICATION_ACLS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> applicationACLs_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> getApplicationACLsList() {
      return applicationACLs_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
        getApplicationACLsOrBuilderList() {
      return applicationACLs_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public int getApplicationACLsCount() {
      return applicationACLs_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto getApplicationACLs(int index) {
      return applicationACLs_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder getApplicationACLsOrBuilder(
        int index) {
      return applicationACLs_.get(index);
    }

    // optional string queue = 5;
    public static final int QUEUE_FIELD_NUMBER = 5;
    private java.lang.Object queue_;
    /**
     * <code>optional string queue = 5;</code>
     */
    public boolean hasQueue() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string queue = 5;</code>
     */
    public java.lang.String getQueue() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          queue_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string queue = 5;</code>
     */
    public com.google.protobuf.ByteString
        getQueueBytes() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        queue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      clientToAmTokenMasterKey_ = com.google.protobuf.ByteString.EMPTY;
      applicationACLs_ = java.util.Collections.emptyList();
      queue_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, maximumCapability_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, clientToAmTokenMasterKey_);
      }
      for (int i = 0; i < applicationACLs_.size(); i++) {
        output.writeMessage(3, applicationACLs_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(5, getQueueBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, maximumCapability_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, clientToAmTokenMasterKey_);
      }
      for (int i = 0; i < applicationACLs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, applicationACLs_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getQueueBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto) obj;

      boolean result = true;
      result = result && (hasMaximumCapability() == other.hasMaximumCapability());
      if (hasMaximumCapability()) {
        result = result && getMaximumCapability()
            .equals(other.getMaximumCapability());
      }
      result = result && (hasClientToAmTokenMasterKey() == other.hasClientToAmTokenMasterKey());
      if (hasClientToAmTokenMasterKey()) {
        result = result && getClientToAmTokenMasterKey()
            .equals(other.getClientToAmTokenMasterKey());
      }
      result = result && getApplicationACLsList()
          .equals(other.getApplicationACLsList());
      result = result && (hasQueue() == other.hasQueue());
      if (hasQueue()) {
        result = result && getQueue()
            .equals(other.getQueue());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasMaximumCapability()) {
        hash = (37 * hash) + MAXIMUMCAPABILITY_FIELD_NUMBER;
        hash = (53 * hash) + getMaximumCapability().hashCode();
      }
      if (hasClientToAmTokenMasterKey()) {
        hash = (37 * hash) + CLIENT_TO_AM_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getClientToAmTokenMasterKey().hashCode();
      }
      if (getApplicationACLsCount() > 0) {
        hash = (37 * hash) + APPLICATION_ACLS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationACLsList().hashCode();
      }
      if (hasQueue()) {
        hash = (37 * hash) + QUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getQueue().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMaximumCapabilityFieldBuilder();
          getApplicationACLsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        clientToAmTokenMasterKey_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (applicationACLsBuilder_ == null) {
          applicationACLs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          applicationACLsBuilder_.clear();
        }
        queue_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (maximumCapabilityBuilder_ == null) {
          result.maximumCapability_ = maximumCapability_;
        } else {
          result.maximumCapability_ = maximumCapabilityBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.clientToAmTokenMasterKey_ = clientToAmTokenMasterKey_;
        if (applicationACLsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            applicationACLs_ = java.util.Collections.unmodifiableList(applicationACLs_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.applicationACLs_ = applicationACLs_;
        } else {
          result.applicationACLs_ = applicationACLsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000004;
        }
        result.queue_ = queue_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance()) return this;
        if (other.hasMaximumCapability()) {
          mergeMaximumCapability(other.getMaximumCapability());
        }
        if (other.hasClientToAmTokenMasterKey()) {
          setClientToAmTokenMasterKey(other.getClientToAmTokenMasterKey());
        }
        if (applicationACLsBuilder_ == null) {
          if (!other.applicationACLs_.isEmpty()) {
            if (applicationACLs_.isEmpty()) {
              applicationACLs_ = other.applicationACLs_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureApplicationACLsIsMutable();
              applicationACLs_.addAll(other.applicationACLs_);
            }
            onChanged();
          }
        } else {
          if (!other.applicationACLs_.isEmpty()) {
            if (applicationACLsBuilder_.isEmpty()) {
              applicationACLsBuilder_.dispose();
              applicationACLsBuilder_ = null;
              applicationACLs_ = other.applicationACLs_;
              bitField0_ = (bitField0_ & ~0x00000004);
              applicationACLsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getApplicationACLsFieldBuilder() : null;
            } else {
              applicationACLsBuilder_.addAllMessages(other.applicationACLs_);
            }
          }
        }
        if (other.hasQueue()) {
          bitField0_ |= 0x00000008;
          queue_ = other.queue_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ResourceProto maximumCapability = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> maximumCapabilityBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public boolean hasMaximumCapability() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          return maximumCapability_;
        } else {
          return maximumCapabilityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder setMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          maximumCapability_ = value;
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder setMaximumCapability(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = builderForValue.build();
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder mergeMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              maximumCapability_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            maximumCapability_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(maximumCapability_).mergeFrom(value).buildPartial();
          } else {
            maximumCapability_ = value;
          }
          onChanged();
        } else {
          maximumCapabilityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder clearMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getMaximumCapabilityBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getMaximumCapabilityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
        if (maximumCapabilityBuilder_ != null) {
          return maximumCapabilityBuilder_.getMessageOrBuilder();
        } else {
          return maximumCapability_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getMaximumCapabilityFieldBuilder() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapabilityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  maximumCapability_,
                  getParentForChildren(),
                  isClean());
          maximumCapability_ = null;
        }
        return maximumCapabilityBuilder_;
      }

      // optional bytes client_to_am_token_master_key = 2;
      private com.google.protobuf.ByteString clientToAmTokenMasterKey_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public boolean hasClientToAmTokenMasterKey() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public com.google.protobuf.ByteString getClientToAmTokenMasterKey() {
        return clientToAmTokenMasterKey_;
      }
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public Builder setClientToAmTokenMasterKey(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        clientToAmTokenMasterKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public Builder clearClientToAmTokenMasterKey() {
        bitField0_ = (bitField0_ & ~0x00000002);
        clientToAmTokenMasterKey_ = getDefaultInstance().getClientToAmTokenMasterKey();
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> applicationACLs_ =
        java.util.Collections.emptyList();
      private void ensureApplicationACLsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          applicationACLs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto>(applicationACLs_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> applicationACLsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> getApplicationACLsList() {
        if (applicationACLsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applicationACLs_);
        } else {
          return applicationACLsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public int getApplicationACLsCount() {
        if (applicationACLsBuilder_ == null) {
          return applicationACLs_.size();
        } else {
          return applicationACLsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto getApplicationACLs(int index) {
        if (applicationACLsBuilder_ == null) {
          return applicationACLs_.get(index);
        } else {
          return applicationACLsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder setApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto value) {
        if (applicationACLsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationACLsIsMutable();
          applicationACLs_.set(index, value);
          onChanged();
        } else {
          applicationACLsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder setApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder builderForValue) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationACLsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto value) {
        if (applicationACLsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(value);
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto value) {
        if (applicationACLsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(index, value);
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder builderForValue) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(builderForValue.build());
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder builderForValue) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addAllApplicationACLs(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> values) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          super.addAll(values, applicationACLs_);
          onChanged();
        } else {
          applicationACLsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder clearApplicationACLs() {
        if (applicationACLsBuilder_ == null) {
          applicationACLs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          applicationACLsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder removeApplicationACLs(int index) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.remove(index);
          onChanged();
        } else {
          applicationACLsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder getApplicationACLsBuilder(
          int index) {
        return getApplicationACLsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder getApplicationACLsOrBuilder(
          int index) {
        if (applicationACLsBuilder_ == null) {
          return applicationACLs_.get(index);  } else {
          return applicationACLsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
           getApplicationACLsOrBuilderList() {
        if (applicationACLsBuilder_ != null) {
          return applicationACLsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applicationACLs_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder addApplicationACLsBuilder() {
        return getApplicationACLsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder addApplicationACLsBuilder(
          int index) {
        return getApplicationACLsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder> 
           getApplicationACLsBuilderList() {
        return getApplicationACLsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
          getApplicationACLsFieldBuilder() {
        if (applicationACLsBuilder_ == null) {
          applicationACLsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder>(
                  applicationACLs_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          applicationACLs_ = null;
        }
        return applicationACLsBuilder_;
      }

      // optional string queue = 5;
      private java.lang.Object queue_ = "";
      /**
       * <code>optional string queue = 5;</code>
       */
      public boolean hasQueue() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public java.lang.String getQueue() {
        java.lang.Object ref = queue_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          queue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public com.google.protobuf.ByteString
          getQueueBytes() {
        java.lang.Object ref = queue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          queue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public Builder setQueue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        queue_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public Builder clearQueue() {
        bitField0_ = (bitField0_ & ~0x00000008);
        queue_ = getDefaultInstance().getQueue();
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public Builder setQueueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        queue_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterApplicationMasterResponseProto)
    }

    static {
      defaultInstance = new RegisterApplicationMasterResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterApplicationMasterResponseProto)
  }

  public interface FinishApplicationMasterRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string diagnostics = 1;
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    com.google.protobuf.ByteString
        getDiagnosticsBytes();

    // optional string tracking_url = 2;
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    boolean hasTrackingUrl();
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    java.lang.String getTrackingUrl();
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    com.google.protobuf.ByteString
        getTrackingUrlBytes();

    // optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    boolean hasFinalApplicationStatus();
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus();
  }
  /**
   * Protobuf type {@code hadoop.yarn.FinishApplicationMasterRequestProto}
   */
  public static final class FinishApplicationMasterRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements FinishApplicationMasterRequestProtoOrBuilder {
    // Use FinishApplicationMasterRequestProto.newBuilder() to construct.
    private FinishApplicationMasterRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FinishApplicationMasterRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FinishApplicationMasterRequestProto defaultInstance;
    public static FinishApplicationMasterRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public FinishApplicationMasterRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FinishApplicationMasterRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              diagnostics_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              trackingUrl_ = input.readBytes();
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto value = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000004;
                finalApplicationStatus_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FinishApplicationMasterRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<FinishApplicationMasterRequestProto>() {
      public FinishApplicationMasterRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinishApplicationMasterRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FinishApplicationMasterRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string diagnostics = 1;
    public static final int DIAGNOSTICS_FIELD_NUMBER = 1;
    private java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    public com.google.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string tracking_url = 2;
    public static final int TRACKING_URL_FIELD_NUMBER = 2;
    private java.lang.Object trackingUrl_;
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    public boolean hasTrackingUrl() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    public java.lang.String getTrackingUrl() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          trackingUrl_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    public com.google.protobuf.ByteString
        getTrackingUrlBytes() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        trackingUrl_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;
    public static final int FINAL_APPLICATION_STATUS_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto finalApplicationStatus_;
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    public boolean hasFinalApplicationStatus() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus() {
      return finalApplicationStatus_;
    }

    private void initFields() {
      diagnostics_ = "";
      trackingUrl_ = "";
      finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getDiagnosticsBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getTrackingUrlBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeEnum(3, finalApplicationStatus_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getDiagnosticsBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getTrackingUrlBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, finalApplicationStatus_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto) obj;

      boolean result = true;
      result = result && (hasDiagnostics() == other.hasDiagnostics());
      if (hasDiagnostics()) {
        result = result && getDiagnostics()
            .equals(other.getDiagnostics());
      }
      result = result && (hasTrackingUrl() == other.hasTrackingUrl());
      if (hasTrackingUrl()) {
        result = result && getTrackingUrl()
            .equals(other.getTrackingUrl());
      }
      result = result && (hasFinalApplicationStatus() == other.hasFinalApplicationStatus());
      if (hasFinalApplicationStatus()) {
        result = result &&
            (getFinalApplicationStatus() == other.getFinalApplicationStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      if (hasTrackingUrl()) {
        hash = (37 * hash) + TRACKING_URL_FIELD_NUMBER;
        hash = (53 * hash) + getTrackingUrl().hashCode();
      }
      if (hasFinalApplicationStatus()) {
        hash = (37 * hash) + FINAL_APPLICATION_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getFinalApplicationStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.FinishApplicationMasterRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        diagnostics_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        trackingUrl_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.diagnostics_ = diagnostics_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.trackingUrl_ = trackingUrl_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.finalApplicationStatus_ = finalApplicationStatus_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.getDefaultInstance()) return this;
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000001;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        if (other.hasTrackingUrl()) {
          bitField0_ |= 0x00000002;
          trackingUrl_ = other.trackingUrl_;
          onChanged();
        }
        if (other.hasFinalApplicationStatus()) {
          setFinalApplicationStatus(other.getFinalApplicationStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string diagnostics = 1;
      private java.lang.Object diagnostics_ = "";
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          diagnostics_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public com.google.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000001);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public Builder setDiagnosticsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      // optional string tracking_url = 2;
      private java.lang.Object trackingUrl_ = "";
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public boolean hasTrackingUrl() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public java.lang.String getTrackingUrl() {
        java.lang.Object ref = trackingUrl_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          trackingUrl_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public com.google.protobuf.ByteString
          getTrackingUrlBytes() {
        java.lang.Object ref = trackingUrl_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          trackingUrl_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public Builder setTrackingUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        trackingUrl_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public Builder clearTrackingUrl() {
        bitField0_ = (bitField0_ & ~0x00000002);
        trackingUrl_ = getDefaultInstance().getTrackingUrl();
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public Builder setTrackingUrlBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        trackingUrl_ = value;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;
      private org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public boolean hasFinalApplicationStatus() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus() {
        return finalApplicationStatus_;
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public Builder setFinalApplicationStatus(org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000004;
        finalApplicationStatus_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public Builder clearFinalApplicationStatus() {
        bitField0_ = (bitField0_ & ~0x00000004);
        finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.FinishApplicationMasterRequestProto)
    }

    static {
      defaultInstance = new FinishApplicationMasterRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.FinishApplicationMasterRequestProto)
  }

  public interface FinishApplicationMasterResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool isUnregistered = 1 [default = false];
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    boolean hasIsUnregistered();
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    boolean getIsUnregistered();
  }
  /**
   * Protobuf type {@code hadoop.yarn.FinishApplicationMasterResponseProto}
   */
  public static final class FinishApplicationMasterResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements FinishApplicationMasterResponseProtoOrBuilder {
    // Use FinishApplicationMasterResponseProto.newBuilder() to construct.
    private FinishApplicationMasterResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FinishApplicationMasterResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FinishApplicationMasterResponseProto defaultInstance;
    public static FinishApplicationMasterResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public FinishApplicationMasterResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FinishApplicationMasterResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              isUnregistered_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FinishApplicationMasterResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<FinishApplicationMasterResponseProto>() {
      public FinishApplicationMasterResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinishApplicationMasterResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FinishApplicationMasterResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool isUnregistered = 1 [default = false];
    public static final int ISUNREGISTERED_FIELD_NUMBER = 1;
    private boolean isUnregistered_;
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    public boolean hasIsUnregistered() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    public boolean getIsUnregistered() {
      return isUnregistered_;
    }

    private void initFields() {
      isUnregistered_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, isUnregistered_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, isUnregistered_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto) obj;

      boolean result = true;
      result = result && (hasIsUnregistered() == other.hasIsUnregistered());
      if (hasIsUnregistered()) {
        result = result && (getIsUnregistered()
            == other.getIsUnregistered());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasIsUnregistered()) {
        hash = (37 * hash) + ISUNREGISTERED_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIsUnregistered());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.FinishApplicationMasterResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        isUnregistered_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.isUnregistered_ = isUnregistered_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.getDefaultInstance()) return this;
        if (other.hasIsUnregistered()) {
          setIsUnregistered(other.getIsUnregistered());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool isUnregistered = 1 [default = false];
      private boolean isUnregistered_ ;
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public boolean hasIsUnregistered() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public boolean getIsUnregistered() {
        return isUnregistered_;
      }
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public Builder setIsUnregistered(boolean value) {
        bitField0_ |= 0x00000001;
        isUnregistered_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public Builder clearIsUnregistered() {
        bitField0_ = (bitField0_ & ~0x00000001);
        isUnregistered_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.FinishApplicationMasterResponseProto)
    }

    static {
      defaultInstance = new FinishApplicationMasterResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.FinishApplicationMasterResponseProto)
  }

  public interface AllocateRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ResourceRequestProto ask = 1;
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> 
        getAskList();
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto getAsk(int index);
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    int getAskCount();
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
        getAskOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder getAskOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerIdProto release = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getReleaseList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getRelease(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    int getReleaseCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getReleaseOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getReleaseOrBuilder(
        int index);

    // optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    boolean hasBlacklistRequest();
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto getBlacklistRequest();
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder getBlacklistRequestOrBuilder();

    // optional int32 response_id = 4;
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    boolean hasResponseId();
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    int getResponseId();

    // optional float progress = 5;
    /**
     * <code>optional float progress = 5;</code>
     */
    boolean hasProgress();
    /**
     * <code>optional float progress = 5;</code>
     */
    float getProgress();

    // repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> 
        getIncreaseRequestList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto getIncreaseRequest(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    int getIncreaseRequestCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
        getIncreaseRequestOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder getIncreaseRequestOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.AllocateRequestProto}
   */
  public static final class AllocateRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocateRequestProtoOrBuilder {
    // Use AllocateRequestProto.newBuilder() to construct.
    private AllocateRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocateRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocateRequestProto defaultInstance;
    public static AllocateRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocateRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocateRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                ask_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              ask_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                release_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              release_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = blacklistRequest_.toBuilder();
              }
              blacklistRequest_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(blacklistRequest_);
                blacklistRequest_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000002;
              responseId_ = input.readInt32();
              break;
            }
            case 45: {
              bitField0_ |= 0x00000004;
              progress_ = input.readFloat();
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                increaseRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              increaseRequest_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          ask_ = java.util.Collections.unmodifiableList(ask_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          release_ = java.util.Collections.unmodifiableList(release_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          increaseRequest_ = java.util.Collections.unmodifiableList(increaseRequest_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocateRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocateRequestProto>() {
      public AllocateRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocateRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocateRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // repeated .hadoop.yarn.ResourceRequestProto ask = 1;
    public static final int ASK_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> ask_;
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> getAskList() {
      return ask_;
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
        getAskOrBuilderList() {
      return ask_;
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public int getAskCount() {
      return ask_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto getAsk(int index) {
      return ask_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder getAskOrBuilder(
        int index) {
      return ask_.get(index);
    }

    // repeated .hadoop.yarn.ContainerIdProto release = 2;
    public static final int RELEASE_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> release_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getReleaseList() {
      return release_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getReleaseOrBuilderList() {
      return release_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public int getReleaseCount() {
      return release_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getRelease(int index) {
      return release_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getReleaseOrBuilder(
        int index) {
      return release_.get(index);
    }

    // optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;
    public static final int BLACKLIST_REQUEST_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto blacklistRequest_;
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    public boolean hasBlacklistRequest() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto getBlacklistRequest() {
      return blacklistRequest_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder getBlacklistRequestOrBuilder() {
      return blacklistRequest_;
    }

    // optional int32 response_id = 4;
    public static final int RESPONSE_ID_FIELD_NUMBER = 4;
    private int responseId_;
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    public boolean hasResponseId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    public int getResponseId() {
      return responseId_;
    }

    // optional float progress = 5;
    public static final int PROGRESS_FIELD_NUMBER = 5;
    private float progress_;
    /**
     * <code>optional float progress = 5;</code>
     */
    public boolean hasProgress() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional float progress = 5;</code>
     */
    public float getProgress() {
      return progress_;
    }

    // repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;
    public static final int INCREASE_REQUEST_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> increaseRequest_;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> getIncreaseRequestList() {
      return increaseRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
        getIncreaseRequestOrBuilderList() {
      return increaseRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public int getIncreaseRequestCount() {
      return increaseRequest_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto getIncreaseRequest(int index) {
      return increaseRequest_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder getIncreaseRequestOrBuilder(
        int index) {
      return increaseRequest_.get(index);
    }

    private void initFields() {
      ask_ = java.util.Collections.emptyList();
      release_ = java.util.Collections.emptyList();
      blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
      responseId_ = 0;
      progress_ = 0F;
      increaseRequest_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < ask_.size(); i++) {
        output.writeMessage(1, ask_.get(i));
      }
      for (int i = 0; i < release_.size(); i++) {
        output.writeMessage(2, release_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(3, blacklistRequest_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(4, responseId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeFloat(5, progress_);
      }
      for (int i = 0; i < increaseRequest_.size(); i++) {
        output.writeMessage(6, increaseRequest_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < ask_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, ask_.get(i));
      }
      for (int i = 0; i < release_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, release_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, blacklistRequest_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, responseId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(5, progress_);
      }
      for (int i = 0; i < increaseRequest_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, increaseRequest_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto) obj;

      boolean result = true;
      result = result && getAskList()
          .equals(other.getAskList());
      result = result && getReleaseList()
          .equals(other.getReleaseList());
      result = result && (hasBlacklistRequest() == other.hasBlacklistRequest());
      if (hasBlacklistRequest()) {
        result = result && getBlacklistRequest()
            .equals(other.getBlacklistRequest());
      }
      result = result && (hasResponseId() == other.hasResponseId());
      if (hasResponseId()) {
        result = result && (getResponseId()
            == other.getResponseId());
      }
      result = result && (hasProgress() == other.hasProgress());
      if (hasProgress()) {
        result = result && (Float.floatToIntBits(getProgress())    == Float.floatToIntBits(other.getProgress()));
      }
      result = result && getIncreaseRequestList()
          .equals(other.getIncreaseRequestList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getAskCount() > 0) {
        hash = (37 * hash) + ASK_FIELD_NUMBER;
        hash = (53 * hash) + getAskList().hashCode();
      }
      if (getReleaseCount() > 0) {
        hash = (37 * hash) + RELEASE_FIELD_NUMBER;
        hash = (53 * hash) + getReleaseList().hashCode();
      }
      if (hasBlacklistRequest()) {
        hash = (37 * hash) + BLACKLIST_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getBlacklistRequest().hashCode();
      }
      if (hasResponseId()) {
        hash = (37 * hash) + RESPONSE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getResponseId();
      }
      if (hasProgress()) {
        hash = (37 * hash) + PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + Float.floatToIntBits(
            getProgress());
      }
      if (getIncreaseRequestCount() > 0) {
        hash = (37 * hash) + INCREASE_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getIncreaseRequestList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.AllocateRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getAskFieldBuilder();
          getReleaseFieldBuilder();
          getBlacklistRequestFieldBuilder();
          getIncreaseRequestFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (askBuilder_ == null) {
          ask_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          askBuilder_.clear();
        }
        if (releaseBuilder_ == null) {
          release_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          releaseBuilder_.clear();
        }
        if (blacklistRequestBuilder_ == null) {
          blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
        } else {
          blacklistRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        responseId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        progress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000010);
        if (increaseRequestBuilder_ == null) {
          increaseRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          increaseRequestBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (askBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            ask_ = java.util.Collections.unmodifiableList(ask_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.ask_ = ask_;
        } else {
          result.ask_ = askBuilder_.build();
        }
        if (releaseBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            release_ = java.util.Collections.unmodifiableList(release_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.release_ = release_;
        } else {
          result.release_ = releaseBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000001;
        }
        if (blacklistRequestBuilder_ == null) {
          result.blacklistRequest_ = blacklistRequest_;
        } else {
          result.blacklistRequest_ = blacklistRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000002;
        }
        result.responseId_ = responseId_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000004;
        }
        result.progress_ = progress_;
        if (increaseRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            increaseRequest_ = java.util.Collections.unmodifiableList(increaseRequest_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.increaseRequest_ = increaseRequest_;
        } else {
          result.increaseRequest_ = increaseRequestBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance()) return this;
        if (askBuilder_ == null) {
          if (!other.ask_.isEmpty()) {
            if (ask_.isEmpty()) {
              ask_ = other.ask_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureAskIsMutable();
              ask_.addAll(other.ask_);
            }
            onChanged();
          }
        } else {
          if (!other.ask_.isEmpty()) {
            if (askBuilder_.isEmpty()) {
              askBuilder_.dispose();
              askBuilder_ = null;
              ask_ = other.ask_;
              bitField0_ = (bitField0_ & ~0x00000001);
              askBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getAskFieldBuilder() : null;
            } else {
              askBuilder_.addAllMessages(other.ask_);
            }
          }
        }
        if (releaseBuilder_ == null) {
          if (!other.release_.isEmpty()) {
            if (release_.isEmpty()) {
              release_ = other.release_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureReleaseIsMutable();
              release_.addAll(other.release_);
            }
            onChanged();
          }
        } else {
          if (!other.release_.isEmpty()) {
            if (releaseBuilder_.isEmpty()) {
              releaseBuilder_.dispose();
              releaseBuilder_ = null;
              release_ = other.release_;
              bitField0_ = (bitField0_ & ~0x00000002);
              releaseBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getReleaseFieldBuilder() : null;
            } else {
              releaseBuilder_.addAllMessages(other.release_);
            }
          }
        }
        if (other.hasBlacklistRequest()) {
          mergeBlacklistRequest(other.getBlacklistRequest());
        }
        if (other.hasResponseId()) {
          setResponseId(other.getResponseId());
        }
        if (other.hasProgress()) {
          setProgress(other.getProgress());
        }
        if (increaseRequestBuilder_ == null) {
          if (!other.increaseRequest_.isEmpty()) {
            if (increaseRequest_.isEmpty()) {
              increaseRequest_ = other.increaseRequest_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureIncreaseRequestIsMutable();
              increaseRequest_.addAll(other.increaseRequest_);
            }
            onChanged();
          }
        } else {
          if (!other.increaseRequest_.isEmpty()) {
            if (increaseRequestBuilder_.isEmpty()) {
              increaseRequestBuilder_.dispose();
              increaseRequestBuilder_ = null;
              increaseRequest_ = other.increaseRequest_;
              bitField0_ = (bitField0_ & ~0x00000020);
              increaseRequestBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getIncreaseRequestFieldBuilder() : null;
            } else {
              increaseRequestBuilder_.addAllMessages(other.increaseRequest_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ResourceRequestProto ask = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> ask_ =
        java.util.Collections.emptyList();
      private void ensureAskIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          ask_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto>(ask_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> askBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> getAskList() {
        if (askBuilder_ == null) {
          return java.util.Collections.unmodifiableList(ask_);
        } else {
          return askBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public int getAskCount() {
        if (askBuilder_ == null) {
          return ask_.size();
        } else {
          return askBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto getAsk(int index) {
        if (askBuilder_ == null) {
          return ask_.get(index);
        } else {
          return askBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder setAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto value) {
        if (askBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAskIsMutable();
          ask_.set(index, value);
          onChanged();
        } else {
          askBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder setAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder builderForValue) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.set(index, builderForValue.build());
          onChanged();
        } else {
          askBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto value) {
        if (askBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAskIsMutable();
          ask_.add(value);
          onChanged();
        } else {
          askBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto value) {
        if (askBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAskIsMutable();
          ask_.add(index, value);
          onChanged();
        } else {
          askBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder builderForValue) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.add(builderForValue.build());
          onChanged();
        } else {
          askBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder builderForValue) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.add(index, builderForValue.build());
          onChanged();
        } else {
          askBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAllAsk(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> values) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          super.addAll(values, ask_);
          onChanged();
        } else {
          askBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder clearAsk() {
        if (askBuilder_ == null) {
          ask_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          askBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder removeAsk(int index) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.remove(index);
          onChanged();
        } else {
          askBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder getAskBuilder(
          int index) {
        return getAskFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder getAskOrBuilder(
          int index) {
        if (askBuilder_ == null) {
          return ask_.get(index);  } else {
          return askBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
           getAskOrBuilderList() {
        if (askBuilder_ != null) {
          return askBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(ask_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder addAskBuilder() {
        return getAskFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder addAskBuilder(
          int index) {
        return getAskFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder> 
           getAskBuilderList() {
        return getAskFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
          getAskFieldBuilder() {
        if (askBuilder_ == null) {
          askBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder>(
                  ask_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          ask_ = null;
        }
        return askBuilder_;
      }

      // repeated .hadoop.yarn.ContainerIdProto release = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> release_ =
        java.util.Collections.emptyList();
      private void ensureReleaseIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          release_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(release_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> releaseBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getReleaseList() {
        if (releaseBuilder_ == null) {
          return java.util.Collections.unmodifiableList(release_);
        } else {
          return releaseBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public int getReleaseCount() {
        if (releaseBuilder_ == null) {
          return release_.size();
        } else {
          return releaseBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getRelease(int index) {
        if (releaseBuilder_ == null) {
          return release_.get(index);
        } else {
          return releaseBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder setRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (releaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReleaseIsMutable();
          release_.set(index, value);
          onChanged();
        } else {
          releaseBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder setRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.set(index, builderForValue.build());
          onChanged();
        } else {
          releaseBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (releaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReleaseIsMutable();
          release_.add(value);
          onChanged();
        } else {
          releaseBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (releaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReleaseIsMutable();
          release_.add(index, value);
          onChanged();
        } else {
          releaseBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.add(builderForValue.build());
          onChanged();
        } else {
          releaseBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.add(index, builderForValue.build());
          onChanged();
        } else {
          releaseBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addAllRelease(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          super.addAll(values, release_);
          onChanged();
        } else {
          releaseBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder clearRelease() {
        if (releaseBuilder_ == null) {
          release_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          releaseBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder removeRelease(int index) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.remove(index);
          onChanged();
        } else {
          releaseBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getReleaseBuilder(
          int index) {
        return getReleaseFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getReleaseOrBuilder(
          int index) {
        if (releaseBuilder_ == null) {
          return release_.get(index);  } else {
          return releaseBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getReleaseOrBuilderList() {
        if (releaseBuilder_ != null) {
          return releaseBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(release_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addReleaseBuilder() {
        return getReleaseFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addReleaseBuilder(
          int index) {
        return getReleaseFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getReleaseBuilderList() {
        return getReleaseFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getReleaseFieldBuilder() {
        if (releaseBuilder_ == null) {
          releaseBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  release_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          release_ = null;
        }
        return releaseBuilder_;
      }

      // optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder> blacklistRequestBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public boolean hasBlacklistRequest() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto getBlacklistRequest() {
        if (blacklistRequestBuilder_ == null) {
          return blacklistRequest_;
        } else {
          return blacklistRequestBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder setBlacklistRequest(org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto value) {
        if (blacklistRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          blacklistRequest_ = value;
          onChanged();
        } else {
          blacklistRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder setBlacklistRequest(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder builderForValue) {
        if (blacklistRequestBuilder_ == null) {
          blacklistRequest_ = builderForValue.build();
          onChanged();
        } else {
          blacklistRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder mergeBlacklistRequest(org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto value) {
        if (blacklistRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004) &&
              blacklistRequest_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance()) {
            blacklistRequest_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.newBuilder(blacklistRequest_).mergeFrom(value).buildPartial();
          } else {
            blacklistRequest_ = value;
          }
          onChanged();
        } else {
          blacklistRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder clearBlacklistRequest() {
        if (blacklistRequestBuilder_ == null) {
          blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
          onChanged();
        } else {
          blacklistRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder getBlacklistRequestBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getBlacklistRequestFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder getBlacklistRequestOrBuilder() {
        if (blacklistRequestBuilder_ != null) {
          return blacklistRequestBuilder_.getMessageOrBuilder();
        } else {
          return blacklistRequest_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder> 
          getBlacklistRequestFieldBuilder() {
        if (blacklistRequestBuilder_ == null) {
          blacklistRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder>(
                  blacklistRequest_,
                  getParentForChildren(),
                  isClean());
          blacklistRequest_ = null;
        }
        return blacklistRequestBuilder_;
      }

      // optional int32 response_id = 4;
      private int responseId_ ;
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public boolean hasResponseId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public int getResponseId() {
        return responseId_;
      }
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public Builder setResponseId(int value) {
        bitField0_ |= 0x00000008;
        responseId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public Builder clearResponseId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        responseId_ = 0;
        onChanged();
        return this;
      }

      // optional float progress = 5;
      private float progress_ ;
      /**
       * <code>optional float progress = 5;</code>
       */
      public boolean hasProgress() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional float progress = 5;</code>
       */
      public float getProgress() {
        return progress_;
      }
      /**
       * <code>optional float progress = 5;</code>
       */
      public Builder setProgress(float value) {
        bitField0_ |= 0x00000010;
        progress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float progress = 5;</code>
       */
      public Builder clearProgress() {
        bitField0_ = (bitField0_ & ~0x00000010);
        progress_ = 0F;
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> increaseRequest_ =
        java.util.Collections.emptyList();
      private void ensureIncreaseRequestIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          increaseRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto>(increaseRequest_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> increaseRequestBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> getIncreaseRequestList() {
        if (increaseRequestBuilder_ == null) {
          return java.util.Collections.unmodifiableList(increaseRequest_);
        } else {
          return increaseRequestBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public int getIncreaseRequestCount() {
        if (increaseRequestBuilder_ == null) {
          return increaseRequest_.size();
        } else {
          return increaseRequestBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto getIncreaseRequest(int index) {
        if (increaseRequestBuilder_ == null) {
          return increaseRequest_.get(index);
        } else {
          return increaseRequestBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder setIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto value) {
        if (increaseRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseRequestIsMutable();
          increaseRequest_.set(index, value);
          onChanged();
        } else {
          increaseRequestBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder setIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder builderForValue) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.set(index, builderForValue.build());
          onChanged();
        } else {
          increaseRequestBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto value) {
        if (increaseRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(value);
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto value) {
        if (increaseRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(index, value);
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder builderForValue) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(builderForValue.build());
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder builderForValue) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(index, builderForValue.build());
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addAllIncreaseRequest(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> values) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          super.addAll(values, increaseRequest_);
          onChanged();
        } else {
          increaseRequestBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder clearIncreaseRequest() {
        if (increaseRequestBuilder_ == null) {
          increaseRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          increaseRequestBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder removeIncreaseRequest(int index) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.remove(index);
          onChanged();
        } else {
          increaseRequestBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder getIncreaseRequestBuilder(
          int index) {
        return getIncreaseRequestFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder getIncreaseRequestOrBuilder(
          int index) {
        if (increaseRequestBuilder_ == null) {
          return increaseRequest_.get(index);  } else {
          return increaseRequestBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
           getIncreaseRequestOrBuilderList() {
        if (increaseRequestBuilder_ != null) {
          return increaseRequestBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(increaseRequest_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder addIncreaseRequestBuilder() {
        return getIncreaseRequestFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder addIncreaseRequestBuilder(
          int index) {
        return getIncreaseRequestFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder> 
           getIncreaseRequestBuilderList() {
        return getIncreaseRequestFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
          getIncreaseRequestFieldBuilder() {
        if (increaseRequestBuilder_ == null) {
          increaseRequestBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder>(
                  increaseRequest_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          increaseRequest_ = null;
        }
        return increaseRequestBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.AllocateRequestProto)
    }

    static {
      defaultInstance = new AllocateRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.AllocateRequestProto)
  }

  public interface NMTokenProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.NodeIdProto nodeId = 1;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    boolean hasNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder();

    // optional .hadoop.common.TokenProto token = 2;
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    boolean hasToken();
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getToken();
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getTokenOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.NMTokenProto}
   */
  public static final class NMTokenProto extends
      com.google.protobuf.GeneratedMessage
      implements NMTokenProtoOrBuilder {
    // Use NMTokenProto.newBuilder() to construct.
    private NMTokenProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NMTokenProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NMTokenProto defaultInstance;
    public static NMTokenProto getDefaultInstance() {
      return defaultInstance;
    }

    public NMTokenProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NMTokenProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = nodeId_.toBuilder();
              }
              nodeId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeId_);
                nodeId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = token_.toBuilder();
              }
              token_ = input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(token_);
                token_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder.class);
    }

    public static com.google.protobuf.Parser<NMTokenProto> PARSER =
        new com.google.protobuf.AbstractParser<NMTokenProto>() {
      public NMTokenProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NMTokenProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NMTokenProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.NodeIdProto nodeId = 1;
    public static final int NODEID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    public boolean hasNodeId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
      return nodeId_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
      return nodeId_;
    }

    // optional .hadoop.common.TokenProto token = 2;
    public static final int TOKEN_FIELD_NUMBER = 2;
    private org.apache.hadoop.security.proto.SecurityProtos.TokenProto token_;
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    public boolean hasToken() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getToken() {
      return token_;
    }
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getTokenOrBuilder() {
      return token_;
    }

    private void initFields() {
      nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
      token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasToken()) {
        if (!getToken().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, nodeId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, token_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, token_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto) obj;

      boolean result = true;
      result = result && (hasNodeId() == other.hasNodeId());
      if (hasNodeId()) {
        result = result && getNodeId()
            .equals(other.getNodeId());
      }
      result = result && (hasToken() == other.hasToken());
      if (hasToken()) {
        result = result && getToken()
            .equals(other.getToken());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNodeId()) {
        hash = (37 * hash) + NODEID_FIELD_NUMBER;
        hash = (53 * hash) + getNodeId().hashCode();
      }
      if (hasToken()) {
        hash = (37 * hash) + TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getToken().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NMTokenProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNodeIdFieldBuilder();
          getTokenFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (nodeIdBuilder_ == null) {
          nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (tokenBuilder_ == null) {
          token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
        } else {
          tokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (nodeIdBuilder_ == null) {
          result.nodeId_ = nodeId_;
        } else {
          result.nodeId_ = nodeIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (tokenBuilder_ == null) {
          result.token_ = token_;
        } else {
          result.token_ = tokenBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance()) return this;
        if (other.hasNodeId()) {
          mergeNodeId(other.getNodeId());
        }
        if (other.hasToken()) {
          mergeToken(other.getToken());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasToken()) {
          if (!getToken().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.NodeIdProto nodeId = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> nodeIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public boolean hasNodeId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
        if (nodeIdBuilder_ == null) {
          return nodeId_;
        } else {
          return nodeIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder setNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeId_ = value;
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder setNodeId(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder builderForValue) {
        if (nodeIdBuilder_ == null) {
          nodeId_ = builderForValue.build();
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder mergeNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              nodeId_ != org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance()) {
            nodeId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.newBuilder(nodeId_).mergeFrom(value).buildPartial();
          } else {
            nodeId_ = value;
          }
          onChanged();
        } else {
          nodeIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder clearNodeId() {
        if (nodeIdBuilder_ == null) {
          nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
          onChanged();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder getNodeIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNodeIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
        if (nodeIdBuilder_ != null) {
          return nodeIdBuilder_.getMessageOrBuilder();
        } else {
          return nodeId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> 
          getNodeIdFieldBuilder() {
        if (nodeIdBuilder_ == null) {
          nodeIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder>(
                  nodeId_,
                  getParentForChildren(),
                  isClean());
          nodeId_ = null;
        }
        return nodeIdBuilder_;
      }

      // optional .hadoop.common.TokenProto token = 2;
      private org.apache.hadoop.security.proto.SecurityProtos.TokenProto token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> tokenBuilder_;
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public boolean hasToken() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getToken() {
        if (tokenBuilder_ == null) {
          return token_;
        } else {
          return tokenBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder setToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (tokenBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          token_ = value;
          onChanged();
        } else {
          tokenBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder setToken(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (tokenBuilder_ == null) {
          token_ = builderForValue.build();
          onChanged();
        } else {
          tokenBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder mergeToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (tokenBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              token_ != org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance()) {
            token_ =
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.newBuilder(token_).mergeFrom(value).buildPartial();
          } else {
            token_ = value;
          }
          onChanged();
        } else {
          tokenBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder clearToken() {
        if (tokenBuilder_ == null) {
          token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
          onChanged();
        } else {
          tokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getTokenBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getTokenFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getTokenOrBuilder() {
        if (tokenBuilder_ != null) {
          return tokenBuilder_.getMessageOrBuilder();
        } else {
          return token_;
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getTokenFieldBuilder() {
        if (tokenBuilder_ == null) {
          tokenBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  token_,
                  getParentForChildren(),
                  isClean());
          token_ = null;
        }
        return tokenBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NMTokenProto)
    }

    static {
      defaultInstance = new NMTokenProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NMTokenProto)
  }

  public interface AllocateResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.AMCommandProto a_m_command = 1;
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    boolean hasAMCommand();
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto getAMCommand();

    // optional int32 response_id = 2;
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    boolean hasResponseId();
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    int getResponseId();

    // repeated .hadoop.yarn.ContainerProto allocated_containers = 3;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> 
        getAllocatedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    int getAllocatedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getAllocatedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> 
        getCompletedContainerStatusesList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getCompletedContainerStatuses(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    int getCompletedContainerStatusesCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getCompletedContainerStatusesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getCompletedContainerStatusesOrBuilder(
        int index);

    // optional .hadoop.yarn.ResourceProto limit = 5;
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    boolean hasLimit();
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getLimit();
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getLimitOrBuilder();

    // repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> 
        getUpdatedNodesList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getUpdatedNodes(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    int getUpdatedNodesCount();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getUpdatedNodesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getUpdatedNodesOrBuilder(
        int index);

    // optional int32 num_cluster_nodes = 7;
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    boolean hasNumClusterNodes();
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    int getNumClusterNodes();

    // optional .hadoop.yarn.PreemptionMessageProto preempt = 8;
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    boolean hasPreempt();
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto getPreempt();
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder getPreemptOrBuilder();

    // repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> 
        getNmTokensList();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokens(int index);
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    int getNmTokensCount();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
        getNmTokensOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> 
        getIncreasedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto getIncreasedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    int getIncreasedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
        getIncreasedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder getIncreasedContainersOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> 
        getDecreasedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto getDecreasedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    int getDecreasedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
        getDecreasedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder getDecreasedContainersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.AllocateResponseProto}
   */
  public static final class AllocateResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocateResponseProtoOrBuilder {
    // Use AllocateResponseProto.newBuilder() to construct.
    private AllocateResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocateResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocateResponseProto defaultInstance;
    public static AllocateResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocateResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocateResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto value = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                aMCommand_ = value;
              }
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              responseId_ = input.readInt32();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                allocatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              allocatedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                completedContainerStatuses_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>();
                mutable_bitField0_ |= 0x00000008;
              }
              completedContainerStatuses_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) == 0x00000004)) {
                subBuilder = limit_.toBuilder();
              }
              limit_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(limit_);
                limit_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                updatedNodes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              updatedNodes_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000008;
              numClusterNodes_ = input.readInt32();
              break;
            }
            case 66: {
              org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) == 0x00000010)) {
                subBuilder = preempt_.toBuilder();
              }
              preempt_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(preempt_);
                preempt_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
                nmTokens_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto>();
                mutable_bitField0_ |= 0x00000100;
              }
              nmTokens_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.PARSER, extensionRegistry));
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
                increasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto>();
                mutable_bitField0_ |= 0x00000200;
              }
              increasedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.PARSER, extensionRegistry));
              break;
            }
            case 90: {
              if (!((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
                decreasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto>();
                mutable_bitField0_ |= 0x00000400;
              }
              decreasedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          allocatedContainers_ = java.util.Collections.unmodifiableList(allocatedContainers_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          completedContainerStatuses_ = java.util.Collections.unmodifiableList(completedContainerStatuses_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          updatedNodes_ = java.util.Collections.unmodifiableList(updatedNodes_);
        }
        if (((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
          nmTokens_ = java.util.Collections.unmodifiableList(nmTokens_);
        }
        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
          increasedContainers_ = java.util.Collections.unmodifiableList(increasedContainers_);
        }
        if (((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
          decreasedContainers_ = java.util.Collections.unmodifiableList(decreasedContainers_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocateResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocateResponseProto>() {
      public AllocateResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocateResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocateResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.AMCommandProto a_m_command = 1;
    public static final int A_M_COMMAND_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto aMCommand_;
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    public boolean hasAMCommand() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto getAMCommand() {
      return aMCommand_;
    }

    // optional int32 response_id = 2;
    public static final int RESPONSE_ID_FIELD_NUMBER = 2;
    private int responseId_;
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    public boolean hasResponseId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    public int getResponseId() {
      return responseId_;
    }

    // repeated .hadoop.yarn.ContainerProto allocated_containers = 3;
    public static final int ALLOCATED_CONTAINERS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> allocatedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getAllocatedContainersList() {
      return allocatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getAllocatedContainersOrBuilderList() {
      return allocatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public int getAllocatedContainersCount() {
      return allocatedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index) {
      return allocatedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
        int index) {
      return allocatedContainers_.get(index);
    }

    // repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;
    public static final int COMPLETED_CONTAINER_STATUSES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> completedContainerStatuses_;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getCompletedContainerStatusesList() {
      return completedContainerStatuses_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getCompletedContainerStatusesOrBuilderList() {
      return completedContainerStatuses_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public int getCompletedContainerStatusesCount() {
      return completedContainerStatuses_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getCompletedContainerStatuses(int index) {
      return completedContainerStatuses_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getCompletedContainerStatusesOrBuilder(
        int index) {
      return completedContainerStatuses_.get(index);
    }

    // optional .hadoop.yarn.ResourceProto limit = 5;
    public static final int LIMIT_FIELD_NUMBER = 5;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto limit_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    public boolean hasLimit() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getLimit() {
      return limit_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getLimitOrBuilder() {
      return limit_;
    }

    // repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;
    public static final int UPDATED_NODES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> updatedNodes_;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getUpdatedNodesList() {
      return updatedNodes_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getUpdatedNodesOrBuilderList() {
      return updatedNodes_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public int getUpdatedNodesCount() {
      return updatedNodes_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getUpdatedNodes(int index) {
      return updatedNodes_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getUpdatedNodesOrBuilder(
        int index) {
      return updatedNodes_.get(index);
    }

    // optional int32 num_cluster_nodes = 7;
    public static final int NUM_CLUSTER_NODES_FIELD_NUMBER = 7;
    private int numClusterNodes_;
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    public boolean hasNumClusterNodes() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    public int getNumClusterNodes() {
      return numClusterNodes_;
    }

    // optional .hadoop.yarn.PreemptionMessageProto preempt = 8;
    public static final int PREEMPT_FIELD_NUMBER = 8;
    private org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto preempt_;
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    public boolean hasPreempt() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto getPreempt() {
      return preempt_;
    }
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder getPreemptOrBuilder() {
      return preempt_;
    }

    // repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;
    public static final int NM_TOKENS_FIELD_NUMBER = 9;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> nmTokens_;
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> getNmTokensList() {
      return nmTokens_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
        getNmTokensOrBuilderList() {
      return nmTokens_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public int getNmTokensCount() {
      return nmTokens_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokens(int index) {
      return nmTokens_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensOrBuilder(
        int index) {
      return nmTokens_.get(index);
    }

    // repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;
    public static final int INCREASED_CONTAINERS_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> increasedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> getIncreasedContainersList() {
      return increasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
        getIncreasedContainersOrBuilderList() {
      return increasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public int getIncreasedContainersCount() {
      return increasedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto getIncreasedContainers(int index) {
      return increasedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder getIncreasedContainersOrBuilder(
        int index) {
      return increasedContainers_.get(index);
    }

    // repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;
    public static final int DECREASED_CONTAINERS_FIELD_NUMBER = 11;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> decreasedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> getDecreasedContainersList() {
      return decreasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
        getDecreasedContainersOrBuilderList() {
      return decreasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public int getDecreasedContainersCount() {
      return decreasedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto getDecreasedContainers(int index) {
      return decreasedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder getDecreasedContainersOrBuilder(
        int index) {
      return decreasedContainers_.get(index);
    }

    private void initFields() {
      aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
      responseId_ = 0;
      allocatedContainers_ = java.util.Collections.emptyList();
      completedContainerStatuses_ = java.util.Collections.emptyList();
      limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      updatedNodes_ = java.util.Collections.emptyList();
      numClusterNodes_ = 0;
      preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
      nmTokens_ = java.util.Collections.emptyList();
      increasedContainers_ = java.util.Collections.emptyList();
      decreasedContainers_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getAllocatedContainersCount(); i++) {
        if (!getAllocatedContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getNmTokensCount(); i++) {
        if (!getNmTokens(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getIncreasedContainersCount(); i++) {
        if (!getIncreasedContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, aMCommand_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, responseId_);
      }
      for (int i = 0; i < allocatedContainers_.size(); i++) {
        output.writeMessage(3, allocatedContainers_.get(i));
      }
      for (int i = 0; i < completedContainerStatuses_.size(); i++) {
        output.writeMessage(4, completedContainerStatuses_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeMessage(5, limit_);
      }
      for (int i = 0; i < updatedNodes_.size(); i++) {
        output.writeMessage(6, updatedNodes_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt32(7, numClusterNodes_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(8, preempt_);
      }
      for (int i = 0; i < nmTokens_.size(); i++) {
        output.writeMessage(9, nmTokens_.get(i));
      }
      for (int i = 0; i < increasedContainers_.size(); i++) {
        output.writeMessage(10, increasedContainers_.get(i));
      }
      for (int i = 0; i < decreasedContainers_.size(); i++) {
        output.writeMessage(11, decreasedContainers_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, aMCommand_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, responseId_);
      }
      for (int i = 0; i < allocatedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, allocatedContainers_.get(i));
      }
      for (int i = 0; i < completedContainerStatuses_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, completedContainerStatuses_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, limit_);
      }
      for (int i = 0; i < updatedNodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, updatedNodes_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, numClusterNodes_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, preempt_);
      }
      for (int i = 0; i < nmTokens_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, nmTokens_.get(i));
      }
      for (int i = 0; i < increasedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, increasedContainers_.get(i));
      }
      for (int i = 0; i < decreasedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, decreasedContainers_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto) obj;

      boolean result = true;
      result = result && (hasAMCommand() == other.hasAMCommand());
      if (hasAMCommand()) {
        result = result &&
            (getAMCommand() == other.getAMCommand());
      }
      result = result && (hasResponseId() == other.hasResponseId());
      if (hasResponseId()) {
        result = result && (getResponseId()
            == other.getResponseId());
      }
      result = result && getAllocatedContainersList()
          .equals(other.getAllocatedContainersList());
      result = result && getCompletedContainerStatusesList()
          .equals(other.getCompletedContainerStatusesList());
      result = result && (hasLimit() == other.hasLimit());
      if (hasLimit()) {
        result = result && getLimit()
            .equals(other.getLimit());
      }
      result = result && getUpdatedNodesList()
          .equals(other.getUpdatedNodesList());
      result = result && (hasNumClusterNodes() == other.hasNumClusterNodes());
      if (hasNumClusterNodes()) {
        result = result && (getNumClusterNodes()
            == other.getNumClusterNodes());
      }
      result = result && (hasPreempt() == other.hasPreempt());
      if (hasPreempt()) {
        result = result && getPreempt()
            .equals(other.getPreempt());
      }
      result = result && getNmTokensList()
          .equals(other.getNmTokensList());
      result = result && getIncreasedContainersList()
          .equals(other.getIncreasedContainersList());
      result = result && getDecreasedContainersList()
          .equals(other.getDecreasedContainersList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasAMCommand()) {
        hash = (37 * hash) + A_M_COMMAND_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getAMCommand());
      }
      if (hasResponseId()) {
        hash = (37 * hash) + RESPONSE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getResponseId();
      }
      if (getAllocatedContainersCount() > 0) {
        hash = (37 * hash) + ALLOCATED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getAllocatedContainersList().hashCode();
      }
      if (getCompletedContainerStatusesCount() > 0) {
        hash = (37 * hash) + COMPLETED_CONTAINER_STATUSES_FIELD_NUMBER;
        hash = (53 * hash) + getCompletedContainerStatusesList().hashCode();
      }
      if (hasLimit()) {
        hash = (37 * hash) + LIMIT_FIELD_NUMBER;
        hash = (53 * hash) + getLimit().hashCode();
      }
      if (getUpdatedNodesCount() > 0) {
        hash = (37 * hash) + UPDATED_NODES_FIELD_NUMBER;
        hash = (53 * hash) + getUpdatedNodesList().hashCode();
      }
      if (hasNumClusterNodes()) {
        hash = (37 * hash) + NUM_CLUSTER_NODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumClusterNodes();
      }
      if (hasPreempt()) {
        hash = (37 * hash) + PREEMPT_FIELD_NUMBER;
        hash = (53 * hash) + getPreempt().hashCode();
      }
      if (getNmTokensCount() > 0) {
        hash = (37 * hash) + NM_TOKENS_FIELD_NUMBER;
        hash = (53 * hash) + getNmTokensList().hashCode();
      }
      if (getIncreasedContainersCount() > 0) {
        hash = (37 * hash) + INCREASED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getIncreasedContainersList().hashCode();
      }
      if (getDecreasedContainersCount() > 0) {
        hash = (37 * hash) + DECREASED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getDecreasedContainersList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.AllocateResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getAllocatedContainersFieldBuilder();
          getCompletedContainerStatusesFieldBuilder();
          getLimitFieldBuilder();
          getUpdatedNodesFieldBuilder();
          getPreemptFieldBuilder();
          getNmTokensFieldBuilder();
          getIncreasedContainersFieldBuilder();
          getDecreasedContainersFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
        bitField0_ = (bitField0_ & ~0x00000001);
        responseId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (allocatedContainersBuilder_ == null) {
          allocatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          allocatedContainersBuilder_.clear();
        }
        if (completedContainerStatusesBuilder_ == null) {
          completedContainerStatuses_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          completedContainerStatusesBuilder_.clear();
        }
        if (limitBuilder_ == null) {
          limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          limitBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        if (updatedNodesBuilder_ == null) {
          updatedNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          updatedNodesBuilder_.clear();
        }
        numClusterNodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (preemptBuilder_ == null) {
          preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
        } else {
          preemptBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (nmTokensBuilder_ == null) {
          nmTokens_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          nmTokensBuilder_.clear();
        }
        if (increasedContainersBuilder_ == null) {
          increasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          increasedContainersBuilder_.clear();
        }
        if (decreasedContainersBuilder_ == null) {
          decreasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
        } else {
          decreasedContainersBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.aMCommand_ = aMCommand_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.responseId_ = responseId_;
        if (allocatedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            allocatedContainers_ = java.util.Collections.unmodifiableList(allocatedContainers_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.allocatedContainers_ = allocatedContainers_;
        } else {
          result.allocatedContainers_ = allocatedContainersBuilder_.build();
        }
        if (completedContainerStatusesBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            completedContainerStatuses_ = java.util.Collections.unmodifiableList(completedContainerStatuses_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.completedContainerStatuses_ = completedContainerStatuses_;
        } else {
          result.completedContainerStatuses_ = completedContainerStatusesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000004;
        }
        if (limitBuilder_ == null) {
          result.limit_ = limit_;
        } else {
          result.limit_ = limitBuilder_.build();
        }
        if (updatedNodesBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            updatedNodes_ = java.util.Collections.unmodifiableList(updatedNodes_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.updatedNodes_ = updatedNodes_;
        } else {
          result.updatedNodes_ = updatedNodesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000008;
        }
        result.numClusterNodes_ = numClusterNodes_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000010;
        }
        if (preemptBuilder_ == null) {
          result.preempt_ = preempt_;
        } else {
          result.preempt_ = preemptBuilder_.build();
        }
        if (nmTokensBuilder_ == null) {
          if (((bitField0_ & 0x00000100) == 0x00000100)) {
            nmTokens_ = java.util.Collections.unmodifiableList(nmTokens_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.nmTokens_ = nmTokens_;
        } else {
          result.nmTokens_ = nmTokensBuilder_.build();
        }
        if (increasedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200)) {
            increasedContainers_ = java.util.Collections.unmodifiableList(increasedContainers_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.increasedContainers_ = increasedContainers_;
        } else {
          result.increasedContainers_ = increasedContainersBuilder_.build();
        }
        if (decreasedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000400) == 0x00000400)) {
            decreasedContainers_ = java.util.Collections.unmodifiableList(decreasedContainers_);
            bitField0_ = (bitField0_ & ~0x00000400);
          }
          result.decreasedContainers_ = decreasedContainers_;
        } else {
          result.decreasedContainers_ = decreasedContainersBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance()) return this;
        if (other.hasAMCommand()) {
          setAMCommand(other.getAMCommand());
        }
        if (other.hasResponseId()) {
          setResponseId(other.getResponseId());
        }
        if (allocatedContainersBuilder_ == null) {
          if (!other.allocatedContainers_.isEmpty()) {
            if (allocatedContainers_.isEmpty()) {
              allocatedContainers_ = other.allocatedContainers_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureAllocatedContainersIsMutable();
              allocatedContainers_.addAll(other.allocatedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.allocatedContainers_.isEmpty()) {
            if (allocatedContainersBuilder_.isEmpty()) {
              allocatedContainersBuilder_.dispose();
              allocatedContainersBuilder_ = null;
              allocatedContainers_ = other.allocatedContainers_;
              bitField0_ = (bitField0_ & ~0x00000004);
              allocatedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getAllocatedContainersFieldBuilder() : null;
            } else {
              allocatedContainersBuilder_.addAllMessages(other.allocatedContainers_);
            }
          }
        }
        if (completedContainerStatusesBuilder_ == null) {
          if (!other.completedContainerStatuses_.isEmpty()) {
            if (completedContainerStatuses_.isEmpty()) {
              completedContainerStatuses_ = other.completedContainerStatuses_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureCompletedContainerStatusesIsMutable();
              completedContainerStatuses_.addAll(other.completedContainerStatuses_);
            }
            onChanged();
          }
        } else {
          if (!other.completedContainerStatuses_.isEmpty()) {
            if (completedContainerStatusesBuilder_.isEmpty()) {
              completedContainerStatusesBuilder_.dispose();
              completedContainerStatusesBuilder_ = null;
              completedContainerStatuses_ = other.completedContainerStatuses_;
              bitField0_ = (bitField0_ & ~0x00000008);
              completedContainerStatusesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getCompletedContainerStatusesFieldBuilder() : null;
            } else {
              completedContainerStatusesBuilder_.addAllMessages(other.completedContainerStatuses_);
            }
          }
        }
        if (other.hasLimit()) {
          mergeLimit(other.getLimit());
        }
        if (updatedNodesBuilder_ == null) {
          if (!other.updatedNodes_.isEmpty()) {
            if (updatedNodes_.isEmpty()) {
              updatedNodes_ = other.updatedNodes_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureUpdatedNodesIsMutable();
              updatedNodes_.addAll(other.updatedNodes_);
            }
            onChanged();
          }
        } else {
          if (!other.updatedNodes_.isEmpty()) {
            if (updatedNodesBuilder_.isEmpty()) {
              updatedNodesBuilder_.dispose();
              updatedNodesBuilder_ = null;
              updatedNodes_ = other.updatedNodes_;
              bitField0_ = (bitField0_ & ~0x00000020);
              updatedNodesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getUpdatedNodesFieldBuilder() : null;
            } else {
              updatedNodesBuilder_.addAllMessages(other.updatedNodes_);
            }
          }
        }
        if (other.hasNumClusterNodes()) {
          setNumClusterNodes(other.getNumClusterNodes());
        }
        if (other.hasPreempt()) {
          mergePreempt(other.getPreempt());
        }
        if (nmTokensBuilder_ == null) {
          if (!other.nmTokens_.isEmpty()) {
            if (nmTokens_.isEmpty()) {
              nmTokens_ = other.nmTokens_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureNmTokensIsMutable();
              nmTokens_.addAll(other.nmTokens_);
            }
            onChanged();
          }
        } else {
          if (!other.nmTokens_.isEmpty()) {
            if (nmTokensBuilder_.isEmpty()) {
              nmTokensBuilder_.dispose();
              nmTokensBuilder_ = null;
              nmTokens_ = other.nmTokens_;
              bitField0_ = (bitField0_ & ~0x00000100);
              nmTokensBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNmTokensFieldBuilder() : null;
            } else {
              nmTokensBuilder_.addAllMessages(other.nmTokens_);
            }
          }
        }
        if (increasedContainersBuilder_ == null) {
          if (!other.increasedContainers_.isEmpty()) {
            if (increasedContainers_.isEmpty()) {
              increasedContainers_ = other.increasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureIncreasedContainersIsMutable();
              increasedContainers_.addAll(other.increasedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.increasedContainers_.isEmpty()) {
            if (increasedContainersBuilder_.isEmpty()) {
              increasedContainersBuilder_.dispose();
              increasedContainersBuilder_ = null;
              increasedContainers_ = other.increasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000200);
              increasedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getIncreasedContainersFieldBuilder() : null;
            } else {
              increasedContainersBuilder_.addAllMessages(other.increasedContainers_);
            }
          }
        }
        if (decreasedContainersBuilder_ == null) {
          if (!other.decreasedContainers_.isEmpty()) {
            if (decreasedContainers_.isEmpty()) {
              decreasedContainers_ = other.decreasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000400);
            } else {
              ensureDecreasedContainersIsMutable();
              decreasedContainers_.addAll(other.decreasedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.decreasedContainers_.isEmpty()) {
            if (decreasedContainersBuilder_.isEmpty()) {
              decreasedContainersBuilder_.dispose();
              decreasedContainersBuilder_ = null;
              decreasedContainers_ = other.decreasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000400);
              decreasedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getDecreasedContainersFieldBuilder() : null;
            } else {
              decreasedContainersBuilder_.addAllMessages(other.decreasedContainers_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getAllocatedContainersCount(); i++) {
          if (!getAllocatedContainers(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getNmTokensCount(); i++) {
          if (!getNmTokens(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getIncreasedContainersCount(); i++) {
          if (!getIncreasedContainers(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.AMCommandProto a_m_command = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public boolean hasAMCommand() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto getAMCommand() {
        return aMCommand_;
      }
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public Builder setAMCommand(org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        aMCommand_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public Builder clearAMCommand() {
        bitField0_ = (bitField0_ & ~0x00000001);
        aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
        onChanged();
        return this;
      }

      // optional int32 response_id = 2;
      private int responseId_ ;
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public boolean hasResponseId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public int getResponseId() {
        return responseId_;
      }
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public Builder setResponseId(int value) {
        bitField0_ |= 0x00000002;
        responseId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public Builder clearResponseId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        responseId_ = 0;
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.ContainerProto allocated_containers = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> allocatedContainers_ =
        java.util.Collections.emptyList();
      private void ensureAllocatedContainersIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          allocatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>(allocatedContainers_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> allocatedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getAllocatedContainersList() {
        if (allocatedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(allocatedContainers_);
        } else {
          return allocatedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public int getAllocatedContainersCount() {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.size();
        } else {
          return allocatedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index) {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.get(index);
        } else {
          return allocatedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder setAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.set(index, value);
          onChanged();
        } else {
          allocatedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder setAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(value);
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(index, value);
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllAllocatedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> values) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          super.addAll(values, allocatedContainers_);
          onChanged();
        } else {
          allocatedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder clearAllocatedContainers() {
        if (allocatedContainersBuilder_ == null) {
          allocatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          allocatedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder removeAllocatedContainers(int index) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.remove(index);
          onChanged();
        } else {
          allocatedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getAllocatedContainersBuilder(
          int index) {
        return getAllocatedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
          int index) {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.get(index);  } else {
          return allocatedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
           getAllocatedContainersOrBuilderList() {
        if (allocatedContainersBuilder_ != null) {
          return allocatedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(allocatedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addAllocatedContainersBuilder() {
        return getAllocatedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addAllocatedContainersBuilder(
          int index) {
        return getAllocatedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder> 
           getAllocatedContainersBuilderList() {
        return getAllocatedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getAllocatedContainersFieldBuilder() {
        if (allocatedContainersBuilder_ == null) {
          allocatedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  allocatedContainers_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          allocatedContainers_ = null;
        }
        return allocatedContainersBuilder_;
      }

      // repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> completedContainerStatuses_ =
        java.util.Collections.emptyList();
      private void ensureCompletedContainerStatusesIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          completedContainerStatuses_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>(completedContainerStatuses_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> completedContainerStatusesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getCompletedContainerStatusesList() {
        if (completedContainerStatusesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(completedContainerStatuses_);
        } else {
          return completedContainerStatusesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public int getCompletedContainerStatusesCount() {
        if (completedContainerStatusesBuilder_ == null) {
          return completedContainerStatuses_.size();
        } else {
          return completedContainerStatusesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getCompletedContainerStatuses(int index) {
        if (completedContainerStatusesBuilder_ == null) {
          return completedContainerStatuses_.get(index);
        } else {
          return completedContainerStatusesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder setCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (completedContainerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.set(index, value);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder setCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.set(index, builderForValue.build());
          onChanged();
        } else {
          completedContainerStatusesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (completedContainerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(value);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (completedContainerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(index, value);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(builderForValue.build());
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(index, builderForValue.build());
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addAllCompletedContainerStatuses(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> values) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          super.addAll(values, completedContainerStatuses_);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder clearCompletedContainerStatuses() {
        if (completedContainerStatusesBuilder_ == null) {
          completedContainerStatuses_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder removeCompletedContainerStatuses(int index) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.remove(index);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder getCompletedContainerStatusesBuilder(
          int index) {
        return getCompletedContainerStatusesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getCompletedContainerStatusesOrBuilder(
          int index) {
        if (completedContainerStatusesBuilder_ == null) {
          return completedContainerStatuses_.get(index);  } else {
          return completedContainerStatusesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
           getCompletedContainerStatusesOrBuilderList() {
        if (completedContainerStatusesBuilder_ != null) {
          return completedContainerStatusesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(completedContainerStatuses_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addCompletedContainerStatusesBuilder() {
        return getCompletedContainerStatusesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addCompletedContainerStatusesBuilder(
          int index) {
        return getCompletedContainerStatusesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder> 
           getCompletedContainerStatusesBuilderList() {
        return getCompletedContainerStatusesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
          getCompletedContainerStatusesFieldBuilder() {
        if (completedContainerStatusesBuilder_ == null) {
          completedContainerStatusesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder>(
                  completedContainerStatuses_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          completedContainerStatuses_ = null;
        }
        return completedContainerStatusesBuilder_;
      }

      // optional .hadoop.yarn.ResourceProto limit = 5;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> limitBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public boolean hasLimit() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getLimit() {
        if (limitBuilder_ == null) {
          return limit_;
        } else {
          return limitBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder setLimit(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (limitBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          limit_ = value;
          onChanged();
        } else {
          limitBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder setLimit(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (limitBuilder_ == null) {
          limit_ = builderForValue.build();
          onChanged();
        } else {
          limitBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder mergeLimit(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (limitBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              limit_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            limit_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(limit_).mergeFrom(value).buildPartial();
          } else {
            limit_ = value;
          }
          onChanged();
        } else {
          limitBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder clearLimit() {
        if (limitBuilder_ == null) {
          limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          limitBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getLimitBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getLimitFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getLimitOrBuilder() {
        if (limitBuilder_ != null) {
          return limitBuilder_.getMessageOrBuilder();
        } else {
          return limit_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getLimitFieldBuilder() {
        if (limitBuilder_ == null) {
          limitBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  limit_,
                  getParentForChildren(),
                  isClean());
          limit_ = null;
        }
        return limitBuilder_;
      }

      // repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> updatedNodes_ =
        java.util.Collections.emptyList();
      private void ensureUpdatedNodesIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          updatedNodes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>(updatedNodes_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> updatedNodesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getUpdatedNodesList() {
        if (updatedNodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(updatedNodes_);
        } else {
          return updatedNodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public int getUpdatedNodesCount() {
        if (updatedNodesBuilder_ == null) {
          return updatedNodes_.size();
        } else {
          return updatedNodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getUpdatedNodes(int index) {
        if (updatedNodesBuilder_ == null) {
          return updatedNodes_.get(index);
        } else {
          return updatedNodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder setUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (updatedNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedNodesIsMutable();
          updatedNodes_.set(index, value);
          onChanged();
        } else {
          updatedNodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder setUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          updatedNodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (updatedNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(value);
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (updatedNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(index, value);
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(builderForValue.build());
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addAllUpdatedNodes(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> values) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          super.addAll(values, updatedNodes_);
          onChanged();
        } else {
          updatedNodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder clearUpdatedNodes() {
        if (updatedNodesBuilder_ == null) {
          updatedNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          updatedNodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder removeUpdatedNodes(int index) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.remove(index);
          onChanged();
        } else {
          updatedNodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder getUpdatedNodesBuilder(
          int index) {
        return getUpdatedNodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getUpdatedNodesOrBuilder(
          int index) {
        if (updatedNodesBuilder_ == null) {
          return updatedNodes_.get(index);  } else {
          return updatedNodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
           getUpdatedNodesOrBuilderList() {
        if (updatedNodesBuilder_ != null) {
          return updatedNodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(updatedNodes_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addUpdatedNodesBuilder() {
        return getUpdatedNodesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addUpdatedNodesBuilder(
          int index) {
        return getUpdatedNodesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder> 
           getUpdatedNodesBuilderList() {
        return getUpdatedNodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
          getUpdatedNodesFieldBuilder() {
        if (updatedNodesBuilder_ == null) {
          updatedNodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder>(
                  updatedNodes_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          updatedNodes_ = null;
        }
        return updatedNodesBuilder_;
      }

      // optional int32 num_cluster_nodes = 7;
      private int numClusterNodes_ ;
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public boolean hasNumClusterNodes() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public int getNumClusterNodes() {
        return numClusterNodes_;
      }
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public Builder setNumClusterNodes(int value) {
        bitField0_ |= 0x00000040;
        numClusterNodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public Builder clearNumClusterNodes() {
        bitField0_ = (bitField0_ & ~0x00000040);
        numClusterNodes_ = 0;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.PreemptionMessageProto preempt = 8;
      private org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder> preemptBuilder_;
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public boolean hasPreempt() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto getPreempt() {
        if (preemptBuilder_ == null) {
          return preempt_;
        } else {
          return preemptBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder setPreempt(org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto value) {
        if (preemptBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          preempt_ = value;
          onChanged();
        } else {
          preemptBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder setPreempt(
          org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder builderForValue) {
        if (preemptBuilder_ == null) {
          preempt_ = builderForValue.build();
          onChanged();
        } else {
          preemptBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder mergePreempt(org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto value) {
        if (preemptBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080) &&
              preempt_ != org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance()) {
            preempt_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.newBuilder(preempt_).mergeFrom(value).buildPartial();
          } else {
            preempt_ = value;
          }
          onChanged();
        } else {
          preemptBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder clearPreempt() {
        if (preemptBuilder_ == null) {
          preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
          onChanged();
        } else {
          preemptBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder getPreemptBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getPreemptFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder getPreemptOrBuilder() {
        if (preemptBuilder_ != null) {
          return preemptBuilder_.getMessageOrBuilder();
        } else {
          return preempt_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder> 
          getPreemptFieldBuilder() {
        if (preemptBuilder_ == null) {
          preemptBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder>(
                  preempt_,
                  getParentForChildren(),
                  isClean());
          preempt_ = null;
        }
        return preemptBuilder_;
      }

      // repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> nmTokens_ =
        java.util.Collections.emptyList();
      private void ensureNmTokensIsMutable() {
        if (!((bitField0_ & 0x00000100) == 0x00000100)) {
          nmTokens_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto>(nmTokens_);
          bitField0_ |= 0x00000100;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> nmTokensBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> getNmTokensList() {
        if (nmTokensBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nmTokens_);
        } else {
          return nmTokensBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public int getNmTokensCount() {
        if (nmTokensBuilder_ == null) {
          return nmTokens_.size();
        } else {
          return nmTokensBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokens(int index) {
        if (nmTokensBuilder_ == null) {
          return nmTokens_.get(index);
        } else {
          return nmTokensBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder setNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensIsMutable();
          nmTokens_.set(index, value);
          onChanged();
        } else {
          nmTokensBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder setNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.set(index, builderForValue.build());
          onChanged();
        } else {
          nmTokensBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensIsMutable();
          nmTokens_.add(value);
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensIsMutable();
          nmTokens_.add(index, value);
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.add(builderForValue.build());
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.add(index, builderForValue.build());
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addAllNmTokens(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> values) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          super.addAll(values, nmTokens_);
          onChanged();
        } else {
          nmTokensBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder clearNmTokens() {
        if (nmTokensBuilder_ == null) {
          nmTokens_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          nmTokensBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder removeNmTokens(int index) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.remove(index);
          onChanged();
        } else {
          nmTokensBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder getNmTokensBuilder(
          int index) {
        return getNmTokensFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensOrBuilder(
          int index) {
        if (nmTokensBuilder_ == null) {
          return nmTokens_.get(index);  } else {
          return nmTokensBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
           getNmTokensOrBuilderList() {
        if (nmTokensBuilder_ != null) {
          return nmTokensBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nmTokens_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder addNmTokensBuilder() {
        return getNmTokensFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder addNmTokensBuilder(
          int index) {
        return getNmTokensFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder> 
           getNmTokensBuilderList() {
        return getNmTokensFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
          getNmTokensFieldBuilder() {
        if (nmTokensBuilder_ == null) {
          nmTokensBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder>(
                  nmTokens_,
                  ((bitField0_ & 0x00000100) == 0x00000100),
                  getParentForChildren(),
                  isClean());
          nmTokens_ = null;
        }
        return nmTokensBuilder_;
      }

      // repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> increasedContainers_ =
        java.util.Collections.emptyList();
      private void ensureIncreasedContainersIsMutable() {
        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
          increasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto>(increasedContainers_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> increasedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> getIncreasedContainersList() {
        if (increasedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(increasedContainers_);
        } else {
          return increasedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public int getIncreasedContainersCount() {
        if (increasedContainersBuilder_ == null) {
          return increasedContainers_.size();
        } else {
          return increasedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto getIncreasedContainers(int index) {
        if (increasedContainersBuilder_ == null) {
          return increasedContainers_.get(index);
        } else {
          return increasedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder setIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto value) {
        if (increasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreasedContainersIsMutable();
          increasedContainers_.set(index, value);
          onChanged();
        } else {
          increasedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder setIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder builderForValue) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          increasedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto value) {
        if (increasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(value);
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto value) {
        if (increasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(index, value);
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder builderForValue) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder builderForValue) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addAllIncreasedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> values) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          super.addAll(values, increasedContainers_);
          onChanged();
        } else {
          increasedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder clearIncreasedContainers() {
        if (increasedContainersBuilder_ == null) {
          increasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          increasedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder removeIncreasedContainers(int index) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.remove(index);
          onChanged();
        } else {
          increasedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder getIncreasedContainersBuilder(
          int index) {
        return getIncreasedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder getIncreasedContainersOrBuilder(
          int index) {
        if (increasedContainersBuilder_ == null) {
          return increasedContainers_.get(index);  } else {
          return increasedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
           getIncreasedContainersOrBuilderList() {
        if (increasedContainersBuilder_ != null) {
          return increasedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(increasedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder addIncreasedContainersBuilder() {
        return getIncreasedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder addIncreasedContainersBuilder(
          int index) {
        return getIncreasedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder> 
           getIncreasedContainersBuilderList() {
        return getIncreasedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
          getIncreasedContainersFieldBuilder() {
        if (increasedContainersBuilder_ == null) {
          increasedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder>(
                  increasedContainers_,
                  ((bitField0_ & 0x00000200) == 0x00000200),
                  getParentForChildren(),
                  isClean());
          increasedContainers_ = null;
        }
        return increasedContainersBuilder_;
      }

      // repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> decreasedContainers_ =
        java.util.Collections.emptyList();
      private void ensureDecreasedContainersIsMutable() {
        if (!((bitField0_ & 0x00000400) == 0x00000400)) {
          decreasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto>(decreasedContainers_);
          bitField0_ |= 0x00000400;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> decreasedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> getDecreasedContainersList() {
        if (decreasedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(decreasedContainers_);
        } else {
          return decreasedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public int getDecreasedContainersCount() {
        if (decreasedContainersBuilder_ == null) {
          return decreasedContainers_.size();
        } else {
          return decreasedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto getDecreasedContainers(int index) {
        if (decreasedContainersBuilder_ == null) {
          return decreasedContainers_.get(index);
        } else {
          return decreasedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder setDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto value) {
        if (decreasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.set(index, value);
          onChanged();
        } else {
          decreasedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder setDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder builderForValue) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          decreasedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto value) {
        if (decreasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(value);
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto value) {
        if (decreasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(index, value);
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder builderForValue) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder builderForValue) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addAllDecreasedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> values) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          super.addAll(values, decreasedContainers_);
          onChanged();
        } else {
          decreasedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder clearDecreasedContainers() {
        if (decreasedContainersBuilder_ == null) {
          decreasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
          onChanged();
        } else {
          decreasedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder removeDecreasedContainers(int index) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.remove(index);
          onChanged();
        } else {
          decreasedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder getDecreasedContainersBuilder(
          int index) {
        return getDecreasedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder getDecreasedContainersOrBuilder(
          int index) {
        if (decreasedContainersBuilder_ == null) {
          return decreasedContainers_.get(index);  } else {
          return decreasedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
           getDecreasedContainersOrBuilderList() {
        if (decreasedContainersBuilder_ != null) {
          return decreasedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(decreasedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder addDecreasedContainersBuilder() {
        return getDecreasedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder addDecreasedContainersBuilder(
          int index) {
        return getDecreasedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder> 
           getDecreasedContainersBuilderList() {
        return getDecreasedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
          getDecreasedContainersFieldBuilder() {
        if (decreasedContainersBuilder_ == null) {
          decreasedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder>(
                  decreasedContainers_,
                  ((bitField0_ & 0x00000400) == 0x00000400),
                  getParentForChildren(),
                  isClean());
          decreasedContainers_ = null;
        }
        return decreasedContainersBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.AllocateResponseProto)
    }

    static {
      defaultInstance = new AllocateResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.AllocateResponseProto)
  }

  public interface GetNewApplicationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNewApplicationRequestProto}
   */
  public static final class GetNewApplicationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNewApplicationRequestProtoOrBuilder {
    // Use GetNewApplicationRequestProto.newBuilder() to construct.
    private GetNewApplicationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNewApplicationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNewApplicationRequestProto defaultInstance;
    public static GetNewApplicationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNewApplicationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNewApplicationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNewApplicationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNewApplicationRequestProto>() {
      public GetNewApplicationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNewApplicationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNewApplicationRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNewApplicationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNewApplicationRequestProto)
    }

    static {
      defaultInstance = new GetNewApplicationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNewApplicationRequestProto)
  }

  public interface GetNewApplicationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // optional .hadoop.yarn.ResourceProto maximumCapability = 2;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    boolean hasMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNewApplicationResponseProto}
   */
  public static final class GetNewApplicationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNewApplicationResponseProtoOrBuilder {
    // Use GetNewApplicationResponseProto.newBuilder() to construct.
    private GetNewApplicationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNewApplicationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNewApplicationResponseProto defaultInstance;
    public static GetNewApplicationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNewApplicationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNewApplicationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = maximumCapability_.toBuilder();
              }
              maximumCapability_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(maximumCapability_);
                maximumCapability_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNewApplicationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNewApplicationResponseProto>() {
      public GetNewApplicationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNewApplicationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNewApplicationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // optional .hadoop.yarn.ResourceProto maximumCapability = 2;
    public static final int MAXIMUMCAPABILITY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    public boolean hasMaximumCapability() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
      return maximumCapability_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
      return maximumCapability_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, maximumCapability_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, maximumCapability_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasMaximumCapability() == other.hasMaximumCapability());
      if (hasMaximumCapability()) {
        result = result && getMaximumCapability()
            .equals(other.getMaximumCapability());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasMaximumCapability()) {
        hash = (37 * hash) + MAXIMUMCAPABILITY_FIELD_NUMBER;
        hash = (53 * hash) + getMaximumCapability().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNewApplicationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
          getMaximumCapabilityFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (maximumCapabilityBuilder_ == null) {
          result.maximumCapability_ = maximumCapability_;
        } else {
          result.maximumCapability_ = maximumCapabilityBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasMaximumCapability()) {
          mergeMaximumCapability(other.getMaximumCapability());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // optional .hadoop.yarn.ResourceProto maximumCapability = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> maximumCapabilityBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public boolean hasMaximumCapability() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          return maximumCapability_;
        } else {
          return maximumCapabilityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder setMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          maximumCapability_ = value;
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder setMaximumCapability(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = builderForValue.build();
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder mergeMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              maximumCapability_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            maximumCapability_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(maximumCapability_).mergeFrom(value).buildPartial();
          } else {
            maximumCapability_ = value;
          }
          onChanged();
        } else {
          maximumCapabilityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder clearMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getMaximumCapabilityBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getMaximumCapabilityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
        if (maximumCapabilityBuilder_ != null) {
          return maximumCapabilityBuilder_.getMessageOrBuilder();
        } else {
          return maximumCapability_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getMaximumCapabilityFieldBuilder() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapabilityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  maximumCapability_,
                  getParentForChildren(),
                  isClean());
          maximumCapability_ = null;
        }
        return maximumCapabilityBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNewApplicationResponseProto)
    }

    static {
      defaultInstance = new GetNewApplicationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNewApplicationResponseProto)
  }

  public interface GetApplicationReportRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationReportRequestProto}
   */
  public static final class GetApplicationReportRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationReportRequestProtoOrBuilder {
    // Use GetApplicationReportRequestProto.newBuilder() to construct.
    private GetApplicationReportRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationReportRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationReportRequestProto defaultInstance;
    public static GetApplicationReportRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationReportRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationReportRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationReportRequestProto>() {
      public GetApplicationReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationReportRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationReportRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationReportRequestProto)
    }

    static {
      defaultInstance = new GetApplicationReportRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationReportRequestProto)
  }

  public interface GetApplicationReportResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationReportProto application_report = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    boolean hasApplicationReport();
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplicationReport();
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationReportOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationReportResponseProto}
   */
  public static final class GetApplicationReportResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationReportResponseProtoOrBuilder {
    // Use GetApplicationReportResponseProto.newBuilder() to construct.
    private GetApplicationReportResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationReportResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationReportResponseProto defaultInstance;
    public static GetApplicationReportResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationReportResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationReport_.toBuilder();
              }
              applicationReport_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationReport_);
                applicationReport_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationReportResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationReportResponseProto>() {
      public GetApplicationReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationReportResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationReportResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationReportProto application_report = 1;
    public static final int APPLICATION_REPORT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto applicationReport_;
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    public boolean hasApplicationReport() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplicationReport() {
      return applicationReport_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationReportOrBuilder() {
      return applicationReport_;
    }

    private void initFields() {
      applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasApplicationReport()) {
        if (!getApplicationReport().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationReport_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationReport_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto) obj;

      boolean result = true;
      result = result && (hasApplicationReport() == other.hasApplicationReport());
      if (hasApplicationReport()) {
        result = result && getApplicationReport()
            .equals(other.getApplicationReport());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationReport()) {
        hash = (37 * hash) + APPLICATION_REPORT_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationReport().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationReportFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationReportBuilder_ == null) {
          applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
        } else {
          applicationReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationReportBuilder_ == null) {
          result.applicationReport_ = applicationReport_;
        } else {
          result.applicationReport_ = applicationReportBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.getDefaultInstance()) return this;
        if (other.hasApplicationReport()) {
          mergeApplicationReport(other.getApplicationReport());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasApplicationReport()) {
          if (!getApplicationReport().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationReportProto application_report = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> applicationReportBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public boolean hasApplicationReport() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplicationReport() {
        if (applicationReportBuilder_ == null) {
          return applicationReport_;
        } else {
          return applicationReportBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder setApplicationReport(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationReportBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationReport_ = value;
          onChanged();
        } else {
          applicationReportBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder setApplicationReport(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationReportBuilder_ == null) {
          applicationReport_ = builderForValue.build();
          onChanged();
        } else {
          applicationReportBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder mergeApplicationReport(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationReportBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationReport_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance()) {
            applicationReport_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.newBuilder(applicationReport_).mergeFrom(value).buildPartial();
          } else {
            applicationReport_ = value;
          }
          onChanged();
        } else {
          applicationReportBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder clearApplicationReport() {
        if (applicationReportBuilder_ == null) {
          applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
          onChanged();
        } else {
          applicationReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder getApplicationReportBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationReportFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationReportOrBuilder() {
        if (applicationReportBuilder_ != null) {
          return applicationReportBuilder_.getMessageOrBuilder();
        } else {
          return applicationReport_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
          getApplicationReportFieldBuilder() {
        if (applicationReportBuilder_ == null) {
          applicationReportBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder>(
                  applicationReport_,
                  getParentForChildren(),
                  isClean());
          applicationReport_ = null;
        }
        return applicationReportBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationReportResponseProto)
    }

    static {
      defaultInstance = new GetApplicationReportResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationReportResponseProto)
  }

  public interface SubmitApplicationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    boolean hasApplicationSubmissionContext();
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext();
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SubmitApplicationRequestProto}
   */
  public static final class SubmitApplicationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements SubmitApplicationRequestProtoOrBuilder {
    // Use SubmitApplicationRequestProto.newBuilder() to construct.
    private SubmitApplicationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SubmitApplicationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SubmitApplicationRequestProto defaultInstance;
    public static SubmitApplicationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public SubmitApplicationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SubmitApplicationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationSubmissionContext_.toBuilder();
              }
              applicationSubmissionContext_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationSubmissionContext_);
                applicationSubmissionContext_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<SubmitApplicationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<SubmitApplicationRequestProto>() {
      public SubmitApplicationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SubmitApplicationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SubmitApplicationRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;
    public static final int APPLICATION_SUBMISSION_CONTEXT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto applicationSubmissionContext_;
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    public boolean hasApplicationSubmissionContext() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext() {
      return applicationSubmissionContext_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder() {
      return applicationSubmissionContext_;
    }

    private void initFields() {
      applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationSubmissionContext_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationSubmissionContext_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationSubmissionContext() == other.hasApplicationSubmissionContext());
      if (hasApplicationSubmissionContext()) {
        result = result && getApplicationSubmissionContext()
            .equals(other.getApplicationSubmissionContext());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationSubmissionContext()) {
        hash = (37 * hash) + APPLICATION_SUBMISSION_CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationSubmissionContext().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SubmitApplicationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationSubmissionContextFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
        } else {
          applicationSubmissionContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationSubmissionContextBuilder_ == null) {
          result.applicationSubmissionContext_ = applicationSubmissionContext_;
        } else {
          result.applicationSubmissionContext_ = applicationSubmissionContextBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationSubmissionContext()) {
          mergeApplicationSubmissionContext(other.getApplicationSubmissionContext());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder> applicationSubmissionContextBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public boolean hasApplicationSubmissionContext() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext() {
        if (applicationSubmissionContextBuilder_ == null) {
          return applicationSubmissionContext_;
        } else {
          return applicationSubmissionContextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder setApplicationSubmissionContext(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto value) {
        if (applicationSubmissionContextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationSubmissionContext_ = value;
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder setApplicationSubmissionContext(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder builderForValue) {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = builderForValue.build();
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder mergeApplicationSubmissionContext(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto value) {
        if (applicationSubmissionContextBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationSubmissionContext_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance()) {
            applicationSubmissionContext_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.newBuilder(applicationSubmissionContext_).mergeFrom(value).buildPartial();
          } else {
            applicationSubmissionContext_ = value;
          }
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder clearApplicationSubmissionContext() {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder getApplicationSubmissionContextBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationSubmissionContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder() {
        if (applicationSubmissionContextBuilder_ != null) {
          return applicationSubmissionContextBuilder_.getMessageOrBuilder();
        } else {
          return applicationSubmissionContext_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder> 
          getApplicationSubmissionContextFieldBuilder() {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContextBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder>(
                  applicationSubmissionContext_,
                  getParentForChildren(),
                  isClean());
          applicationSubmissionContext_ = null;
        }
        return applicationSubmissionContextBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SubmitApplicationRequestProto)
    }

    static {
      defaultInstance = new SubmitApplicationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SubmitApplicationRequestProto)
  }

  public interface SubmitApplicationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.SubmitApplicationResponseProto}
   */
  public static final class SubmitApplicationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements SubmitApplicationResponseProtoOrBuilder {
    // Use SubmitApplicationResponseProto.newBuilder() to construct.
    private SubmitApplicationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SubmitApplicationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SubmitApplicationResponseProto defaultInstance;
    public static SubmitApplicationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public SubmitApplicationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SubmitApplicationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<SubmitApplicationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<SubmitApplicationResponseProto>() {
      public SubmitApplicationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SubmitApplicationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SubmitApplicationResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SubmitApplicationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SubmitApplicationResponseProto)
    }

    static {
      defaultInstance = new SubmitApplicationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SubmitApplicationResponseProto)
  }

  public interface KillApplicationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.KillApplicationRequestProto}
   */
  public static final class KillApplicationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements KillApplicationRequestProtoOrBuilder {
    // Use KillApplicationRequestProto.newBuilder() to construct.
    private KillApplicationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private KillApplicationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final KillApplicationRequestProto defaultInstance;
    public static KillApplicationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public KillApplicationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private KillApplicationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<KillApplicationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<KillApplicationRequestProto>() {
      public KillApplicationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new KillApplicationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<KillApplicationRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.KillApplicationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.KillApplicationRequestProto)
    }

    static {
      defaultInstance = new KillApplicationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.KillApplicationRequestProto)
  }

  public interface KillApplicationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool is_kill_completed = 1 [default = false];
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    boolean hasIsKillCompleted();
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    boolean getIsKillCompleted();
  }
  /**
   * Protobuf type {@code hadoop.yarn.KillApplicationResponseProto}
   */
  public static final class KillApplicationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements KillApplicationResponseProtoOrBuilder {
    // Use KillApplicationResponseProto.newBuilder() to construct.
    private KillApplicationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private KillApplicationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final KillApplicationResponseProto defaultInstance;
    public static KillApplicationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public KillApplicationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private KillApplicationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              isKillCompleted_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<KillApplicationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<KillApplicationResponseProto>() {
      public KillApplicationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new KillApplicationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<KillApplicationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool is_kill_completed = 1 [default = false];
    public static final int IS_KILL_COMPLETED_FIELD_NUMBER = 1;
    private boolean isKillCompleted_;
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    public boolean hasIsKillCompleted() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    public boolean getIsKillCompleted() {
      return isKillCompleted_;
    }

    private void initFields() {
      isKillCompleted_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, isKillCompleted_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, isKillCompleted_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto) obj;

      boolean result = true;
      result = result && (hasIsKillCompleted() == other.hasIsKillCompleted());
      if (hasIsKillCompleted()) {
        result = result && (getIsKillCompleted()
            == other.getIsKillCompleted());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasIsKillCompleted()) {
        hash = (37 * hash) + IS_KILL_COMPLETED_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIsKillCompleted());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.KillApplicationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        isKillCompleted_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.isKillCompleted_ = isKillCompleted_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.getDefaultInstance()) return this;
        if (other.hasIsKillCompleted()) {
          setIsKillCompleted(other.getIsKillCompleted());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool is_kill_completed = 1 [default = false];
      private boolean isKillCompleted_ ;
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public boolean hasIsKillCompleted() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public boolean getIsKillCompleted() {
        return isKillCompleted_;
      }
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public Builder setIsKillCompleted(boolean value) {
        bitField0_ |= 0x00000001;
        isKillCompleted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public Builder clearIsKillCompleted() {
        bitField0_ = (bitField0_ & ~0x00000001);
        isKillCompleted_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.KillApplicationResponseProto)
    }

    static {
      defaultInstance = new KillApplicationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.KillApplicationResponseProto)
  }

  public interface GetClusterMetricsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterMetricsRequestProto}
   */
  public static final class GetClusterMetricsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterMetricsRequestProtoOrBuilder {
    // Use GetClusterMetricsRequestProto.newBuilder() to construct.
    private GetClusterMetricsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterMetricsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterMetricsRequestProto defaultInstance;
    public static GetClusterMetricsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterMetricsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterMetricsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterMetricsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterMetricsRequestProto>() {
      public GetClusterMetricsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterMetricsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterMetricsRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterMetricsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterMetricsRequestProto)
    }

    static {
      defaultInstance = new GetClusterMetricsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterMetricsRequestProto)
  }

  public interface GetClusterMetricsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    boolean hasClusterMetrics();
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto getClusterMetrics();
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder getClusterMetricsOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterMetricsResponseProto}
   */
  public static final class GetClusterMetricsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterMetricsResponseProtoOrBuilder {
    // Use GetClusterMetricsResponseProto.newBuilder() to construct.
    private GetClusterMetricsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterMetricsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterMetricsResponseProto defaultInstance;
    public static GetClusterMetricsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterMetricsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterMetricsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = clusterMetrics_.toBuilder();
              }
              clusterMetrics_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(clusterMetrics_);
                clusterMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterMetricsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterMetricsResponseProto>() {
      public GetClusterMetricsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterMetricsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterMetricsResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;
    public static final int CLUSTER_METRICS_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto clusterMetrics_;
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    public boolean hasClusterMetrics() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto getClusterMetrics() {
      return clusterMetrics_;
    }
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder getClusterMetricsOrBuilder() {
      return clusterMetrics_;
    }

    private void initFields() {
      clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, clusterMetrics_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, clusterMetrics_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto) obj;

      boolean result = true;
      result = result && (hasClusterMetrics() == other.hasClusterMetrics());
      if (hasClusterMetrics()) {
        result = result && getClusterMetrics()
            .equals(other.getClusterMetrics());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasClusterMetrics()) {
        hash = (37 * hash) + CLUSTER_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getClusterMetrics().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterMetricsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getClusterMetricsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (clusterMetricsBuilder_ == null) {
          clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
        } else {
          clusterMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (clusterMetricsBuilder_ == null) {
          result.clusterMetrics_ = clusterMetrics_;
        } else {
          result.clusterMetrics_ = clusterMetricsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.getDefaultInstance()) return this;
        if (other.hasClusterMetrics()) {
          mergeClusterMetrics(other.getClusterMetrics());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder> clusterMetricsBuilder_;
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public boolean hasClusterMetrics() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto getClusterMetrics() {
        if (clusterMetricsBuilder_ == null) {
          return clusterMetrics_;
        } else {
          return clusterMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder setClusterMetrics(org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto value) {
        if (clusterMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          clusterMetrics_ = value;
          onChanged();
        } else {
          clusterMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder setClusterMetrics(
          org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder builderForValue) {
        if (clusterMetricsBuilder_ == null) {
          clusterMetrics_ = builderForValue.build();
          onChanged();
        } else {
          clusterMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder mergeClusterMetrics(org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto value) {
        if (clusterMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              clusterMetrics_ != org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance()) {
            clusterMetrics_ =
              org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.newBuilder(clusterMetrics_).mergeFrom(value).buildPartial();
          } else {
            clusterMetrics_ = value;
          }
          onChanged();
        } else {
          clusterMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder clearClusterMetrics() {
        if (clusterMetricsBuilder_ == null) {
          clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
          onChanged();
        } else {
          clusterMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder getClusterMetricsBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getClusterMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder getClusterMetricsOrBuilder() {
        if (clusterMetricsBuilder_ != null) {
          return clusterMetricsBuilder_.getMessageOrBuilder();
        } else {
          return clusterMetrics_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder> 
          getClusterMetricsFieldBuilder() {
        if (clusterMetricsBuilder_ == null) {
          clusterMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder>(
                  clusterMetrics_,
                  getParentForChildren(),
                  isClean());
          clusterMetrics_ = null;
        }
        return clusterMetricsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterMetricsResponseProto)
    }

    static {
      defaultInstance = new GetClusterMetricsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterMetricsResponseProto)
  }

  public interface GetApplicationsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated string application_types = 1;
    /**
     * <code>repeated string application_types = 1;</code>
     */
    java.util.List<java.lang.String>
    getApplicationTypesList();
    /**
     * <code>repeated string application_types = 1;</code>
     */
    int getApplicationTypesCount();
    /**
     * <code>repeated string application_types = 1;</code>
     */
    java.lang.String getApplicationTypes(int index);
    /**
     * <code>repeated string application_types = 1;</code>
     */
    com.google.protobuf.ByteString
        getApplicationTypesBytes(int index);

    // repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> getApplicationStatesList();
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    int getApplicationStatesCount();
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto getApplicationStates(int index);

    // repeated string users = 3;
    /**
     * <code>repeated string users = 3;</code>
     */
    java.util.List<java.lang.String>
    getUsersList();
    /**
     * <code>repeated string users = 3;</code>
     */
    int getUsersCount();
    /**
     * <code>repeated string users = 3;</code>
     */
    java.lang.String getUsers(int index);
    /**
     * <code>repeated string users = 3;</code>
     */
    com.google.protobuf.ByteString
        getUsersBytes(int index);

    // repeated string queues = 4;
    /**
     * <code>repeated string queues = 4;</code>
     */
    java.util.List<java.lang.String>
    getQueuesList();
    /**
     * <code>repeated string queues = 4;</code>
     */
    int getQueuesCount();
    /**
     * <code>repeated string queues = 4;</code>
     */
    java.lang.String getQueues(int index);
    /**
     * <code>repeated string queues = 4;</code>
     */
    com.google.protobuf.ByteString
        getQueuesBytes(int index);

    // optional int64 limit = 5;
    /**
     * <code>optional int64 limit = 5;</code>
     */
    boolean hasLimit();
    /**
     * <code>optional int64 limit = 5;</code>
     */
    long getLimit();

    // optional int64 start_begin = 6;
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    boolean hasStartBegin();
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    long getStartBegin();

    // optional int64 start_end = 7;
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    boolean hasStartEnd();
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    long getStartEnd();

    // optional int64 finish_begin = 8;
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    boolean hasFinishBegin();
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    long getFinishBegin();

    // optional int64 finish_end = 9;
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    boolean hasFinishEnd();
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    long getFinishEnd();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationsRequestProto}
   */
  public static final class GetApplicationsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationsRequestProtoOrBuilder {
    // Use GetApplicationsRequestProto.newBuilder() to construct.
    private GetApplicationsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationsRequestProto defaultInstance;
    public static GetApplicationsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                applicationTypes_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              applicationTypes_.add(input.readBytes());
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  applicationStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto>();
                  mutable_bitField0_ |= 0x00000002;
                }
                applicationStates_.add(value);
              }
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int oldLimit = input.pushLimit(length);
              while(input.getBytesUntilLimit() > 0) {
                int rawValue = input.readEnum();
                org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(2, rawValue);
                } else {
                  if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                    applicationStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto>();
                    mutable_bitField0_ |= 0x00000002;
                  }
                  applicationStates_.add(value);
                }
              }
              input.popLimit(oldLimit);
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                users_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000004;
              }
              users_.add(input.readBytes());
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                queues_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000008;
              }
              queues_.add(input.readBytes());
              break;
            }
            case 40: {
              bitField0_ |= 0x00000001;
              limit_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000002;
              startBegin_ = input.readInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000004;
              startEnd_ = input.readInt64();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000008;
              finishBegin_ = input.readInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000010;
              finishEnd_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          applicationTypes_ = new com.google.protobuf.UnmodifiableLazyStringList(applicationTypes_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          applicationStates_ = java.util.Collections.unmodifiableList(applicationStates_);
        }
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          users_ = new com.google.protobuf.UnmodifiableLazyStringList(users_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          queues_ = new com.google.protobuf.UnmodifiableLazyStringList(queues_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationsRequestProto>() {
      public GetApplicationsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationsRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // repeated string application_types = 1;
    public static final int APPLICATION_TYPES_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList applicationTypes_;
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public java.util.List<java.lang.String>
        getApplicationTypesList() {
      return applicationTypes_;
    }
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public int getApplicationTypesCount() {
      return applicationTypes_.size();
    }
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public java.lang.String getApplicationTypes(int index) {
      return applicationTypes_.get(index);
    }
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public com.google.protobuf.ByteString
        getApplicationTypesBytes(int index) {
      return applicationTypes_.getByteString(index);
    }

    // repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;
    public static final int APPLICATION_STATES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> applicationStates_;
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> getApplicationStatesList() {
      return applicationStates_;
    }
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    public int getApplicationStatesCount() {
      return applicationStates_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto getApplicationStates(int index) {
      return applicationStates_.get(index);
    }

    // repeated string users = 3;
    public static final int USERS_FIELD_NUMBER = 3;
    private com.google.protobuf.LazyStringList users_;
    /**
     * <code>repeated string users = 3;</code>
     */
    public java.util.List<java.lang.String>
        getUsersList() {
      return users_;
    }
    /**
     * <code>repeated string users = 3;</code>
     */
    public int getUsersCount() {
      return users_.size();
    }
    /**
     * <code>repeated string users = 3;</code>
     */
    public java.lang.String getUsers(int index) {
      return users_.get(index);
    }
    /**
     * <code>repeated string users = 3;</code>
     */
    public com.google.protobuf.ByteString
        getUsersBytes(int index) {
      return users_.getByteString(index);
    }

    // repeated string queues = 4;
    public static final int QUEUES_FIELD_NUMBER = 4;
    private com.google.protobuf.LazyStringList queues_;
    /**
     * <code>repeated string queues = 4;</code>
     */
    public java.util.List<java.lang.String>
        getQueuesList() {
      return queues_;
    }
    /**
     * <code>repeated string queues = 4;</code>
     */
    public int getQueuesCount() {
      return queues_.size();
    }
    /**
     * <code>repeated string queues = 4;</code>
     */
    public java.lang.String getQueues(int index) {
      return queues_.get(index);
    }
    /**
     * <code>repeated string queues = 4;</code>
     */
    public com.google.protobuf.ByteString
        getQueuesBytes(int index) {
      return queues_.getByteString(index);
    }

    // optional int64 limit = 5;
    public static final int LIMIT_FIELD_NUMBER = 5;
    private long limit_;
    /**
     * <code>optional int64 limit = 5;</code>
     */
    public boolean hasLimit() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional int64 limit = 5;</code>
     */
    public long getLimit() {
      return limit_;
    }

    // optional int64 start_begin = 6;
    public static final int START_BEGIN_FIELD_NUMBER = 6;
    private long startBegin_;
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    public boolean hasStartBegin() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    public long getStartBegin() {
      return startBegin_;
    }

    // optional int64 start_end = 7;
    public static final int START_END_FIELD_NUMBER = 7;
    private long startEnd_;
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    public boolean hasStartEnd() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    public long getStartEnd() {
      return startEnd_;
    }

    // optional int64 finish_begin = 8;
    public static final int FINISH_BEGIN_FIELD_NUMBER = 8;
    private long finishBegin_;
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    public boolean hasFinishBegin() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    public long getFinishBegin() {
      return finishBegin_;
    }

    // optional int64 finish_end = 9;
    public static final int FINISH_END_FIELD_NUMBER = 9;
    private long finishEnd_;
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    public boolean hasFinishEnd() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    public long getFinishEnd() {
      return finishEnd_;
    }

    private void initFields() {
      applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      applicationStates_ = java.util.Collections.emptyList();
      users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      limit_ = 0L;
      startBegin_ = 0L;
      startEnd_ = 0L;
      finishBegin_ = 0L;
      finishEnd_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < applicationTypes_.size(); i++) {
        output.writeBytes(1, applicationTypes_.getByteString(i));
      }
      for (int i = 0; i < applicationStates_.size(); i++) {
        output.writeEnum(2, applicationStates_.get(i).getNumber());
      }
      for (int i = 0; i < users_.size(); i++) {
        output.writeBytes(3, users_.getByteString(i));
      }
      for (int i = 0; i < queues_.size(); i++) {
        output.writeBytes(4, queues_.getByteString(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt64(5, limit_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt64(6, startBegin_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(7, startEnd_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt64(8, finishBegin_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeInt64(9, finishEnd_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < applicationTypes_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(applicationTypes_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getApplicationTypesList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < applicationStates_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeEnumSizeNoTag(applicationStates_.get(i).getNumber());
        }
        size += dataSize;
        size += 1 * applicationStates_.size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < users_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(users_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getUsersList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < queues_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(queues_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getQueuesList().size();
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, limit_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, startBegin_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, startEnd_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, finishBegin_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, finishEnd_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto) obj;

      boolean result = true;
      result = result && getApplicationTypesList()
          .equals(other.getApplicationTypesList());
      result = result && getApplicationStatesList()
          .equals(other.getApplicationStatesList());
      result = result && getUsersList()
          .equals(other.getUsersList());
      result = result && getQueuesList()
          .equals(other.getQueuesList());
      result = result && (hasLimit() == other.hasLimit());
      if (hasLimit()) {
        result = result && (getLimit()
            == other.getLimit());
      }
      result = result && (hasStartBegin() == other.hasStartBegin());
      if (hasStartBegin()) {
        result = result && (getStartBegin()
            == other.getStartBegin());
      }
      result = result && (hasStartEnd() == other.hasStartEnd());
      if (hasStartEnd()) {
        result = result && (getStartEnd()
            == other.getStartEnd());
      }
      result = result && (hasFinishBegin() == other.hasFinishBegin());
      if (hasFinishBegin()) {
        result = result && (getFinishBegin()
            == other.getFinishBegin());
      }
      result = result && (hasFinishEnd() == other.hasFinishEnd());
      if (hasFinishEnd()) {
        result = result && (getFinishEnd()
            == other.getFinishEnd());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getApplicationTypesCount() > 0) {
        hash = (37 * hash) + APPLICATION_TYPES_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationTypesList().hashCode();
      }
      if (getApplicationStatesCount() > 0) {
        hash = (37 * hash) + APPLICATION_STATES_FIELD_NUMBER;
        hash = (53 * hash) + hashEnumList(getApplicationStatesList());
      }
      if (getUsersCount() > 0) {
        hash = (37 * hash) + USERS_FIELD_NUMBER;
        hash = (53 * hash) + getUsersList().hashCode();
      }
      if (getQueuesCount() > 0) {
        hash = (37 * hash) + QUEUES_FIELD_NUMBER;
        hash = (53 * hash) + getQueuesList().hashCode();
      }
      if (hasLimit()) {
        hash = (37 * hash) + LIMIT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLimit());
      }
      if (hasStartBegin()) {
        hash = (37 * hash) + START_BEGIN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStartBegin());
      }
      if (hasStartEnd()) {
        hash = (37 * hash) + START_END_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStartEnd());
      }
      if (hasFinishBegin()) {
        hash = (37 * hash) + FINISH_BEGIN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFinishBegin());
      }
      if (hasFinishEnd()) {
        hash = (37 * hash) + FINISH_END_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFinishEnd());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        applicationStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        limit_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        startBegin_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        startEnd_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        finishBegin_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        finishEnd_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          applicationTypes_ = new com.google.protobuf.UnmodifiableLazyStringList(
              applicationTypes_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.applicationTypes_ = applicationTypes_;
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          applicationStates_ = java.util.Collections.unmodifiableList(applicationStates_);
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.applicationStates_ = applicationStates_;
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          users_ = new com.google.protobuf.UnmodifiableLazyStringList(
              users_);
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.users_ = users_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          queues_ = new com.google.protobuf.UnmodifiableLazyStringList(
              queues_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.queues_ = queues_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000001;
        }
        result.limit_ = limit_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000002;
        }
        result.startBegin_ = startBegin_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000004;
        }
        result.startEnd_ = startEnd_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000008;
        }
        result.finishBegin_ = finishBegin_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000010;
        }
        result.finishEnd_ = finishEnd_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.getDefaultInstance()) return this;
        if (!other.applicationTypes_.isEmpty()) {
          if (applicationTypes_.isEmpty()) {
            applicationTypes_ = other.applicationTypes_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureApplicationTypesIsMutable();
            applicationTypes_.addAll(other.applicationTypes_);
          }
          onChanged();
        }
        if (!other.applicationStates_.isEmpty()) {
          if (applicationStates_.isEmpty()) {
            applicationStates_ = other.applicationStates_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureApplicationStatesIsMutable();
            applicationStates_.addAll(other.applicationStates_);
          }
          onChanged();
        }
        if (!other.users_.isEmpty()) {
          if (users_.isEmpty()) {
            users_ = other.users_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureUsersIsMutable();
            users_.addAll(other.users_);
          }
          onChanged();
        }
        if (!other.queues_.isEmpty()) {
          if (queues_.isEmpty()) {
            queues_ = other.queues_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureQueuesIsMutable();
            queues_.addAll(other.queues_);
          }
          onChanged();
        }
        if (other.hasLimit()) {
          setLimit(other.getLimit());
        }
        if (other.hasStartBegin()) {
          setStartBegin(other.getStartBegin());
        }
        if (other.hasStartEnd()) {
          setStartEnd(other.getStartEnd());
        }
        if (other.hasFinishBegin()) {
          setFinishBegin(other.getFinishBegin());
        }
        if (other.hasFinishEnd()) {
          setFinishEnd(other.getFinishEnd());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated string application_types = 1;
      private com.google.protobuf.LazyStringList applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureApplicationTypesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          applicationTypes_ = new com.google.protobuf.LazyStringArrayList(applicationTypes_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public java.util.List<java.lang.String>
          getApplicationTypesList() {
        return java.util.Collections.unmodifiableList(applicationTypes_);
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public int getApplicationTypesCount() {
        return applicationTypes_.size();
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public java.lang.String getApplicationTypes(int index) {
        return applicationTypes_.get(index);
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public com.google.protobuf.ByteString
          getApplicationTypesBytes(int index) {
        return applicationTypes_.getByteString(index);
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder setApplicationTypes(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTypesIsMutable();
        applicationTypes_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder addApplicationTypes(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTypesIsMutable();
        applicationTypes_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder addAllApplicationTypes(
          java.lang.Iterable<java.lang.String> values) {
        ensureApplicationTypesIsMutable();
        super.addAll(values, applicationTypes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder clearApplicationTypes() {
        applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder addApplicationTypesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTypesIsMutable();
        applicationTypes_.add(value);
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> applicationStates_ =
        java.util.Collections.emptyList();
      private void ensureApplicationStatesIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          applicationStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto>(applicationStates_);
          bitField0_ |= 0x00000002;
        }
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> getApplicationStatesList() {
        return java.util.Collections.unmodifiableList(applicationStates_);
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public int getApplicationStatesCount() {
        return applicationStates_.size();
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto getApplicationStates(int index) {
        return applicationStates_.get(index);
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder setApplicationStates(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureApplicationStatesIsMutable();
        applicationStates_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder addApplicationStates(org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureApplicationStatesIsMutable();
        applicationStates_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder addAllApplicationStates(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> values) {
        ensureApplicationStatesIsMutable();
        super.addAll(values, applicationStates_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder clearApplicationStates() {
        applicationStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      // repeated string users = 3;
      private com.google.protobuf.LazyStringList users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureUsersIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          users_ = new com.google.protobuf.LazyStringArrayList(users_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public java.util.List<java.lang.String>
          getUsersList() {
        return java.util.Collections.unmodifiableList(users_);
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public int getUsersCount() {
        return users_.size();
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public java.lang.String getUsers(int index) {
        return users_.get(index);
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public com.google.protobuf.ByteString
          getUsersBytes(int index) {
        return users_.getByteString(index);
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder setUsers(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureUsersIsMutable();
        users_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder addUsers(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureUsersIsMutable();
        users_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder addAllUsers(
          java.lang.Iterable<java.lang.String> values) {
        ensureUsersIsMutable();
        super.addAll(values, users_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder clearUsers() {
        users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder addUsersBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureUsersIsMutable();
        users_.add(value);
        onChanged();
        return this;
      }

      // repeated string queues = 4;
      private com.google.protobuf.LazyStringList queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureQueuesIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          queues_ = new com.google.protobuf.LazyStringArrayList(queues_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public java.util.List<java.lang.String>
          getQueuesList() {
        return java.util.Collections.unmodifiableList(queues_);
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public int getQueuesCount() {
        return queues_.size();
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public java.lang.String getQueues(int index) {
        return queues_.get(index);
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public com.google.protobuf.ByteString
          getQueuesBytes(int index) {
        return queues_.getByteString(index);
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder setQueues(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureQueuesIsMutable();
        queues_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder addQueues(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureQueuesIsMutable();
        queues_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder addAllQueues(
          java.lang.Iterable<java.lang.String> values) {
        ensureQueuesIsMutable();
        super.addAll(values, queues_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder clearQueues() {
        queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder addQueuesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureQueuesIsMutable();
        queues_.add(value);
        onChanged();
        return this;
      }

      // optional int64 limit = 5;
      private long limit_ ;
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public boolean hasLimit() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public long getLimit() {
        return limit_;
      }
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public Builder setLimit(long value) {
        bitField0_ |= 0x00000010;
        limit_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public Builder clearLimit() {
        bitField0_ = (bitField0_ & ~0x00000010);
        limit_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 start_begin = 6;
      private long startBegin_ ;
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public boolean hasStartBegin() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public long getStartBegin() {
        return startBegin_;
      }
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public Builder setStartBegin(long value) {
        bitField0_ |= 0x00000020;
        startBegin_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public Builder clearStartBegin() {
        bitField0_ = (bitField0_ & ~0x00000020);
        startBegin_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 start_end = 7;
      private long startEnd_ ;
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public boolean hasStartEnd() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public long getStartEnd() {
        return startEnd_;
      }
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public Builder setStartEnd(long value) {
        bitField0_ |= 0x00000040;
        startEnd_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public Builder clearStartEnd() {
        bitField0_ = (bitField0_ & ~0x00000040);
        startEnd_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 finish_begin = 8;
      private long finishBegin_ ;
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public boolean hasFinishBegin() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public long getFinishBegin() {
        return finishBegin_;
      }
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public Builder setFinishBegin(long value) {
        bitField0_ |= 0x00000080;
        finishBegin_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public Builder clearFinishBegin() {
        bitField0_ = (bitField0_ & ~0x00000080);
        finishBegin_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 finish_end = 9;
      private long finishEnd_ ;
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public boolean hasFinishEnd() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public long getFinishEnd() {
        return finishEnd_;
      }
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public Builder setFinishEnd(long value) {
        bitField0_ |= 0x00000100;
        finishEnd_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public Builder clearFinishEnd() {
        bitField0_ = (bitField0_ & ~0x00000100);
        finishEnd_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationsRequestProto)
    }

    static {
      defaultInstance = new GetApplicationsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationsRequestProto)
  }

  public interface GetApplicationsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ApplicationReportProto applications = 1;
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> 
        getApplicationsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplications(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    int getApplicationsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
        getApplicationsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationsResponseProto}
   */
  public static final class GetApplicationsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationsResponseProtoOrBuilder {
    // Use GetApplicationsResponseProto.newBuilder() to construct.
    private GetApplicationsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationsResponseProto defaultInstance;
    public static GetApplicationsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                applications_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              applications_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          applications_ = java.util.Collections.unmodifiableList(applications_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationsResponseProto>() {
      public GetApplicationsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationsResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ApplicationReportProto applications = 1;
    public static final int APPLICATIONS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> applications_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> getApplicationsList() {
      return applications_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
        getApplicationsOrBuilderList() {
      return applications_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public int getApplicationsCount() {
      return applications_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplications(int index) {
      return applications_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationsOrBuilder(
        int index) {
      return applications_.get(index);
    }

    private void initFields() {
      applications_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getApplicationsCount(); i++) {
        if (!getApplications(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < applications_.size(); i++) {
        output.writeMessage(1, applications_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < applications_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applications_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto) obj;

      boolean result = true;
      result = result && getApplicationsList()
          .equals(other.getApplicationsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getApplicationsCount() > 0) {
        hash = (37 * hash) + APPLICATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationsBuilder_ == null) {
          applications_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          applicationsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (applicationsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            applications_ = java.util.Collections.unmodifiableList(applications_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.applications_ = applications_;
        } else {
          result.applications_ = applicationsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.getDefaultInstance()) return this;
        if (applicationsBuilder_ == null) {
          if (!other.applications_.isEmpty()) {
            if (applications_.isEmpty()) {
              applications_ = other.applications_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureApplicationsIsMutable();
              applications_.addAll(other.applications_);
            }
            onChanged();
          }
        } else {
          if (!other.applications_.isEmpty()) {
            if (applicationsBuilder_.isEmpty()) {
              applicationsBuilder_.dispose();
              applicationsBuilder_ = null;
              applications_ = other.applications_;
              bitField0_ = (bitField0_ & ~0x00000001);
              applicationsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getApplicationsFieldBuilder() : null;
            } else {
              applicationsBuilder_.addAllMessages(other.applications_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getApplicationsCount(); i++) {
          if (!getApplications(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ApplicationReportProto applications = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> applications_ =
        java.util.Collections.emptyList();
      private void ensureApplicationsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          applications_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto>(applications_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> applicationsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> getApplicationsList() {
        if (applicationsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applications_);
        } else {
          return applicationsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public int getApplicationsCount() {
        if (applicationsBuilder_ == null) {
          return applications_.size();
        } else {
          return applicationsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplications(int index) {
        if (applicationsBuilder_ == null) {
          return applications_.get(index);
        } else {
          return applicationsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder setApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsIsMutable();
          applications_.set(index, value);
          onChanged();
        } else {
          applicationsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder setApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsIsMutable();
          applications_.add(value);
          onChanged();
        } else {
          applicationsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsIsMutable();
          applications_.add(index, value);
          onChanged();
        } else {
          applicationsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.add(builderForValue.build());
          onChanged();
        } else {
          applicationsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addAllApplications(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> values) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          super.addAll(values, applications_);
          onChanged();
        } else {
          applicationsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder clearApplications() {
        if (applicationsBuilder_ == null) {
          applications_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          applicationsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder removeApplications(int index) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.remove(index);
          onChanged();
        } else {
          applicationsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder getApplicationsBuilder(
          int index) {
        return getApplicationsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationsOrBuilder(
          int index) {
        if (applicationsBuilder_ == null) {
          return applications_.get(index);  } else {
          return applicationsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
           getApplicationsOrBuilderList() {
        if (applicationsBuilder_ != null) {
          return applicationsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applications_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder addApplicationsBuilder() {
        return getApplicationsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder addApplicationsBuilder(
          int index) {
        return getApplicationsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder> 
           getApplicationsBuilderList() {
        return getApplicationsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
          getApplicationsFieldBuilder() {
        if (applicationsBuilder_ == null) {
          applicationsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder>(
                  applications_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          applications_ = null;
        }
        return applicationsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationsResponseProto)
    }

    static {
      defaultInstance = new GetApplicationsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationsResponseProto)
  }

  public interface GetClusterNodesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.NodeStateProto nodeStates = 1;
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> getNodeStatesList();
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    int getNodeStatesCount();
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto getNodeStates(int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterNodesRequestProto}
   */
  public static final class GetClusterNodesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterNodesRequestProtoOrBuilder {
    // Use GetClusterNodesRequestProto.newBuilder() to construct.
    private GetClusterNodesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterNodesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterNodesRequestProto defaultInstance;
    public static GetClusterNodesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterNodesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterNodesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                  nodeStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto>();
                  mutable_bitField0_ |= 0x00000001;
                }
                nodeStates_.add(value);
              }
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int oldLimit = input.pushLimit(length);
              while(input.getBytesUntilLimit() > 0) {
                int rawValue = input.readEnum();
                org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(1, rawValue);
                } else {
                  if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                    nodeStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto>();
                    mutable_bitField0_ |= 0x00000001;
                  }
                  nodeStates_.add(value);
                }
              }
              input.popLimit(oldLimit);
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nodeStates_ = java.util.Collections.unmodifiableList(nodeStates_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterNodesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterNodesRequestProto>() {
      public GetClusterNodesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterNodesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterNodesRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.NodeStateProto nodeStates = 1;
    public static final int NODESTATES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> nodeStates_;
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> getNodeStatesList() {
      return nodeStates_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    public int getNodeStatesCount() {
      return nodeStates_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto getNodeStates(int index) {
      return nodeStates_.get(index);
    }

    private void initFields() {
      nodeStates_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nodeStates_.size(); i++) {
        output.writeEnum(1, nodeStates_.get(i).getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < nodeStates_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeEnumSizeNoTag(nodeStates_.get(i).getNumber());
        }
        size += dataSize;
        size += 1 * nodeStates_.size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto) obj;

      boolean result = true;
      result = result && getNodeStatesList()
          .equals(other.getNodeStatesList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNodeStatesCount() > 0) {
        hash = (37 * hash) + NODESTATES_FIELD_NUMBER;
        hash = (53 * hash) + hashEnumList(getNodeStatesList());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterNodesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nodeStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeStates_ = java.util.Collections.unmodifiableList(nodeStates_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.nodeStates_ = nodeStates_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.getDefaultInstance()) return this;
        if (!other.nodeStates_.isEmpty()) {
          if (nodeStates_.isEmpty()) {
            nodeStates_ = other.nodeStates_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureNodeStatesIsMutable();
            nodeStates_.addAll(other.nodeStates_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.NodeStateProto nodeStates = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> nodeStates_ =
        java.util.Collections.emptyList();
      private void ensureNodeStatesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto>(nodeStates_);
          bitField0_ |= 0x00000001;
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> getNodeStatesList() {
        return java.util.Collections.unmodifiableList(nodeStates_);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public int getNodeStatesCount() {
        return nodeStates_.size();
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto getNodeStates(int index) {
        return nodeStates_.get(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder setNodeStates(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureNodeStatesIsMutable();
        nodeStates_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder addNodeStates(org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureNodeStatesIsMutable();
        nodeStates_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder addAllNodeStates(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> values) {
        ensureNodeStatesIsMutable();
        super.addAll(values, nodeStates_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder clearNodeStates() {
        nodeStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterNodesRequestProto)
    }

    static {
      defaultInstance = new GetClusterNodesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterNodesRequestProto)
  }

  public interface GetClusterNodesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.NodeReportProto nodeReports = 1;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> 
        getNodeReportsList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getNodeReports(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    int getNodeReportsCount();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getNodeReportsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getNodeReportsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterNodesResponseProto}
   */
  public static final class GetClusterNodesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterNodesResponseProtoOrBuilder {
    // Use GetClusterNodesResponseProto.newBuilder() to construct.
    private GetClusterNodesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterNodesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterNodesResponseProto defaultInstance;
    public static GetClusterNodesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterNodesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterNodesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                nodeReports_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              nodeReports_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nodeReports_ = java.util.Collections.unmodifiableList(nodeReports_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterNodesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterNodesResponseProto>() {
      public GetClusterNodesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterNodesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterNodesResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.NodeReportProto nodeReports = 1;
    public static final int NODEREPORTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> nodeReports_;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getNodeReportsList() {
      return nodeReports_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getNodeReportsOrBuilderList() {
      return nodeReports_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public int getNodeReportsCount() {
      return nodeReports_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getNodeReports(int index) {
      return nodeReports_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getNodeReportsOrBuilder(
        int index) {
      return nodeReports_.get(index);
    }

    private void initFields() {
      nodeReports_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nodeReports_.size(); i++) {
        output.writeMessage(1, nodeReports_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < nodeReports_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeReports_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto) obj;

      boolean result = true;
      result = result && getNodeReportsList()
          .equals(other.getNodeReportsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNodeReportsCount() > 0) {
        hash = (37 * hash) + NODEREPORTS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeReportsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterNodesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNodeReportsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (nodeReportsBuilder_ == null) {
          nodeReports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodeReportsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (nodeReportsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            nodeReports_ = java.util.Collections.unmodifiableList(nodeReports_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.nodeReports_ = nodeReports_;
        } else {
          result.nodeReports_ = nodeReportsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.getDefaultInstance()) return this;
        if (nodeReportsBuilder_ == null) {
          if (!other.nodeReports_.isEmpty()) {
            if (nodeReports_.isEmpty()) {
              nodeReports_ = other.nodeReports_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodeReportsIsMutable();
              nodeReports_.addAll(other.nodeReports_);
            }
            onChanged();
          }
        } else {
          if (!other.nodeReports_.isEmpty()) {
            if (nodeReportsBuilder_.isEmpty()) {
              nodeReportsBuilder_.dispose();
              nodeReportsBuilder_ = null;
              nodeReports_ = other.nodeReports_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodeReportsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNodeReportsFieldBuilder() : null;
            } else {
              nodeReportsBuilder_.addAllMessages(other.nodeReports_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.NodeReportProto nodeReports = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> nodeReports_ =
        java.util.Collections.emptyList();
      private void ensureNodeReportsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeReports_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>(nodeReports_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> nodeReportsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getNodeReportsList() {
        if (nodeReportsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodeReports_);
        } else {
          return nodeReportsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public int getNodeReportsCount() {
        if (nodeReportsBuilder_ == null) {
          return nodeReports_.size();
        } else {
          return nodeReportsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getNodeReports(int index) {
        if (nodeReportsBuilder_ == null) {
          return nodeReports_.get(index);
        } else {
          return nodeReportsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder setNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (nodeReportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeReportsIsMutable();
          nodeReports_.set(index, value);
          onChanged();
        } else {
          nodeReportsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder setNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeReportsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (nodeReportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeReportsIsMutable();
          nodeReports_.add(value);
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (nodeReportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeReportsIsMutable();
          nodeReports_.add(index, value);
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.add(builderForValue.build());
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addAllNodeReports(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> values) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          super.addAll(values, nodeReports_);
          onChanged();
        } else {
          nodeReportsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder clearNodeReports() {
        if (nodeReportsBuilder_ == null) {
          nodeReports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodeReportsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder removeNodeReports(int index) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.remove(index);
          onChanged();
        } else {
          nodeReportsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder getNodeReportsBuilder(
          int index) {
        return getNodeReportsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getNodeReportsOrBuilder(
          int index) {
        if (nodeReportsBuilder_ == null) {
          return nodeReports_.get(index);  } else {
          return nodeReportsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
           getNodeReportsOrBuilderList() {
        if (nodeReportsBuilder_ != null) {
          return nodeReportsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodeReports_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addNodeReportsBuilder() {
        return getNodeReportsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addNodeReportsBuilder(
          int index) {
        return getNodeReportsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder> 
           getNodeReportsBuilderList() {
        return getNodeReportsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
          getNodeReportsFieldBuilder() {
        if (nodeReportsBuilder_ == null) {
          nodeReportsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder>(
                  nodeReports_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          nodeReports_ = null;
        }
        return nodeReportsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterNodesResponseProto)
    }

    static {
      defaultInstance = new GetClusterNodesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterNodesResponseProto)
  }

  public interface GetQueueInfoRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string queueName = 1;
    /**
     * <code>optional string queueName = 1;</code>
     */
    boolean hasQueueName();
    /**
     * <code>optional string queueName = 1;</code>
     */
    java.lang.String getQueueName();
    /**
     * <code>optional string queueName = 1;</code>
     */
    com.google.protobuf.ByteString
        getQueueNameBytes();

    // optional bool includeApplications = 2;
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    boolean hasIncludeApplications();
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    boolean getIncludeApplications();

    // optional bool includeChildQueues = 3;
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    boolean hasIncludeChildQueues();
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    boolean getIncludeChildQueues();

    // optional bool recursive = 4;
    /**
     * <code>optional bool recursive = 4;</code>
     */
    boolean hasRecursive();
    /**
     * <code>optional bool recursive = 4;</code>
     */
    boolean getRecursive();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueInfoRequestProto}
   */
  public static final class GetQueueInfoRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueInfoRequestProtoOrBuilder {
    // Use GetQueueInfoRequestProto.newBuilder() to construct.
    private GetQueueInfoRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueInfoRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueInfoRequestProto defaultInstance;
    public static GetQueueInfoRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueInfoRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueInfoRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              queueName_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              includeApplications_ = input.readBool();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              includeChildQueues_ = input.readBool();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              recursive_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueInfoRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueInfoRequestProto>() {
      public GetQueueInfoRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueInfoRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueInfoRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string queueName = 1;
    public static final int QUEUENAME_FIELD_NUMBER = 1;
    private java.lang.Object queueName_;
    /**
     * <code>optional string queueName = 1;</code>
     */
    public boolean hasQueueName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string queueName = 1;</code>
     */
    public java.lang.String getQueueName() {
      java.lang.Object ref = queueName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          queueName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string queueName = 1;</code>
     */
    public com.google.protobuf.ByteString
        getQueueNameBytes() {
      java.lang.Object ref = queueName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        queueName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional bool includeApplications = 2;
    public static final int INCLUDEAPPLICATIONS_FIELD_NUMBER = 2;
    private boolean includeApplications_;
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    public boolean hasIncludeApplications() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    public boolean getIncludeApplications() {
      return includeApplications_;
    }

    // optional bool includeChildQueues = 3;
    public static final int INCLUDECHILDQUEUES_FIELD_NUMBER = 3;
    private boolean includeChildQueues_;
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    public boolean hasIncludeChildQueues() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    public boolean getIncludeChildQueues() {
      return includeChildQueues_;
    }

    // optional bool recursive = 4;
    public static final int RECURSIVE_FIELD_NUMBER = 4;
    private boolean recursive_;
    /**
     * <code>optional bool recursive = 4;</code>
     */
    public boolean hasRecursive() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional bool recursive = 4;</code>
     */
    public boolean getRecursive() {
      return recursive_;
    }

    private void initFields() {
      queueName_ = "";
      includeApplications_ = false;
      includeChildQueues_ = false;
      recursive_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getQueueNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBool(2, includeApplications_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBool(3, includeChildQueues_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBool(4, recursive_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getQueueNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, includeApplications_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, includeChildQueues_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, recursive_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto) obj;

      boolean result = true;
      result = result && (hasQueueName() == other.hasQueueName());
      if (hasQueueName()) {
        result = result && getQueueName()
            .equals(other.getQueueName());
      }
      result = result && (hasIncludeApplications() == other.hasIncludeApplications());
      if (hasIncludeApplications()) {
        result = result && (getIncludeApplications()
            == other.getIncludeApplications());
      }
      result = result && (hasIncludeChildQueues() == other.hasIncludeChildQueues());
      if (hasIncludeChildQueues()) {
        result = result && (getIncludeChildQueues()
            == other.getIncludeChildQueues());
      }
      result = result && (hasRecursive() == other.hasRecursive());
      if (hasRecursive()) {
        result = result && (getRecursive()
            == other.getRecursive());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasQueueName()) {
        hash = (37 * hash) + QUEUENAME_FIELD_NUMBER;
        hash = (53 * hash) + getQueueName().hashCode();
      }
      if (hasIncludeApplications()) {
        hash = (37 * hash) + INCLUDEAPPLICATIONS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIncludeApplications());
      }
      if (hasIncludeChildQueues()) {
        hash = (37 * hash) + INCLUDECHILDQUEUES_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIncludeChildQueues());
      }
      if (hasRecursive()) {
        hash = (37 * hash) + RECURSIVE_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getRecursive());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueInfoRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        queueName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        includeApplications_ = false;
        bitField0_ = (bitField0_ & ~0x00000002);
        includeChildQueues_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        recursive_ = false;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.queueName_ = queueName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.includeApplications_ = includeApplications_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.includeChildQueues_ = includeChildQueues_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.recursive_ = recursive_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.getDefaultInstance()) return this;
        if (other.hasQueueName()) {
          bitField0_ |= 0x00000001;
          queueName_ = other.queueName_;
          onChanged();
        }
        if (other.hasIncludeApplications()) {
          setIncludeApplications(other.getIncludeApplications());
        }
        if (other.hasIncludeChildQueues()) {
          setIncludeChildQueues(other.getIncludeChildQueues());
        }
        if (other.hasRecursive()) {
          setRecursive(other.getRecursive());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string queueName = 1;
      private java.lang.Object queueName_ = "";
      /**
       * <code>optional string queueName = 1;</code>
       */
      public boolean hasQueueName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public java.lang.String getQueueName() {
        java.lang.Object ref = queueName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          queueName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public com.google.protobuf.ByteString
          getQueueNameBytes() {
        java.lang.Object ref = queueName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          queueName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public Builder setQueueName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queueName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public Builder clearQueueName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        queueName_ = getDefaultInstance().getQueueName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public Builder setQueueNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queueName_ = value;
        onChanged();
        return this;
      }

      // optional bool includeApplications = 2;
      private boolean includeApplications_ ;
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public boolean hasIncludeApplications() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public boolean getIncludeApplications() {
        return includeApplications_;
      }
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public Builder setIncludeApplications(boolean value) {
        bitField0_ |= 0x00000002;
        includeApplications_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public Builder clearIncludeApplications() {
        bitField0_ = (bitField0_ & ~0x00000002);
        includeApplications_ = false;
        onChanged();
        return this;
      }

      // optional bool includeChildQueues = 3;
      private boolean includeChildQueues_ ;
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public boolean hasIncludeChildQueues() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public boolean getIncludeChildQueues() {
        return includeChildQueues_;
      }
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public Builder setIncludeChildQueues(boolean value) {
        bitField0_ |= 0x00000004;
        includeChildQueues_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public Builder clearIncludeChildQueues() {
        bitField0_ = (bitField0_ & ~0x00000004);
        includeChildQueues_ = false;
        onChanged();
        return this;
      }

      // optional bool recursive = 4;
      private boolean recursive_ ;
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public boolean hasRecursive() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public boolean getRecursive() {
        return recursive_;
      }
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public Builder setRecursive(boolean value) {
        bitField0_ |= 0x00000008;
        recursive_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public Builder clearRecursive() {
        bitField0_ = (bitField0_ & ~0x00000008);
        recursive_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueInfoRequestProto)
    }

    static {
      defaultInstance = new GetQueueInfoRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueInfoRequestProto)
  }

  public interface GetQueueInfoResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.QueueInfoProto queueInfo = 1;
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    boolean hasQueueInfo();
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto getQueueInfo();
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder getQueueInfoOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueInfoResponseProto}
   */
  public static final class GetQueueInfoResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueInfoResponseProtoOrBuilder {
    // Use GetQueueInfoResponseProto.newBuilder() to construct.
    private GetQueueInfoResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueInfoResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueInfoResponseProto defaultInstance;
    public static GetQueueInfoResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueInfoResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueInfoResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = queueInfo_.toBuilder();
              }
              queueInfo_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queueInfo_);
                queueInfo_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueInfoResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueInfoResponseProto>() {
      public GetQueueInfoResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueInfoResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueInfoResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.QueueInfoProto queueInfo = 1;
    public static final int QUEUEINFO_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto queueInfo_;
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    public boolean hasQueueInfo() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto getQueueInfo() {
      return queueInfo_;
    }
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder getQueueInfoOrBuilder() {
      return queueInfo_;
    }

    private void initFields() {
      queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasQueueInfo()) {
        if (!getQueueInfo().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, queueInfo_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, queueInfo_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto) obj;

      boolean result = true;
      result = result && (hasQueueInfo() == other.hasQueueInfo());
      if (hasQueueInfo()) {
        result = result && getQueueInfo()
            .equals(other.getQueueInfo());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasQueueInfo()) {
        hash = (37 * hash) + QUEUEINFO_FIELD_NUMBER;
        hash = (53 * hash) + getQueueInfo().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueInfoResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getQueueInfoFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (queueInfoBuilder_ == null) {
          queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
        } else {
          queueInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (queueInfoBuilder_ == null) {
          result.queueInfo_ = queueInfo_;
        } else {
          result.queueInfo_ = queueInfoBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.getDefaultInstance()) return this;
        if (other.hasQueueInfo()) {
          mergeQueueInfo(other.getQueueInfo());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasQueueInfo()) {
          if (!getQueueInfo().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.QueueInfoProto queueInfo = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder> queueInfoBuilder_;
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public boolean hasQueueInfo() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto getQueueInfo() {
        if (queueInfoBuilder_ == null) {
          return queueInfo_;
        } else {
          return queueInfoBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder setQueueInfo(org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto value) {
        if (queueInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queueInfo_ = value;
          onChanged();
        } else {
          queueInfoBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder setQueueInfo(
          org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder builderForValue) {
        if (queueInfoBuilder_ == null) {
          queueInfo_ = builderForValue.build();
          onChanged();
        } else {
          queueInfoBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder mergeQueueInfo(org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto value) {
        if (queueInfoBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              queueInfo_ != org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance()) {
            queueInfo_ =
              org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.newBuilder(queueInfo_).mergeFrom(value).buildPartial();
          } else {
            queueInfo_ = value;
          }
          onChanged();
        } else {
          queueInfoBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder clearQueueInfo() {
        if (queueInfoBuilder_ == null) {
          queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
          onChanged();
        } else {
          queueInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder getQueueInfoBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getQueueInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder getQueueInfoOrBuilder() {
        if (queueInfoBuilder_ != null) {
          return queueInfoBuilder_.getMessageOrBuilder();
        } else {
          return queueInfo_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder> 
          getQueueInfoFieldBuilder() {
        if (queueInfoBuilder_ == null) {
          queueInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder>(
                  queueInfo_,
                  getParentForChildren(),
                  isClean());
          queueInfo_ = null;
        }
        return queueInfoBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueInfoResponseProto)
    }

    static {
      defaultInstance = new GetQueueInfoResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueInfoResponseProto)
  }

  public interface GetQueueUserAclsInfoRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoRequestProto}
   */
  public static final class GetQueueUserAclsInfoRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueUserAclsInfoRequestProtoOrBuilder {
    // Use GetQueueUserAclsInfoRequestProto.newBuilder() to construct.
    private GetQueueUserAclsInfoRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueUserAclsInfoRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueUserAclsInfoRequestProto defaultInstance;
    public static GetQueueUserAclsInfoRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueUserAclsInfoRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueUserAclsInfoRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueUserAclsInfoRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueUserAclsInfoRequestProto>() {
      public GetQueueUserAclsInfoRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueUserAclsInfoRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueUserAclsInfoRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueUserAclsInfoRequestProto)
    }

    static {
      defaultInstance = new GetQueueUserAclsInfoRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueUserAclsInfoRequestProto)
  }

  public interface GetQueueUserAclsInfoResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> 
        getQueueUserAclsList();
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto getQueueUserAcls(int index);
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    int getQueueUserAclsCount();
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
        getQueueUserAclsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder getQueueUserAclsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoResponseProto}
   */
  public static final class GetQueueUserAclsInfoResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueUserAclsInfoResponseProtoOrBuilder {
    // Use GetQueueUserAclsInfoResponseProto.newBuilder() to construct.
    private GetQueueUserAclsInfoResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueUserAclsInfoResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueUserAclsInfoResponseProto defaultInstance;
    public static GetQueueUserAclsInfoResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueUserAclsInfoResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueUserAclsInfoResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                queueUserAcls_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              queueUserAcls_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          queueUserAcls_ = java.util.Collections.unmodifiableList(queueUserAcls_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueUserAclsInfoResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueUserAclsInfoResponseProto>() {
      public GetQueueUserAclsInfoResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueUserAclsInfoResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueUserAclsInfoResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;
    public static final int QUEUEUSERACLS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> queueUserAcls_;
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> getQueueUserAclsList() {
      return queueUserAcls_;
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
        getQueueUserAclsOrBuilderList() {
      return queueUserAcls_;
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public int getQueueUserAclsCount() {
      return queueUserAcls_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto getQueueUserAcls(int index) {
      return queueUserAcls_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder getQueueUserAclsOrBuilder(
        int index) {
      return queueUserAcls_.get(index);
    }

    private void initFields() {
      queueUserAcls_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < queueUserAcls_.size(); i++) {
        output.writeMessage(1, queueUserAcls_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < queueUserAcls_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, queueUserAcls_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto) obj;

      boolean result = true;
      result = result && getQueueUserAclsList()
          .equals(other.getQueueUserAclsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getQueueUserAclsCount() > 0) {
        hash = (37 * hash) + QUEUEUSERACLS_FIELD_NUMBER;
        hash = (53 * hash) + getQueueUserAclsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getQueueUserAclsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (queueUserAclsBuilder_ == null) {
          queueUserAcls_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          queueUserAclsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (queueUserAclsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            queueUserAcls_ = java.util.Collections.unmodifiableList(queueUserAcls_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.queueUserAcls_ = queueUserAcls_;
        } else {
          result.queueUserAcls_ = queueUserAclsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.getDefaultInstance()) return this;
        if (queueUserAclsBuilder_ == null) {
          if (!other.queueUserAcls_.isEmpty()) {
            if (queueUserAcls_.isEmpty()) {
              queueUserAcls_ = other.queueUserAcls_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureQueueUserAclsIsMutable();
              queueUserAcls_.addAll(other.queueUserAcls_);
            }
            onChanged();
          }
        } else {
          if (!other.queueUserAcls_.isEmpty()) {
            if (queueUserAclsBuilder_.isEmpty()) {
              queueUserAclsBuilder_.dispose();
              queueUserAclsBuilder_ = null;
              queueUserAcls_ = other.queueUserAcls_;
              bitField0_ = (bitField0_ & ~0x00000001);
              queueUserAclsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getQueueUserAclsFieldBuilder() : null;
            } else {
              queueUserAclsBuilder_.addAllMessages(other.queueUserAcls_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> queueUserAcls_ =
        java.util.Collections.emptyList();
      private void ensureQueueUserAclsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          queueUserAcls_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto>(queueUserAcls_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> queueUserAclsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> getQueueUserAclsList() {
        if (queueUserAclsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(queueUserAcls_);
        } else {
          return queueUserAclsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public int getQueueUserAclsCount() {
        if (queueUserAclsBuilder_ == null) {
          return queueUserAcls_.size();
        } else {
          return queueUserAclsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto getQueueUserAcls(int index) {
        if (queueUserAclsBuilder_ == null) {
          return queueUserAcls_.get(index);
        } else {
          return queueUserAclsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder setQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto value) {
        if (queueUserAclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.set(index, value);
          onChanged();
        } else {
          queueUserAclsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder setQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder builderForValue) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.set(index, builderForValue.build());
          onChanged();
        } else {
          queueUserAclsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto value) {
        if (queueUserAclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(value);
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto value) {
        if (queueUserAclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(index, value);
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(
          org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder builderForValue) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(builderForValue.build());
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder builderForValue) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(index, builderForValue.build());
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addAllQueueUserAcls(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> values) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          super.addAll(values, queueUserAcls_);
          onChanged();
        } else {
          queueUserAclsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder clearQueueUserAcls() {
        if (queueUserAclsBuilder_ == null) {
          queueUserAcls_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          queueUserAclsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder removeQueueUserAcls(int index) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.remove(index);
          onChanged();
        } else {
          queueUserAclsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder getQueueUserAclsBuilder(
          int index) {
        return getQueueUserAclsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder getQueueUserAclsOrBuilder(
          int index) {
        if (queueUserAclsBuilder_ == null) {
          return queueUserAcls_.get(index);  } else {
          return queueUserAclsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
           getQueueUserAclsOrBuilderList() {
        if (queueUserAclsBuilder_ != null) {
          return queueUserAclsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(queueUserAcls_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder addQueueUserAclsBuilder() {
        return getQueueUserAclsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder addQueueUserAclsBuilder(
          int index) {
        return getQueueUserAclsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder> 
           getQueueUserAclsBuilderList() {
        return getQueueUserAclsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
          getQueueUserAclsFieldBuilder() {
        if (queueUserAclsBuilder_ == null) {
          queueUserAclsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder>(
                  queueUserAcls_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          queueUserAcls_ = null;
        }
        return queueUserAclsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueUserAclsInfoResponseProto)
    }

    static {
      defaultInstance = new GetQueueUserAclsInfoResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueUserAclsInfoResponseProto)
  }

  public interface StartContainerRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    boolean hasContainerLaunchContext();
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto getContainerLaunchContext();
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder getContainerLaunchContextOrBuilder();

    // optional .hadoop.common.TokenProto container_token = 2;
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    boolean hasContainerToken();
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getContainerToken();
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getContainerTokenOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainerRequestProto}
   */
  public static final class StartContainerRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainerRequestProtoOrBuilder {
    // Use StartContainerRequestProto.newBuilder() to construct.
    private StartContainerRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainerRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainerRequestProto defaultInstance;
    public static StartContainerRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainerRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainerRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerLaunchContext_.toBuilder();
              }
              containerLaunchContext_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerLaunchContext_);
                containerLaunchContext_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = containerToken_.toBuilder();
              }
              containerToken_ = input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerToken_);
                containerToken_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainerRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainerRequestProto>() {
      public StartContainerRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainerRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainerRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;
    public static final int CONTAINER_LAUNCH_CONTEXT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto containerLaunchContext_;
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    public boolean hasContainerLaunchContext() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto getContainerLaunchContext() {
      return containerLaunchContext_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder getContainerLaunchContextOrBuilder() {
      return containerLaunchContext_;
    }

    // optional .hadoop.common.TokenProto container_token = 2;
    public static final int CONTAINER_TOKEN_FIELD_NUMBER = 2;
    private org.apache.hadoop.security.proto.SecurityProtos.TokenProto containerToken_;
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    public boolean hasContainerToken() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getContainerToken() {
      return containerToken_;
    }
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getContainerTokenOrBuilder() {
      return containerToken_;
    }

    private void initFields() {
      containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
      containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasContainerToken()) {
        if (!getContainerToken().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerLaunchContext_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, containerToken_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerLaunchContext_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, containerToken_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerLaunchContext() == other.hasContainerLaunchContext());
      if (hasContainerLaunchContext()) {
        result = result && getContainerLaunchContext()
            .equals(other.getContainerLaunchContext());
      }
      result = result && (hasContainerToken() == other.hasContainerToken());
      if (hasContainerToken()) {
        result = result && getContainerToken()
            .equals(other.getContainerToken());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerLaunchContext()) {
        hash = (37 * hash) + CONTAINER_LAUNCH_CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getContainerLaunchContext().hashCode();
      }
      if (hasContainerToken()) {
        hash = (37 * hash) + CONTAINER_TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getContainerToken().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainerRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerLaunchContextFieldBuilder();
          getContainerTokenFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
        } else {
          containerLaunchContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (containerTokenBuilder_ == null) {
          containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
        } else {
          containerTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerLaunchContextBuilder_ == null) {
          result.containerLaunchContext_ = containerLaunchContext_;
        } else {
          result.containerLaunchContext_ = containerLaunchContextBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (containerTokenBuilder_ == null) {
          result.containerToken_ = containerToken_;
        } else {
          result.containerToken_ = containerTokenBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerLaunchContext()) {
          mergeContainerLaunchContext(other.getContainerLaunchContext());
        }
        if (other.hasContainerToken()) {
          mergeContainerToken(other.getContainerToken());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasContainerToken()) {
          if (!getContainerToken().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder> containerLaunchContextBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public boolean hasContainerLaunchContext() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto getContainerLaunchContext() {
        if (containerLaunchContextBuilder_ == null) {
          return containerLaunchContext_;
        } else {
          return containerLaunchContextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder setContainerLaunchContext(org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto value) {
        if (containerLaunchContextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerLaunchContext_ = value;
          onChanged();
        } else {
          containerLaunchContextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder setContainerLaunchContext(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder builderForValue) {
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContext_ = builderForValue.build();
          onChanged();
        } else {
          containerLaunchContextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder mergeContainerLaunchContext(org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto value) {
        if (containerLaunchContextBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerLaunchContext_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance()) {
            containerLaunchContext_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.newBuilder(containerLaunchContext_).mergeFrom(value).buildPartial();
          } else {
            containerLaunchContext_ = value;
          }
          onChanged();
        } else {
          containerLaunchContextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder clearContainerLaunchContext() {
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
          onChanged();
        } else {
          containerLaunchContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder getContainerLaunchContextBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerLaunchContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder getContainerLaunchContextOrBuilder() {
        if (containerLaunchContextBuilder_ != null) {
          return containerLaunchContextBuilder_.getMessageOrBuilder();
        } else {
          return containerLaunchContext_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder> 
          getContainerLaunchContextFieldBuilder() {
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContextBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder>(
                  containerLaunchContext_,
                  getParentForChildren(),
                  isClean());
          containerLaunchContext_ = null;
        }
        return containerLaunchContextBuilder_;
      }

      // optional .hadoop.common.TokenProto container_token = 2;
      private org.apache.hadoop.security.proto.SecurityProtos.TokenProto containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> containerTokenBuilder_;
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public boolean hasContainerToken() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getContainerToken() {
        if (containerTokenBuilder_ == null) {
          return containerToken_;
        } else {
          return containerTokenBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder setContainerToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (containerTokenBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerToken_ = value;
          onChanged();
        } else {
          containerTokenBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder setContainerToken(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (containerTokenBuilder_ == null) {
          containerToken_ = builderForValue.build();
          onChanged();
        } else {
          containerTokenBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder mergeContainerToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (containerTokenBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              containerToken_ != org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance()) {
            containerToken_ =
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.newBuilder(containerToken_).mergeFrom(value).buildPartial();
          } else {
            containerToken_ = value;
          }
          onChanged();
        } else {
          containerTokenBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder clearContainerToken() {
        if (containerTokenBuilder_ == null) {
          containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
          onChanged();
        } else {
          containerTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getContainerTokenBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getContainerTokenFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getContainerTokenOrBuilder() {
        if (containerTokenBuilder_ != null) {
          return containerTokenBuilder_.getMessageOrBuilder();
        } else {
          return containerToken_;
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getContainerTokenFieldBuilder() {
        if (containerTokenBuilder_ == null) {
          containerTokenBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  containerToken_,
                  getParentForChildren(),
                  isClean());
          containerToken_ = null;
        }
        return containerTokenBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainerRequestProto)
    }

    static {
      defaultInstance = new StartContainerRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainerRequestProto)
  }

  public interface StartContainerResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> 
        getServicesMetaDataList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index);
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    int getServicesMetaDataCount();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainerResponseProto}
   */
  public static final class StartContainerResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainerResponseProtoOrBuilder {
    // Use StartContainerResponseProto.newBuilder() to construct.
    private StartContainerResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainerResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainerResponseProto defaultInstance;
    public static StartContainerResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainerResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainerResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              servicesMetaData_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainerResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainerResponseProto>() {
      public StartContainerResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainerResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainerResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    public static final int SERVICES_META_DATA_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public int getServicesMetaDataCount() {
      return servicesMetaData_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
      return servicesMetaData_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index) {
      return servicesMetaData_.get(index);
    }

    private void initFields() {
      servicesMetaData_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        output.writeMessage(1, servicesMetaData_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, servicesMetaData_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto) obj;

      boolean result = true;
      result = result && getServicesMetaDataList()
          .equals(other.getServicesMetaDataList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getServicesMetaDataCount() > 0) {
        hash = (37 * hash) + SERVICES_META_DATA_FIELD_NUMBER;
        hash = (53 * hash) + getServicesMetaDataList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainerResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getServicesMetaDataFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          servicesMetaDataBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (servicesMetaDataBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.servicesMetaData_ = servicesMetaData_;
        } else {
          result.servicesMetaData_ = servicesMetaDataBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.getDefaultInstance()) return this;
        if (servicesMetaDataBuilder_ == null) {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaData_.isEmpty()) {
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureServicesMetaDataIsMutable();
              servicesMetaData_.addAll(other.servicesMetaData_);
            }
            onChanged();
          }
        } else {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaDataBuilder_.isEmpty()) {
              servicesMetaDataBuilder_.dispose();
              servicesMetaDataBuilder_ = null;
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
              servicesMetaDataBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getServicesMetaDataFieldBuilder() : null;
            } else {
              servicesMetaDataBuilder_.addAllMessages(other.servicesMetaData_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_ =
        java.util.Collections.emptyList();
      private void ensureServicesMetaDataIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>(servicesMetaData_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> servicesMetaDataBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
        if (servicesMetaDataBuilder_ == null) {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        } else {
          return servicesMetaDataBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public int getServicesMetaDataCount() {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.size();
        } else {
          return servicesMetaDataBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);
        } else {
          return servicesMetaDataBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addAllServicesMetaData(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> values) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          super.addAll(values, servicesMetaData_);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder clearServicesMetaData() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          servicesMetaDataBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder removeServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.remove(index);
          onChanged();
        } else {
          servicesMetaDataBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder getServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
          int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);  } else {
          return servicesMetaDataBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
           getServicesMetaDataOrBuilderList() {
        if (servicesMetaDataBuilder_ != null) {
          return servicesMetaDataBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder() {
        return getServicesMetaDataFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder> 
           getServicesMetaDataBuilderList() {
        return getServicesMetaDataFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
          getServicesMetaDataFieldBuilder() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaDataBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder>(
                  servicesMetaData_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          servicesMetaData_ = null;
        }
        return servicesMetaDataBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainerResponseProto)
    }

    static {
      defaultInstance = new StartContainerResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainerResponseProto)
  }

  public interface StopContainerRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainerRequestProto}
   */
  public static final class StopContainerRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainerRequestProtoOrBuilder {
    // Use StopContainerRequestProto.newBuilder() to construct.
    private StopContainerRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainerRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainerRequestProto defaultInstance;
    public static StopContainerRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainerRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainerRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainerRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainerRequestProto>() {
      public StopContainerRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainerRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainerRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainerRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainerRequestProto)
    }

    static {
      defaultInstance = new StopContainerRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainerRequestProto)
  }

  public interface StopContainerResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainerResponseProto}
   */
  public static final class StopContainerResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainerResponseProtoOrBuilder {
    // Use StopContainerResponseProto.newBuilder() to construct.
    private StopContainerResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainerResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainerResponseProto defaultInstance;
    public static StopContainerResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainerResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainerResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainerResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainerResponseProto>() {
      public StopContainerResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainerResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainerResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainerResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainerResponseProto)
    }

    static {
      defaultInstance = new StopContainerResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainerResponseProto)
  }

  public interface GetContainerStatusRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerStatusRequestProto}
   */
  public static final class GetContainerStatusRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerStatusRequestProtoOrBuilder {
    // Use GetContainerStatusRequestProto.newBuilder() to construct.
    private GetContainerStatusRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerStatusRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerStatusRequestProto defaultInstance;
    public static GetContainerStatusRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerStatusRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerStatusRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerStatusRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerStatusRequestProto>() {
      public GetContainerStatusRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerStatusRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerStatusRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerStatusRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerStatusRequestProto)
    }

    static {
      defaultInstance = new GetContainerStatusRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerStatusRequestProto)
  }

  public interface GetContainerStatusResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerStatusProto status = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    boolean hasStatus();
    /**
     * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus();
    /**
     * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerStatusResponseProto}
   */
  public static final class GetContainerStatusResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerStatusResponseProtoOrBuilder {
    // Use GetContainerStatusResponseProto.newBuilder() to construct.
    private GetContainerStatusResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerStatusResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerStatusResponseProto defaultInstance;
    public static GetContainerStatusResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerStatusResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerStatusResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = status_.toBuilder();
              }
              status_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(status_);
                status_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerStatusResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerStatusResponseProto>() {
      public GetContainerStatusResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerStatusResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerStatusResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerStatusProto status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto status_;
    /**
     * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public boolean hasStatus() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus() {
      return status_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder() {
      return status_;
    }

    private void initFields() {
      status_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, status_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, status_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto) obj;

      boolean result = true;
      result = result && (hasStatus() == other.hasStatus());
      if (hasStatus()) {
        result = result && getStatus()
            .equals(other.getStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasStatus()) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getStatus().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerStatusResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStatusFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (statusBuilder_ == null) {
          status_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance();
        } else {
          statusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (statusBuilder_ == null) {
          result.status_ = status_;
        } else {
          result.status_ = statusBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto.getDefaultInstance()) return this;
        if (other.hasStatus()) {
          mergeStatus(other.getStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerStatusProto status = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto status_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> statusBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public boolean hasStatus() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus() {
        if (statusBuilder_ == null) {
          return status_;
        } else {
          return statusBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder setStatus(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          status_ = value;
          onChanged();
        } else {
          statusBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder setStatus(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          status_ = builderForValue.build();
          onChanged();
        } else {
          statusBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder mergeStatus(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              status_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance()) {
            status_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.newBuilder(status_).mergeFrom(value).buildPartial();
          } else {
            status_ = value;
          }
          onChanged();
        } else {
          statusBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder clearStatus() {
        if (statusBuilder_ == null) {
          status_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance();
          onChanged();
        } else {
          statusBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder getStatusBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getStatusFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder() {
        if (statusBuilder_ != null) {
          return statusBuilder_.getMessageOrBuilder();
        } else {
          return status_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
          getStatusFieldBuilder() {
        if (statusBuilder_ == null) {
          statusBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder>(
                  status_,
                  getParentForChildren(),
                  isClean());
          status_ = null;
        }
        return statusBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerStatusResponseProto)
    }

    static {
      defaultInstance = new GetContainerStatusResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerStatusResponseProto)
  }

  public interface StartContainersRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> 
        getStartContainerRequestList();
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getStartContainerRequest(int index);
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    int getStartContainerRequestCount();
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
        getStartContainerRequestOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder getStartContainerRequestOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainersRequestProto}
   *
   * <pre>
   *&#47;/ bulk API records
   * </pre>
   */
  public static final class StartContainersRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainersRequestProtoOrBuilder {
    // Use StartContainersRequestProto.newBuilder() to construct.
    private StartContainersRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainersRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainersRequestProto defaultInstance;
    public static StartContainersRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainersRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainersRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                startContainerRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              startContainerRequest_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          startContainerRequest_ = java.util.Collections.unmodifiableList(startContainerRequest_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainersRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainersRequestProto>() {
      public StartContainersRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainersRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainersRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;
    public static final int START_CONTAINER_REQUEST_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> startContainerRequest_;
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> getStartContainerRequestList() {
      return startContainerRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
        getStartContainerRequestOrBuilderList() {
      return startContainerRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public int getStartContainerRequestCount() {
      return startContainerRequest_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getStartContainerRequest(int index) {
      return startContainerRequest_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder getStartContainerRequestOrBuilder(
        int index) {
      return startContainerRequest_.get(index);
    }

    private void initFields() {
      startContainerRequest_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getStartContainerRequestCount(); i++) {
        if (!getStartContainerRequest(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < startContainerRequest_.size(); i++) {
        output.writeMessage(1, startContainerRequest_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < startContainerRequest_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, startContainerRequest_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto) obj;

      boolean result = true;
      result = result && getStartContainerRequestList()
          .equals(other.getStartContainerRequestList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getStartContainerRequestCount() > 0) {
        hash = (37 * hash) + START_CONTAINER_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getStartContainerRequestList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainersRequestProto}
     *
     * <pre>
     *&#47;/ bulk API records
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStartContainerRequestFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (startContainerRequestBuilder_ == null) {
          startContainerRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          startContainerRequestBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (startContainerRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            startContainerRequest_ = java.util.Collections.unmodifiableList(startContainerRequest_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.startContainerRequest_ = startContainerRequest_;
        } else {
          result.startContainerRequest_ = startContainerRequestBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.getDefaultInstance()) return this;
        if (startContainerRequestBuilder_ == null) {
          if (!other.startContainerRequest_.isEmpty()) {
            if (startContainerRequest_.isEmpty()) {
              startContainerRequest_ = other.startContainerRequest_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStartContainerRequestIsMutable();
              startContainerRequest_.addAll(other.startContainerRequest_);
            }
            onChanged();
          }
        } else {
          if (!other.startContainerRequest_.isEmpty()) {
            if (startContainerRequestBuilder_.isEmpty()) {
              startContainerRequestBuilder_.dispose();
              startContainerRequestBuilder_ = null;
              startContainerRequest_ = other.startContainerRequest_;
              bitField0_ = (bitField0_ & ~0x00000001);
              startContainerRequestBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStartContainerRequestFieldBuilder() : null;
            } else {
              startContainerRequestBuilder_.addAllMessages(other.startContainerRequest_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getStartContainerRequestCount(); i++) {
          if (!getStartContainerRequest(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> startContainerRequest_ =
        java.util.Collections.emptyList();
      private void ensureStartContainerRequestIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          startContainerRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto>(startContainerRequest_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> startContainerRequestBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> getStartContainerRequestList() {
        if (startContainerRequestBuilder_ == null) {
          return java.util.Collections.unmodifiableList(startContainerRequest_);
        } else {
          return startContainerRequestBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public int getStartContainerRequestCount() {
        if (startContainerRequestBuilder_ == null) {
          return startContainerRequest_.size();
        } else {
          return startContainerRequestBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getStartContainerRequest(int index) {
        if (startContainerRequestBuilder_ == null) {
          return startContainerRequest_.get(index);
        } else {
          return startContainerRequestBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder setStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto value) {
        if (startContainerRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.set(index, value);
          onChanged();
        } else {
          startContainerRequestBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder setStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder builderForValue) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.set(index, builderForValue.build());
          onChanged();
        } else {
          startContainerRequestBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto value) {
        if (startContainerRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(value);
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto value) {
        if (startContainerRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(index, value);
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder builderForValue) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(builderForValue.build());
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder builderForValue) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(index, builderForValue.build());
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addAllStartContainerRequest(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> values) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          super.addAll(values, startContainerRequest_);
          onChanged();
        } else {
          startContainerRequestBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder clearStartContainerRequest() {
        if (startContainerRequestBuilder_ == null) {
          startContainerRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          startContainerRequestBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder removeStartContainerRequest(int index) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.remove(index);
          onChanged();
        } else {
          startContainerRequestBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder getStartContainerRequestBuilder(
          int index) {
        return getStartContainerRequestFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder getStartContainerRequestOrBuilder(
          int index) {
        if (startContainerRequestBuilder_ == null) {
          return startContainerRequest_.get(index);  } else {
          return startContainerRequestBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
           getStartContainerRequestOrBuilderList() {
        if (startContainerRequestBuilder_ != null) {
          return startContainerRequestBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(startContainerRequest_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder addStartContainerRequestBuilder() {
        return getStartContainerRequestFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder addStartContainerRequestBuilder(
          int index) {
        return getStartContainerRequestFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder> 
           getStartContainerRequestBuilderList() {
        return getStartContainerRequestFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
          getStartContainerRequestFieldBuilder() {
        if (startContainerRequestBuilder_ == null) {
          startContainerRequestBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder>(
                  startContainerRequest_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          startContainerRequest_ = null;
        }
        return startContainerRequestBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainersRequestProto)
    }

    static {
      defaultInstance = new StartContainersRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainersRequestProto)
  }

  public interface ContainerExceptionMapProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();

    // optional .hadoop.yarn.SerializedExceptionProto exception = 2;
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    boolean hasException();
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto getException();
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder getExceptionOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ContainerExceptionMapProto}
   */
  public static final class ContainerExceptionMapProto extends
      com.google.protobuf.GeneratedMessage
      implements ContainerExceptionMapProtoOrBuilder {
    // Use ContainerExceptionMapProto.newBuilder() to construct.
    private ContainerExceptionMapProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ContainerExceptionMapProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ContainerExceptionMapProto defaultInstance;
    public static ContainerExceptionMapProto getDefaultInstance() {
      return defaultInstance;
    }

    public ContainerExceptionMapProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ContainerExceptionMapProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = exception_.toBuilder();
              }
              exception_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(exception_);
                exception_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ContainerExceptionMapProto> PARSER =
        new com.google.protobuf.AbstractParser<ContainerExceptionMapProto>() {
      public ContainerExceptionMapProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ContainerExceptionMapProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ContainerExceptionMapProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    // optional .hadoop.yarn.SerializedExceptionProto exception = 2;
    public static final int EXCEPTION_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto exception_;
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    public boolean hasException() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto getException() {
      return exception_;
    }
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder getExceptionOrBuilder() {
      return exception_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, exception_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, exception_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result && (hasException() == other.hasException());
      if (hasException()) {
        result = result && getException()
            .equals(other.getException());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      if (hasException()) {
        hash = (37 * hash) + EXCEPTION_FIELD_NUMBER;
        hash = (53 * hash) + getException().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ContainerExceptionMapProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
          getExceptionFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (exceptionBuilder_ == null) {
          exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
        } else {
          exceptionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (exceptionBuilder_ == null) {
          result.exception_ = exception_;
        } else {
          result.exception_ = exceptionBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        if (other.hasException()) {
          mergeException(other.getException());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // optional .hadoop.yarn.SerializedExceptionProto exception = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder> exceptionBuilder_;
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public boolean hasException() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto getException() {
        if (exceptionBuilder_ == null) {
          return exception_;
        } else {
          return exceptionBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder setException(org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto value) {
        if (exceptionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          exception_ = value;
          onChanged();
        } else {
          exceptionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder setException(
          org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder builderForValue) {
        if (exceptionBuilder_ == null) {
          exception_ = builderForValue.build();
          onChanged();
        } else {
          exceptionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder mergeException(org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto value) {
        if (exceptionBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              exception_ != org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance()) {
            exception_ =
              org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.newBuilder(exception_).mergeFrom(value).buildPartial();
          } else {
            exception_ = value;
          }
          onChanged();
        } else {
          exceptionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder clearException() {
        if (exceptionBuilder_ == null) {
          exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
          onChanged();
        } else {
          exceptionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder getExceptionBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getExceptionFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder getExceptionOrBuilder() {
        if (exceptionBuilder_ != null) {
          return exceptionBuilder_.getMessageOrBuilder();
        } else {
          return exception_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder> 
          getExceptionFieldBuilder() {
        if (exceptionBuilder_ == null) {
          exceptionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder>(
                  exception_,
                  getParentForChildren(),
                  isClean());
          exception_ = null;
        }
        return exceptionBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ContainerExceptionMapProto)
    }

    static {
      defaultInstance = new ContainerExceptionMapProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ContainerExceptionMapProto)
  }

  public interface StartContainersResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> 
        getServicesMetaDataList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index);
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    int getServicesMetaDataCount();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getSucceededRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    int getSucceededRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainersResponseProto}
   */
  public static final class StartContainersResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainersResponseProtoOrBuilder {
    // Use StartContainersResponseProto.newBuilder() to construct.
    private StartContainersResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainersResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainersResponseProto defaultInstance;
    public static StartContainersResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainersResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainersResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              servicesMetaData_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              succeededRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
        }
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainersResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainersResponseProto>() {
      public StartContainersResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainersResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainersResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    public static final int SERVICES_META_DATA_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public int getServicesMetaDataCount() {
      return servicesMetaData_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
      return servicesMetaData_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index) {
      return servicesMetaData_.get(index);
    }

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;
    public static final int SUCCEEDED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public int getSucceededRequestsCount() {
      return succeededRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
      return succeededRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index) {
      return succeededRequests_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      servicesMetaData_ = java.util.Collections.emptyList();
      succeededRequests_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        output.writeMessage(1, servicesMetaData_.get(i));
      }
      for (int i = 0; i < succeededRequests_.size(); i++) {
        output.writeMessage(2, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(3, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, servicesMetaData_.get(i));
      }
      for (int i = 0; i < succeededRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto) obj;

      boolean result = true;
      result = result && getServicesMetaDataList()
          .equals(other.getServicesMetaDataList());
      result = result && getSucceededRequestsList()
          .equals(other.getSucceededRequestsList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getServicesMetaDataCount() > 0) {
        hash = (37 * hash) + SERVICES_META_DATA_FIELD_NUMBER;
        hash = (53 * hash) + getServicesMetaDataList().hashCode();
      }
      if (getSucceededRequestsCount() > 0) {
        hash = (37 * hash) + SUCCEEDED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getSucceededRequestsList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainersResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getServicesMetaDataFieldBuilder();
          getSucceededRequestsFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          servicesMetaDataBuilder_.clear();
        }
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          succeededRequestsBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (servicesMetaDataBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.servicesMetaData_ = servicesMetaData_;
        } else {
          result.servicesMetaData_ = servicesMetaDataBuilder_.build();
        }
        if (succeededRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.succeededRequests_ = succeededRequests_;
        } else {
          result.succeededRequests_ = succeededRequestsBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.getDefaultInstance()) return this;
        if (servicesMetaDataBuilder_ == null) {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaData_.isEmpty()) {
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureServicesMetaDataIsMutable();
              servicesMetaData_.addAll(other.servicesMetaData_);
            }
            onChanged();
          }
        } else {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaDataBuilder_.isEmpty()) {
              servicesMetaDataBuilder_.dispose();
              servicesMetaDataBuilder_ = null;
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
              servicesMetaDataBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getServicesMetaDataFieldBuilder() : null;
            } else {
              servicesMetaDataBuilder_.addAllMessages(other.servicesMetaData_);
            }
          }
        }
        if (succeededRequestsBuilder_ == null) {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequests_.isEmpty()) {
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureSucceededRequestsIsMutable();
              succeededRequests_.addAll(other.succeededRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequestsBuilder_.isEmpty()) {
              succeededRequestsBuilder_.dispose();
              succeededRequestsBuilder_ = null;
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              succeededRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSucceededRequestsFieldBuilder() : null;
            } else {
              succeededRequestsBuilder_.addAllMessages(other.succeededRequests_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000004);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_ =
        java.util.Collections.emptyList();
      private void ensureServicesMetaDataIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>(servicesMetaData_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> servicesMetaDataBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
        if (servicesMetaDataBuilder_ == null) {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        } else {
          return servicesMetaDataBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public int getServicesMetaDataCount() {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.size();
        } else {
          return servicesMetaDataBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);
        } else {
          return servicesMetaDataBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addAllServicesMetaData(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> values) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          super.addAll(values, servicesMetaData_);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder clearServicesMetaData() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          servicesMetaDataBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder removeServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.remove(index);
          onChanged();
        } else {
          servicesMetaDataBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder getServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
          int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);  } else {
          return servicesMetaDataBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
           getServicesMetaDataOrBuilderList() {
        if (servicesMetaDataBuilder_ != null) {
          return servicesMetaDataBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder() {
        return getServicesMetaDataFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder> 
           getServicesMetaDataBuilderList() {
        return getServicesMetaDataFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
          getServicesMetaDataFieldBuilder() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaDataBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder>(
                  servicesMetaData_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          servicesMetaData_ = null;
        }
        return servicesMetaDataBuilder_;
      }

      // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_ =
        java.util.Collections.emptyList();
      private void ensureSucceededRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(succeededRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> succeededRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
        if (succeededRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        } else {
          return succeededRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public int getSucceededRequestsCount() {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.size();
        } else {
          return succeededRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);
        } else {
          return succeededRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addAllSucceededRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          super.addAll(values, succeededRequests_);
          onChanged();
        } else {
          succeededRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder clearSucceededRequests() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          succeededRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder removeSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.remove(index);
          onChanged();
        } else {
          succeededRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
          int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);  } else {
          return succeededRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getSucceededRequestsOrBuilderList() {
        if (succeededRequestsBuilder_ != null) {
          return succeededRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder() {
        return getSucceededRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getSucceededRequestsBuilderList() {
        return getSucceededRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getSucceededRequestsFieldBuilder() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  succeededRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          succeededRequests_ = null;
        }
        return succeededRequestsBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainersResponseProto)
    }

    static {
      defaultInstance = new StartContainersResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainersResponseProto)
  }

  public interface StopContainersRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getContainerIdList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    int getContainerIdCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainersRequestProto}
   */
  public static final class StopContainersRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainersRequestProtoOrBuilder {
    // Use StopContainersRequestProto.newBuilder() to construct.
    private StopContainersRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainersRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainersRequestProto defaultInstance;
    public static StopContainersRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainersRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainersRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              containerId_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = java.util.Collections.unmodifiableList(containerId_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainersRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainersRequestProto>() {
      public StopContainersRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainersRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainersRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public int getContainerIdCount() {
      return containerId_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
      return containerId_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index) {
      return containerId_.get(index);
    }

    private void initFields() {
      containerId_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < containerId_.size(); i++) {
        output.writeMessage(1, containerId_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < containerId_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto) obj;

      boolean result = true;
      result = result && getContainerIdList()
          .equals(other.getContainerIdList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getContainerIdCount() > 0) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerIdList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainersRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            containerId_ = java.util.Collections.unmodifiableList(containerId_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.getDefaultInstance()) return this;
        if (containerIdBuilder_ == null) {
          if (!other.containerId_.isEmpty()) {
            if (containerId_.isEmpty()) {
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureContainerIdIsMutable();
              containerId_.addAll(other.containerId_);
            }
            onChanged();
          }
        } else {
          if (!other.containerId_.isEmpty()) {
            if (containerIdBuilder_.isEmpty()) {
              containerIdBuilder_.dispose();
              containerIdBuilder_ = null;
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
              containerIdBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getContainerIdFieldBuilder() : null;
            } else {
              containerIdBuilder_.addAllMessages(other.containerId_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_ =
        java.util.Collections.emptyList();
      private void ensureContainerIdIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(containerId_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
        if (containerIdBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containerId_);
        } else {
          return containerIdBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public int getContainerIdCount() {
        if (containerIdBuilder_ == null) {
          return containerId_.size();
        } else {
          return containerIdBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);
        } else {
          return containerIdBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.set(index, value);
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.set(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(index, value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addAllContainerId(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          super.addAll(values, containerId_);
          onChanged();
        } else {
          containerIdBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder removeContainerId(int index) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.remove(index);
          onChanged();
        } else {
          containerIdBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
          int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);  } else {
          return containerIdBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getContainerIdOrBuilderList() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containerId_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder() {
        return getContainerIdFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getContainerIdBuilderList() {
        return getContainerIdFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainersRequestProto)
    }

    static {
      defaultInstance = new StopContainersRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainersRequestProto)
  }

  public interface StopContainersResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getSucceededRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    int getSucceededRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainersResponseProto}
   */
  public static final class StopContainersResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainersResponseProtoOrBuilder {
    // Use StopContainersResponseProto.newBuilder() to construct.
    private StopContainersResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainersResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainersResponseProto defaultInstance;
    public static StopContainersResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainersResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainersResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              succeededRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainersResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainersResponseProto>() {
      public StopContainersResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainersResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainersResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
    public static final int SUCCEEDED_REQUESTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public int getSucceededRequestsCount() {
      return succeededRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
      return succeededRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index) {
      return succeededRequests_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      succeededRequests_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < succeededRequests_.size(); i++) {
        output.writeMessage(1, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(2, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < succeededRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto) obj;

      boolean result = true;
      result = result && getSucceededRequestsList()
          .equals(other.getSucceededRequestsList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getSucceededRequestsCount() > 0) {
        hash = (37 * hash) + SUCCEEDED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getSucceededRequestsList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainersResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getSucceededRequestsFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          succeededRequestsBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (succeededRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.succeededRequests_ = succeededRequests_;
        } else {
          result.succeededRequests_ = succeededRequestsBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.getDefaultInstance()) return this;
        if (succeededRequestsBuilder_ == null) {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequests_.isEmpty()) {
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureSucceededRequestsIsMutable();
              succeededRequests_.addAll(other.succeededRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequestsBuilder_.isEmpty()) {
              succeededRequestsBuilder_.dispose();
              succeededRequestsBuilder_ = null;
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000001);
              succeededRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSucceededRequestsFieldBuilder() : null;
            } else {
              succeededRequestsBuilder_.addAllMessages(other.succeededRequests_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_ =
        java.util.Collections.emptyList();
      private void ensureSucceededRequestsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(succeededRequests_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> succeededRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
        if (succeededRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        } else {
          return succeededRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public int getSucceededRequestsCount() {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.size();
        } else {
          return succeededRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);
        } else {
          return succeededRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addAllSucceededRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          super.addAll(values, succeededRequests_);
          onChanged();
        } else {
          succeededRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder clearSucceededRequests() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          succeededRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder removeSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.remove(index);
          onChanged();
        } else {
          succeededRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
          int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);  } else {
          return succeededRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getSucceededRequestsOrBuilderList() {
        if (succeededRequestsBuilder_ != null) {
          return succeededRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder() {
        return getSucceededRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getSucceededRequestsBuilderList() {
        return getSucceededRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getSucceededRequestsFieldBuilder() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  succeededRequests_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          succeededRequests_ = null;
        }
        return succeededRequestsBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainersResponseProto)
    }

    static {
      defaultInstance = new StopContainersResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainersResponseProto)
  }

  public interface GetContainerStatusesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getContainerIdList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    int getContainerIdCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerStatusesRequestProto}
   */
  public static final class GetContainerStatusesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerStatusesRequestProtoOrBuilder {
    // Use GetContainerStatusesRequestProto.newBuilder() to construct.
    private GetContainerStatusesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerStatusesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerStatusesRequestProto defaultInstance;
    public static GetContainerStatusesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerStatusesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerStatusesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              containerId_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = java.util.Collections.unmodifiableList(containerId_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerStatusesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerStatusesRequestProto>() {
      public GetContainerStatusesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerStatusesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerStatusesRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public int getContainerIdCount() {
      return containerId_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
      return containerId_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index) {
      return containerId_.get(index);
    }

    private void initFields() {
      containerId_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < containerId_.size(); i++) {
        output.writeMessage(1, containerId_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < containerId_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto) obj;

      boolean result = true;
      result = result && getContainerIdList()
          .equals(other.getContainerIdList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getContainerIdCount() > 0) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerIdList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerStatusesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            containerId_ = java.util.Collections.unmodifiableList(containerId_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.getDefaultInstance()) return this;
        if (containerIdBuilder_ == null) {
          if (!other.containerId_.isEmpty()) {
            if (containerId_.isEmpty()) {
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureContainerIdIsMutable();
              containerId_.addAll(other.containerId_);
            }
            onChanged();
          }
        } else {
          if (!other.containerId_.isEmpty()) {
            if (containerIdBuilder_.isEmpty()) {
              containerIdBuilder_.dispose();
              containerIdBuilder_ = null;
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
              containerIdBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getContainerIdFieldBuilder() : null;
            } else {
              containerIdBuilder_.addAllMessages(other.containerId_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_ =
        java.util.Collections.emptyList();
      private void ensureContainerIdIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(containerId_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
        if (containerIdBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containerId_);
        } else {
          return containerIdBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public int getContainerIdCount() {
        if (containerIdBuilder_ == null) {
          return containerId_.size();
        } else {
          return containerIdBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);
        } else {
          return containerIdBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.set(index, value);
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.set(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(index, value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addAllContainerId(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          super.addAll(values, containerId_);
          onChanged();
        } else {
          containerIdBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder removeContainerId(int index) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.remove(index);
          onChanged();
        } else {
          containerIdBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
          int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);  } else {
          return containerIdBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getContainerIdOrBuilderList() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containerId_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder() {
        return getContainerIdFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getContainerIdBuilderList() {
        return getContainerIdFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerStatusesRequestProto)
    }

    static {
      defaultInstance = new GetContainerStatusesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerStatusesRequestProto)
  }

  public interface GetContainerStatusesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerStatusProto status = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> 
        getStatusList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    int getStatusCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getStatusOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerStatusesResponseProto}
   */
  public static final class GetContainerStatusesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerStatusesResponseProtoOrBuilder {
    // Use GetContainerStatusesResponseProto.newBuilder() to construct.
    private GetContainerStatusesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerStatusesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerStatusesResponseProto defaultInstance;
    public static GetContainerStatusesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerStatusesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerStatusesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                status_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              status_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          status_ = java.util.Collections.unmodifiableList(status_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerStatusesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerStatusesResponseProto>() {
      public GetContainerStatusesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerStatusesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerStatusesResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerStatusProto status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> status_;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getStatusList() {
      return status_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getStatusOrBuilderList() {
      return status_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public int getStatusCount() {
      return status_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus(int index) {
      return status_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder(
        int index) {
      return status_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      status_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < status_.size(); i++) {
        output.writeMessage(1, status_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(2, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < status_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, status_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto) obj;

      boolean result = true;
      result = result && getStatusList()
          .equals(other.getStatusList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getStatusCount() > 0) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getStatusList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerStatusesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStatusFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (statusBuilder_ == null) {
          status_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          statusBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (statusBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            status_ = java.util.Collections.unmodifiableList(status_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.status_ = status_;
        } else {
          result.status_ = statusBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.getDefaultInstance()) return this;
        if (statusBuilder_ == null) {
          if (!other.status_.isEmpty()) {
            if (status_.isEmpty()) {
              status_ = other.status_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStatusIsMutable();
              status_.addAll(other.status_);
            }
            onChanged();
          }
        } else {
          if (!other.status_.isEmpty()) {
            if (statusBuilder_.isEmpty()) {
              statusBuilder_.dispose();
              statusBuilder_ = null;
              status_ = other.status_;
              bitField0_ = (bitField0_ & ~0x00000001);
              statusBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStatusFieldBuilder() : null;
            } else {
              statusBuilder_.addAllMessages(other.status_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerStatusProto status = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> status_ =
        java.util.Collections.emptyList();
      private void ensureStatusIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          status_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>(status_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> statusBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getStatusList() {
        if (statusBuilder_ == null) {
          return java.util.Collections.unmodifiableList(status_);
        } else {
          return statusBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public int getStatusCount() {
        if (statusBuilder_ == null) {
          return status_.size();
        } else {
          return statusBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus(int index) {
        if (statusBuilder_ == null) {
          return status_.get(index);
        } else {
          return statusBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder setStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStatusIsMutable();
          status_.set(index, value);
          onChanged();
        } else {
          statusBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder setStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.set(index, builderForValue.build());
          onChanged();
        } else {
          statusBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStatusIsMutable();
          status_.add(value);
          onChanged();
        } else {
          statusBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStatusIsMutable();
          status_.add(index, value);
          onChanged();
        } else {
          statusBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.add(builderForValue.build());
          onChanged();
        } else {
          statusBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.add(index, builderForValue.build());
          onChanged();
        } else {
          statusBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addAllStatus(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> values) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          super.addAll(values, status_);
          onChanged();
        } else {
          statusBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder clearStatus() {
        if (statusBuilder_ == null) {
          status_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          statusBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder removeStatus(int index) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.remove(index);
          onChanged();
        } else {
          statusBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder getStatusBuilder(
          int index) {
        return getStatusFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder(
          int index) {
        if (statusBuilder_ == null) {
          return status_.get(index);  } else {
          return statusBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
           getStatusOrBuilderList() {
        if (statusBuilder_ != null) {
          return statusBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(status_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addStatusBuilder() {
        return getStatusFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addStatusBuilder(
          int index) {
        return getStatusFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder> 
           getStatusBuilderList() {
        return getStatusFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
          getStatusFieldBuilder() {
        if (statusBuilder_ == null) {
          statusBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder>(
                  status_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          status_ = null;
        }
        return statusBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerStatusesResponseProto)
    }

    static {
      defaultInstance = new GetContainerStatusesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerStatusesResponseProto)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NMTokenProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerStatusRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerStatusRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerStatusResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerStatusResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\031yarn_service_protos.proto\022\013hadoop.yarn" +
      "\032\016Security.proto\032\021yarn_protos.proto\"]\n%R" +
      "egisterApplicationMasterRequestProto\022\014\n\004" +
      "host\030\001 \001(\t\022\020\n\010rpc_port\030\002 \001(\005\022\024\n\014tracking" +
      "_url\030\003 \001(\t\"\324\001\n&RegisterApplicationMaster" +
      "ResponseProto\0225\n\021maximumCapability\030\001 \001(\013" +
      "2\032.hadoop.yarn.ResourceProto\022%\n\035client_t" +
      "o_am_token_master_key\030\002 \001(\014\022=\n\020applicati" +
      "on_ACLs\030\003 \003(\0132#.hadoop.yarn.ApplicationA" +
      "CLMapProto\022\r\n\005queue\030\005 \001(\t\"\234\001\n#FinishAppl",
      "icationMasterRequestProto\022\023\n\013diagnostics" +
      "\030\001 \001(\t\022\024\n\014tracking_url\030\002 \001(\t\022J\n\030final_ap" +
      "plication_status\030\003 \001(\0162(.hadoop.yarn.Fin" +
      "alApplicationStatusProto\"E\n$FinishApplic" +
      "ationMasterResponseProto\022\035\n\016isUnregister" +
      "ed\030\001 \001(\010:\005false\"\262\002\n\024AllocateRequestProto" +
      "\022.\n\003ask\030\001 \003(\0132!.hadoop.yarn.ResourceRequ" +
      "estProto\022.\n\007release\030\002 \003(\0132\035.hadoop.yarn." +
      "ContainerIdProto\022E\n\021blacklist_request\030\003 " +
      "\001(\0132*.hadoop.yarn.ResourceBlacklistReque",
      "stProto\022\023\n\013response_id\030\004 \001(\005\022\020\n\010progress" +
      "\030\005 \001(\002\022L\n\020increase_request\030\006 \003(\01322.hadoo" +
      "p.yarn.ContainerResourceIncreaseRequestP" +
      "roto\"b\n\014NMTokenProto\022(\n\006nodeId\030\001 \001(\0132\030.h" +
      "adoop.yarn.NodeIdProto\022(\n\005token\030\002 \001(\0132\031." +
      "hadoop.common.TokenProto\"\327\004\n\025AllocateRes" +
      "ponseProto\0220\n\013a_m_command\030\001 \001(\0162\033.hadoop" +
      ".yarn.AMCommandProto\022\023\n\013response_id\030\002 \001(" +
      "\005\0229\n\024allocated_containers\030\003 \003(\0132\033.hadoop" +
      ".yarn.ContainerProto\022G\n\034completed_contai",
      "ner_statuses\030\004 \003(\0132!.hadoop.yarn.Contain" +
      "erStatusProto\022)\n\005limit\030\005 \001(\0132\032.hadoop.ya" +
      "rn.ResourceProto\0223\n\rupdated_nodes\030\006 \003(\0132" +
      "\034.hadoop.yarn.NodeReportProto\022\031\n\021num_clu" +
      "ster_nodes\030\007 \001(\005\0224\n\007preempt\030\010 \001(\0132#.hado" +
      "op.yarn.PreemptionMessageProto\022,\n\tnm_tok" +
      "ens\030\t \003(\0132\031.hadoop.yarn.NMTokenProto\022I\n\024" +
      "increased_containers\030\n \003(\0132+.hadoop.yarn" +
      ".ContainerResourceIncreaseProto\022I\n\024decre" +
      "ased_containers\030\013 \003(\0132+.hadoop.yarn.Cont",
      "ainerResourceDecreaseProto\"\037\n\035GetNewAppl" +
      "icationRequestProto\"\220\001\n\036GetNewApplicatio" +
      "nResponseProto\0227\n\016application_id\030\001 \001(\0132\037" +
      ".hadoop.yarn.ApplicationIdProto\0225\n\021maxim" +
      "umCapability\030\002 \001(\0132\032.hadoop.yarn.Resourc" +
      "eProto\"[\n GetApplicationReportRequestPro" +
      "to\0227\n\016application_id\030\001 \001(\0132\037.hadoop.yarn" +
      ".ApplicationIdProto\"d\n!GetApplicationRep" +
      "ortResponseProto\022?\n\022application_report\030\001" +
      " \001(\0132#.hadoop.yarn.ApplicationReportProt",
      "o\"w\n\035SubmitApplicationRequestProto\022V\n\036ap" +
      "plication_submission_context\030\001 \001(\0132..had" +
      "oop.yarn.ApplicationSubmissionContextPro" +
      "to\" \n\036SubmitApplicationResponseProto\"V\n\033" +
      "KillApplicationRequestProto\0227\n\016applicati" +
      "on_id\030\001 \001(\0132\037.hadoop.yarn.ApplicationIdP" +
      "roto\"@\n\034KillApplicationResponseProto\022 \n\021" +
      "is_kill_completed\030\001 \001(\010:\005false\"\037\n\035GetClu" +
      "sterMetricsRequestProto\"_\n\036GetClusterMet" +
      "ricsResponseProto\022=\n\017cluster_metrics\030\001 \001",
      "(\0132$.hadoop.yarn.YarnClusterMetricsProto" +
      "\"\374\001\n\033GetApplicationsRequestProto\022\031\n\021appl" +
      "ication_types\030\001 \003(\t\022B\n\022application_state" +
      "s\030\002 \003(\0162&.hadoop.yarn.YarnApplicationSta" +
      "teProto\022\r\n\005users\030\003 \003(\t\022\016\n\006queues\030\004 \003(\t\022\r" +
      "\n\005limit\030\005 \001(\003\022\023\n\013start_begin\030\006 \001(\003\022\021\n\tst" +
      "art_end\030\007 \001(\003\022\024\n\014finish_begin\030\010 \001(\003\022\022\n\nf" +
      "inish_end\030\t \001(\003\"Y\n\034GetApplicationsRespon" +
      "seProto\0229\n\014applications\030\001 \003(\0132#.hadoop.y" +
      "arn.ApplicationReportProto\"N\n\033GetCluster",
      "NodesRequestProto\022/\n\nnodeStates\030\001 \003(\0162\033." +
      "hadoop.yarn.NodeStateProto\"Q\n\034GetCluster" +
      "NodesResponseProto\0221\n\013nodeReports\030\001 \003(\0132" +
      "\034.hadoop.yarn.NodeReportProto\"y\n\030GetQueu" +
      "eInfoRequestProto\022\021\n\tqueueName\030\001 \001(\t\022\033\n\023" +
      "includeApplications\030\002 \001(\010\022\032\n\022includeChil" +
      "dQueues\030\003 \001(\010\022\021\n\trecursive\030\004 \001(\010\"K\n\031GetQ" +
      "ueueInfoResponseProto\022.\n\tqueueInfo\030\001 \001(\013" +
      "2\033.hadoop.yarn.QueueInfoProto\"\"\n GetQueu" +
      "eUserAclsInfoRequestProto\"^\n!GetQueueUse",
      "rAclsInfoResponseProto\0229\n\rqueueUserAcls\030" +
      "\001 \003(\0132\".hadoop.yarn.QueueUserACLInfoProt" +
      "o\"\234\001\n\032StartContainerRequestProto\022J\n\030cont" +
      "ainer_launch_context\030\001 \001(\0132(.hadoop.yarn" +
      ".ContainerLaunchContextProto\0222\n\017containe" +
      "r_token\030\002 \001(\0132\031.hadoop.common.TokenProto" +
      "\"[\n\033StartContainerResponseProto\022<\n\022servi" +
      "ces_meta_data\030\001 \003(\0132 .hadoop.yarn.String" +
      "BytesMapProto\"P\n\031StopContainerRequestPro" +
      "to\0223\n\014container_id\030\001 \001(\0132\035.hadoop.yarn.C",
      "ontainerIdProto\"\034\n\032StopContainerResponse" +
      "Proto\"U\n\036GetContainerStatusRequestProto\022" +
      "3\n\014container_id\030\001 \001(\0132\035.hadoop.yarn.Cont" +
      "ainerIdProto\"T\n\037GetContainerStatusRespon" +
      "seProto\0221\n\006status\030\001 \001(\0132!.hadoop.yarn.Co" +
      "ntainerStatusProto\"g\n\033StartContainersReq" +
      "uestProto\022H\n\027start_container_request\030\001 \003" +
      "(\0132\'.hadoop.yarn.StartContainerRequestPr" +
      "oto\"\213\001\n\032ContainerExceptionMapProto\0223\n\014co" +
      "ntainer_id\030\001 \001(\0132\035.hadoop.yarn.Container",
      "IdProto\0228\n\texception\030\002 \001(\0132%.hadoop.yarn" +
      ".SerializedExceptionProto\"\331\001\n\034StartConta" +
      "inersResponseProto\022<\n\022services_meta_data" +
      "\030\001 \003(\0132 .hadoop.yarn.StringBytesMapProto" +
      "\0229\n\022succeeded_requests\030\002 \003(\0132\035.hadoop.ya" +
      "rn.ContainerIdProto\022@\n\017failed_requests\030\003" +
      " \003(\0132\'.hadoop.yarn.ContainerExceptionMap" +
      "Proto\"Q\n\032StopContainersRequestProto\0223\n\014c" +
      "ontainer_id\030\001 \003(\0132\035.hadoop.yarn.Containe" +
      "rIdProto\"\232\001\n\033StopContainersResponseProto",
      "\0229\n\022succeeded_requests\030\001 \003(\0132\035.hadoop.ya" +
      "rn.ContainerIdProto\022@\n\017failed_requests\030\002" +
      " \003(\0132\'.hadoop.yarn.ContainerExceptionMap" +
      "Proto\"W\n GetContainerStatusesRequestProt" +
      "o\0223\n\014container_id\030\001 \003(\0132\035.hadoop.yarn.Co" +
      "ntainerIdProto\"\230\001\n!GetContainerStatusesR" +
      "esponseProto\0221\n\006status\030\001 \003(\0132!.hadoop.ya" +
      "rn.ContainerStatusProto\022@\n\017failed_reques" +
      "ts\030\002 \003(\0132\'.hadoop.yarn.ContainerExceptio" +
      "nMapProtoB7\n\034org.apache.hadoop.yarn.prot",
      "oB\021YarnServiceProtos\210\001\001\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor,
              new java.lang.String[] { "Host", "RpcPort", "TrackingUrl", });
          internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor,
              new java.lang.String[] { "MaximumCapability", "ClientToAmTokenMasterKey", "ApplicationACLs", "Queue", });
          internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor,
              new java.lang.String[] { "Diagnostics", "TrackingUrl", "FinalApplicationStatus", });
          internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor,
              new java.lang.String[] { "IsUnregistered", });
          internal_static_hadoop_yarn_AllocateRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_AllocateRequestProto_descriptor,
              new java.lang.String[] { "Ask", "Release", "BlacklistRequest", "ResponseId", "Progress", "IncreaseRequest", });
          internal_static_hadoop_yarn_NMTokenProto_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_NMTokenProto_descriptor,
              new java.lang.String[] { "NodeId", "Token", });
          internal_static_hadoop_yarn_AllocateResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_AllocateResponseProto_descriptor,
              new java.lang.String[] { "AMCommand", "ResponseId", "AllocatedContainers", "CompletedContainerStatuses", "Limit", "UpdatedNodes", "NumClusterNodes", "Preempt", "NmTokens", "IncreasedContainers", "DecreasedContainers", });
          internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor,
              new java.lang.String[] { "ApplicationId", "MaximumCapability", });
          internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", });
          internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(10);
          internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor,
              new java.lang.String[] { "ApplicationReport", });
          internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(11);
          internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor,
              new java.lang.String[] { "ApplicationSubmissionContext", });
          internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(12);
          internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(13);
          internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", });
          internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(14);
          internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor,
              new java.lang.String[] { "IsKillCompleted", });
          internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(15);
          internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(16);
          internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor,
              new java.lang.String[] { "ClusterMetrics", });
          internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(17);
          internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor,
              new java.lang.String[] { "ApplicationTypes", "ApplicationStates", "Users", "Queues", "Limit", "StartBegin", "StartEnd", "FinishBegin", "FinishEnd", });
          internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(18);
          internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor,
              new java.lang.String[] { "Applications", });
          internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(19);
          internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor,
              new java.lang.String[] { "NodeStates", });
          internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(20);
          internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor,
              new java.lang.String[] { "NodeReports", });
          internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(21);
          internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor,
              new java.lang.String[] { "QueueName", "IncludeApplications", "IncludeChildQueues", "Recursive", });
          internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(22);
          internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor,
              new java.lang.String[] { "QueueInfo", });
          internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(23);
          internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(24);
          internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor,
              new java.lang.String[] { "QueueUserAcls", });
          internal_static_hadoop_yarn_StartContainerRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(25);
          internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainerRequestProto_descriptor,
              new java.lang.String[] { "ContainerLaunchContext", "ContainerToken", });
          internal_static_hadoop_yarn_StartContainerResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(26);
          internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainerResponseProto_descriptor,
              new java.lang.String[] { "ServicesMetaData", });
          internal_static_hadoop_yarn_StopContainerRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(27);
          internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainerRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_StopContainerResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(28);
          internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainerResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetContainerStatusRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(29);
          internal_static_hadoop_yarn_GetContainerStatusRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerStatusRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_GetContainerStatusResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(30);
          internal_static_hadoop_yarn_GetContainerStatusResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerStatusResponseProto_descriptor,
              new java.lang.String[] { "Status", });
          internal_static_hadoop_yarn_StartContainersRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(31);
          internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainersRequestProto_descriptor,
              new java.lang.String[] { "StartContainerRequest", });
          internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor =
            getDescriptor().getMessageTypes().get(32);
          internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor,
              new java.lang.String[] { "ContainerId", "Exception", });
          internal_static_hadoop_yarn_StartContainersResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(33);
          internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainersResponseProto_descriptor,
              new java.lang.String[] { "ServicesMetaData", "SucceededRequests", "FailedRequests", });
          internal_static_hadoop_yarn_StopContainersRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(34);
          internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainersRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_StopContainersResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(35);
          internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainersResponseProto_descriptor,
              new java.lang.String[] { "SucceededRequests", "FailedRequests", });
          internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(36);
          internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(37);
          internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor,
              new java.lang.String[] { "Status", "FailedRequests", });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.security.proto.SecurityProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
